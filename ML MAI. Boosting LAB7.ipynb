{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zgoi8i6plWlO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### О ЛР:\n",
    "\n",
    "- Coding Gradient boosting\n",
    "\n",
    "----\n",
    "\n",
    "#### Самостоятельная оценка результатов\n",
    "\n",
    "Для удобства проверки, исходя из набора решенных задач, посчитайте свою максимальную оценку (Она тут равняется 6).\n",
    "\n",
    "**Оценка**: 5\n",
    "\n",
    "***DeadLine - 09.01.2025 23:59***\n",
    "\n",
    "### Формат сдачи\n",
    "Задания сдаются через lms. Вы прикрепляете **ССЫЛКУ НА ПУБЛИЧНЫЙ РЕПОЗИТОРИЙ**, где выполнено ваше задание. Иначе задание не проверяется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1XDUNTn4lWlP",
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-01-05T10:17:54.013801300Z",
     "start_time": "2025-01-05T10:17:52.127451300Z"
    }
   },
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.sparse import load_npz\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yvolL0KvlWlQ",
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-01-05T10:17:54.023316800Z",
     "start_time": "2025-01-05T10:17:54.014801900Z"
    }
   },
   "outputs": [],
   "source": [
    "x = load_npz(\"x.npz\")\n",
    "y = np.load(\"y.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRMjj9ZslWlQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Разделим на обучающую, валидационную и тестовую выборки (`random_state` оставьте равным 666 для воспроизводимости)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Hme6Cf0HlWlR",
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-01-05T10:17:54.045254900Z",
     "start_time": "2025-01-05T10:17:54.022316100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "((18825, 169), (2354, 169), (2353, 169))"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=666\n",
    ")\n",
    "\n",
    "x_test, x_valid, y_test, y_valid = train_test_split(\n",
    "    x_test, y_test, test_size=0.5, random_state=666\n",
    ")\n",
    "\n",
    "x_train.shape, x_valid.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHaBuXarlWlR",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задание 1. Реализация градиентного бустинга [2 балла]\n",
    "\n",
    "Необходимо дописать код в файле `boosting.py`. Уже создан шаблон класса `Boosting`, который можно модифицировать по своему усмотрению.\n",
    "\n",
    "### Описание функций:\n",
    "\n",
    "#### `__init__`\n",
    "\n",
    "Конструктор класса принимает следующие параметры:\n",
    "\n",
    "- `base_model_class` — класс базовой модели для бустинга.\n",
    "- `base_model_params` — словарь гиперпараметров для базовой модели.\n",
    "- `n_estimators` — количество базовых моделей для обучения.\n",
    "- `learning_rate` — темп обучения, должен быть в диапазоне (0, 1].\n",
    "- `subsample` — доля обучающей выборки для тренировки базовой модели (размер бутстрап-выборки относительно исходной).\n",
    "- `early_stopping_rounds` — число итераций без улучшения на валидационной выборке, после которых обучение прекращается.\n",
    "- `plot` — флаг для построения графика качества моделей после обучения.\n",
    "\n",
    "#### `fit`\n",
    "\n",
    "Метод `fit` принимает обучающую и валидационную выборки.\n",
    "\n",
    "1. Инициализируем нулевую модель и делаем предсказания (например, все нули) для обеих выборок.\n",
    "2. Обучаем `n_estimators` базовых моделей:\n",
    "   - Обучаем новую базовую модель на текущих остатках.\n",
    "   - Обновляем предсказания на обучающей и валидационной выборках.\n",
    "   - Рассчитываем ошибки на обеих выборках с помощью `loss_fn`.\n",
    "   - Проверяем условия для ранней остановки.\n",
    "\n",
    "3. Если флаг `plot` установлен, строим график качества после обучения всех моделей.\n",
    "\n",
    "#### `fit_new_base_model`\n",
    "\n",
    "Метод `fit_new_base_model` принимает обучающую выборку и текущие предсказания для неё.\n",
    "\n",
    "1. Генерируем бутстрап-выборку.\n",
    "2. Обучаем базовую модель на этой выборке.\n",
    "3. Оптимизируем значение гаммы.\n",
    "4. Добавляем новую базовую модель и гамму в соответствующие списки (учитывая `learning_rate`).\n",
    "\n",
    "#### `predict_proba`\n",
    "\n",
    "Метод `predict_proba` принимает выборку для предсказания вероятностей.\n",
    "\n",
    "1. Суммируем предсказания базовых моделей (учитывая гамму и `learning_rate`).\n",
    "2. Применяем сигмоидальную функцию для получения вероятностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xh3nawbUlWlS",
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-01-05T10:17:54.103799200Z",
     "start_time": "2025-01-05T10:17:54.041255600Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "reqHbUEBlWlS",
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-01-05T10:17:55.130566Z",
     "start_time": "2025-01-05T10:17:54.103799200Z"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from boosting import Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7wWK5RplWlT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Проверка кода\n",
    "\n",
    "У автора задания всё учится около одной секунды."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "00lZjzI3lWlT",
    "outputId": "c1aa6886-069b-433f-f0c1-5cf70bdb15cb",
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-01-05T10:17:56.398475800Z",
     "start_time": "2025-01-05T10:17:55.132558300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10: Train Loss = 0.6737, Valid Loss = 0.6754\n",
      "Iteration 2/10: Train Loss = 0.6553, Valid Loss = 0.6590\n",
      "Iteration 3/10: Train Loss = 0.6376, Valid Loss = 0.6427\n",
      "Iteration 4/10: Train Loss = 0.6207, Valid Loss = 0.6274\n",
      "Iteration 5/10: Train Loss = 0.6046, Valid Loss = 0.6132\n",
      "Iteration 6/10: Train Loss = 0.5892, Valid Loss = 0.5997\n",
      "Iteration 7/10: Train Loss = 0.5745, Valid Loss = 0.5869\n",
      "Iteration 8/10: Train Loss = 0.5605, Valid Loss = 0.5743\n",
      "Iteration 9/10: Train Loss = 0.5473, Valid Loss = 0.5628\n",
      "Iteration 10/10: Train Loss = 0.5346, Valid Loss = 0.5515\n",
      "CPU times: total: 1.11 s\n",
      "Wall time: 1.18 s\n",
      "Train ROC-AUC 0.9867\n",
      "Valid ROC-AUC 0.9493\n",
      "Test ROC-AUC 0.9440\n"
     ]
    }
   ],
   "source": [
    "boosting = Boosting()\n",
    "\n",
    "%time boosting.fit(x_train, y_train, x_valid, y_valid)\n",
    "\n",
    "assert len(boosting.models) == boosting.n_estimators\n",
    "assert len(boosting.gammas) == boosting.n_estimators\n",
    "\n",
    "assert boosting.predict_proba(x_test).shape == (x_test.shape[0], 2)\n",
    "\n",
    "print(f'Train ROC-AUC {boosting.score(x_train, y_train):.4f}')\n",
    "print(f'Valid ROC-AUC {boosting.score(x_valid, y_valid):.4f}')\n",
    "print(f'Test ROC-AUC {boosting.score(x_test, y_test):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlU-c9CxlWlU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задание 2. Обучение градиентного бустинга [0.5 балла]\n",
    "\n",
    "Оцените качество вашей реализации градиентного бустинга на тестовой выборке, используя базовые модели — решающие деревья с различной максимальной глубиной. Метрикой будет ROC-AUC.\n",
    "\n",
    "**Инструкция:**\n",
    "1. Перебирайте значения максимальной глубины деревьев от 1 до 30 с шагом 2.\n",
    "2. Оставьте остальные параметры бустинга по умолчанию.\n",
    "3. Постройте график зависимости качества на обучающей и тестовой выборке от максимальной глубины деревьев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-IjO9FqelWlU",
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-01-05T10:18:07.043931400Z",
     "start_time": "2025-01-05T10:17:56.399475700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10: Train Loss = 0.6819, Valid Loss = 0.6823\n",
      "Iteration 2/10: Train Loss = 0.6712, Valid Loss = 0.6720\n",
      "Iteration 3/10: Train Loss = 0.6610, Valid Loss = 0.6623\n",
      "Iteration 4/10: Train Loss = 0.6512, Valid Loss = 0.6528\n",
      "Iteration 5/10: Train Loss = 0.6422, Valid Loss = 0.6441\n",
      "Iteration 6/10: Train Loss = 0.6335, Valid Loss = 0.6357\n",
      "Iteration 7/10: Train Loss = 0.6252, Valid Loss = 0.6277\n",
      "Iteration 8/10: Train Loss = 0.6172, Valid Loss = 0.6200\n",
      "Iteration 9/10: Train Loss = 0.6096, Valid Loss = 0.6127\n",
      "Iteration 10/10: Train Loss = 0.6023, Valid Loss = 0.6055\n",
      "Iteration 1/10: Train Loss = 0.6789, Valid Loss = 0.6793\n",
      "Iteration 2/10: Train Loss = 0.6654, Valid Loss = 0.6662\n",
      "Iteration 3/10: Train Loss = 0.6524, Valid Loss = 0.6534\n",
      "Iteration 4/10: Train Loss = 0.6400, Valid Loss = 0.6413\n",
      "Iteration 5/10: Train Loss = 0.6283, Valid Loss = 0.6297\n",
      "Iteration 6/10: Train Loss = 0.6169, Valid Loss = 0.6186\n",
      "Iteration 7/10: Train Loss = 0.6060, Valid Loss = 0.6079\n",
      "Iteration 8/10: Train Loss = 0.5958, Valid Loss = 0.5979\n",
      "Iteration 9/10: Train Loss = 0.5861, Valid Loss = 0.5884\n",
      "Iteration 10/10: Train Loss = 0.5767, Valid Loss = 0.5792\n",
      "Iteration 1/10: Train Loss = 0.6772, Valid Loss = 0.6775\n",
      "Iteration 2/10: Train Loss = 0.6622, Valid Loss = 0.6630\n",
      "Iteration 3/10: Train Loss = 0.6479, Valid Loss = 0.6489\n",
      "Iteration 4/10: Train Loss = 0.6343, Valid Loss = 0.6357\n",
      "Iteration 5/10: Train Loss = 0.6213, Valid Loss = 0.6230\n",
      "Iteration 6/10: Train Loss = 0.6089, Valid Loss = 0.6111\n",
      "Iteration 7/10: Train Loss = 0.5967, Valid Loss = 0.5992\n",
      "Iteration 8/10: Train Loss = 0.5855, Valid Loss = 0.5881\n",
      "Iteration 9/10: Train Loss = 0.5748, Valid Loss = 0.5775\n",
      "Iteration 10/10: Train Loss = 0.5644, Valid Loss = 0.5675\n",
      "Iteration 1/10: Train Loss = 0.6764, Valid Loss = 0.6773\n",
      "Iteration 2/10: Train Loss = 0.6601, Valid Loss = 0.6617\n",
      "Iteration 3/10: Train Loss = 0.6447, Valid Loss = 0.6470\n",
      "Iteration 4/10: Train Loss = 0.6301, Valid Loss = 0.6327\n",
      "Iteration 5/10: Train Loss = 0.6159, Valid Loss = 0.6189\n",
      "Iteration 6/10: Train Loss = 0.6024, Valid Loss = 0.6058\n",
      "Iteration 7/10: Train Loss = 0.5900, Valid Loss = 0.5938\n",
      "Iteration 8/10: Train Loss = 0.5779, Valid Loss = 0.5821\n",
      "Iteration 9/10: Train Loss = 0.5666, Valid Loss = 0.5714\n",
      "Iteration 10/10: Train Loss = 0.5556, Valid Loss = 0.5609\n",
      "Iteration 1/10: Train Loss = 0.6755, Valid Loss = 0.6759\n",
      "Iteration 2/10: Train Loss = 0.6587, Valid Loss = 0.6601\n",
      "Iteration 3/10: Train Loss = 0.6429, Valid Loss = 0.6453\n",
      "Iteration 4/10: Train Loss = 0.6278, Valid Loss = 0.6309\n",
      "Iteration 5/10: Train Loss = 0.6130, Valid Loss = 0.6168\n",
      "Iteration 6/10: Train Loss = 0.5993, Valid Loss = 0.6040\n",
      "Iteration 7/10: Train Loss = 0.5862, Valid Loss = 0.5917\n",
      "Iteration 8/10: Train Loss = 0.5737, Valid Loss = 0.5799\n",
      "Iteration 9/10: Train Loss = 0.5617, Valid Loss = 0.5684\n",
      "Iteration 10/10: Train Loss = 0.5502, Valid Loss = 0.5576\n",
      "Iteration 1/10: Train Loss = 0.6750, Valid Loss = 0.6762\n",
      "Iteration 2/10: Train Loss = 0.6581, Valid Loss = 0.6597\n",
      "Iteration 3/10: Train Loss = 0.6414, Valid Loss = 0.6436\n",
      "Iteration 4/10: Train Loss = 0.6259, Valid Loss = 0.6288\n",
      "Iteration 5/10: Train Loss = 0.6109, Valid Loss = 0.6146\n",
      "Iteration 6/10: Train Loss = 0.5964, Valid Loss = 0.6010\n",
      "Iteration 7/10: Train Loss = 0.5827, Valid Loss = 0.5882\n",
      "Iteration 8/10: Train Loss = 0.5696, Valid Loss = 0.5761\n",
      "Iteration 9/10: Train Loss = 0.5571, Valid Loss = 0.5645\n",
      "Iteration 10/10: Train Loss = 0.5452, Valid Loss = 0.5536\n",
      "Iteration 1/10: Train Loss = 0.6745, Valid Loss = 0.6760\n",
      "Iteration 2/10: Train Loss = 0.6568, Valid Loss = 0.6597\n",
      "Iteration 3/10: Train Loss = 0.6400, Valid Loss = 0.6440\n",
      "Iteration 4/10: Train Loss = 0.6238, Valid Loss = 0.6296\n",
      "Iteration 5/10: Train Loss = 0.6083, Valid Loss = 0.6153\n",
      "Iteration 6/10: Train Loss = 0.5937, Valid Loss = 0.6021\n",
      "Iteration 7/10: Train Loss = 0.5796, Valid Loss = 0.5888\n",
      "Iteration 8/10: Train Loss = 0.5664, Valid Loss = 0.5766\n",
      "Iteration 9/10: Train Loss = 0.5535, Valid Loss = 0.5649\n",
      "Iteration 10/10: Train Loss = 0.5412, Valid Loss = 0.5539\n",
      "Iteration 1/10: Train Loss = 0.6742, Valid Loss = 0.6757\n",
      "Iteration 2/10: Train Loss = 0.6562, Valid Loss = 0.6592\n",
      "Iteration 3/10: Train Loss = 0.6391, Valid Loss = 0.6438\n",
      "Iteration 4/10: Train Loss = 0.6226, Valid Loss = 0.6285\n",
      "Iteration 5/10: Train Loss = 0.6070, Valid Loss = 0.6145\n",
      "Iteration 6/10: Train Loss = 0.5922, Valid Loss = 0.6010\n",
      "Iteration 7/10: Train Loss = 0.5783, Valid Loss = 0.5885\n",
      "Iteration 8/10: Train Loss = 0.5647, Valid Loss = 0.5764\n",
      "Iteration 9/10: Train Loss = 0.5518, Valid Loss = 0.5647\n",
      "Iteration 10/10: Train Loss = 0.5393, Valid Loss = 0.5528\n",
      "Iteration 1/10: Train Loss = 0.6739, Valid Loss = 0.6756\n",
      "Iteration 2/10: Train Loss = 0.6557, Valid Loss = 0.6589\n",
      "Iteration 3/10: Train Loss = 0.6382, Valid Loss = 0.6429\n",
      "Iteration 4/10: Train Loss = 0.6216, Valid Loss = 0.6280\n",
      "Iteration 5/10: Train Loss = 0.6061, Valid Loss = 0.6141\n",
      "Iteration 6/10: Train Loss = 0.5911, Valid Loss = 0.6005\n",
      "Iteration 7/10: Train Loss = 0.5767, Valid Loss = 0.5877\n",
      "Iteration 8/10: Train Loss = 0.5630, Valid Loss = 0.5756\n",
      "Iteration 9/10: Train Loss = 0.5499, Valid Loss = 0.5635\n",
      "Iteration 10/10: Train Loss = 0.5375, Valid Loss = 0.5518\n",
      "Iteration 1/10: Train Loss = 0.6738, Valid Loss = 0.6761\n",
      "Iteration 2/10: Train Loss = 0.6555, Valid Loss = 0.6593\n",
      "Iteration 3/10: Train Loss = 0.6380, Valid Loss = 0.6434\n",
      "Iteration 4/10: Train Loss = 0.6212, Valid Loss = 0.6288\n",
      "Iteration 5/10: Train Loss = 0.6053, Valid Loss = 0.6143\n",
      "Iteration 6/10: Train Loss = 0.5902, Valid Loss = 0.6004\n",
      "Iteration 7/10: Train Loss = 0.5757, Valid Loss = 0.5877\n",
      "Iteration 8/10: Train Loss = 0.5619, Valid Loss = 0.5755\n",
      "Iteration 9/10: Train Loss = 0.5486, Valid Loss = 0.5637\n",
      "Iteration 10/10: Train Loss = 0.5361, Valid Loss = 0.5523\n",
      "Iteration 1/10: Train Loss = 0.6740, Valid Loss = 0.6758\n",
      "Iteration 2/10: Train Loss = 0.6555, Valid Loss = 0.6590\n",
      "Iteration 3/10: Train Loss = 0.6378, Valid Loss = 0.6434\n",
      "Iteration 4/10: Train Loss = 0.6210, Valid Loss = 0.6282\n",
      "Iteration 5/10: Train Loss = 0.6052, Valid Loss = 0.6140\n",
      "Iteration 6/10: Train Loss = 0.5899, Valid Loss = 0.6005\n",
      "Iteration 7/10: Train Loss = 0.5755, Valid Loss = 0.5874\n",
      "Iteration 8/10: Train Loss = 0.5616, Valid Loss = 0.5751\n",
      "Iteration 9/10: Train Loss = 0.5482, Valid Loss = 0.5631\n",
      "Iteration 10/10: Train Loss = 0.5357, Valid Loss = 0.5521\n",
      "Iteration 1/10: Train Loss = 0.6738, Valid Loss = 0.6756\n",
      "Iteration 2/10: Train Loss = 0.6553, Valid Loss = 0.6587\n",
      "Iteration 3/10: Train Loss = 0.6376, Valid Loss = 0.6426\n",
      "Iteration 4/10: Train Loss = 0.6209, Valid Loss = 0.6273\n",
      "Iteration 5/10: Train Loss = 0.6048, Valid Loss = 0.6126\n",
      "Iteration 6/10: Train Loss = 0.5894, Valid Loss = 0.5990\n",
      "Iteration 7/10: Train Loss = 0.5748, Valid Loss = 0.5856\n",
      "Iteration 8/10: Train Loss = 0.5608, Valid Loss = 0.5731\n",
      "Iteration 9/10: Train Loss = 0.5476, Valid Loss = 0.5618\n",
      "Iteration 10/10: Train Loss = 0.5347, Valid Loss = 0.5503\n",
      "Iteration 1/10: Train Loss = 0.6737, Valid Loss = 0.6758\n",
      "Iteration 2/10: Train Loss = 0.6552, Valid Loss = 0.6592\n",
      "Iteration 3/10: Train Loss = 0.6377, Valid Loss = 0.6435\n",
      "Iteration 4/10: Train Loss = 0.6209, Valid Loss = 0.6289\n",
      "Iteration 5/10: Train Loss = 0.6047, Valid Loss = 0.6141\n",
      "Iteration 6/10: Train Loss = 0.5893, Valid Loss = 0.6007\n",
      "Iteration 7/10: Train Loss = 0.5746, Valid Loss = 0.5886\n",
      "Iteration 8/10: Train Loss = 0.5606, Valid Loss = 0.5760\n",
      "Iteration 9/10: Train Loss = 0.5473, Valid Loss = 0.5644\n",
      "Iteration 10/10: Train Loss = 0.5345, Valid Loss = 0.5531\n",
      "Iteration 1/10: Train Loss = 0.6738, Valid Loss = 0.6763\n",
      "Iteration 2/10: Train Loss = 0.6553, Valid Loss = 0.6594\n",
      "Iteration 3/10: Train Loss = 0.6378, Valid Loss = 0.6432\n",
      "Iteration 4/10: Train Loss = 0.6210, Valid Loss = 0.6287\n",
      "Iteration 5/10: Train Loss = 0.6052, Valid Loss = 0.6147\n",
      "Iteration 6/10: Train Loss = 0.5901, Valid Loss = 0.6015\n",
      "Iteration 7/10: Train Loss = 0.5754, Valid Loss = 0.5886\n",
      "Iteration 8/10: Train Loss = 0.5613, Valid Loss = 0.5765\n",
      "Iteration 9/10: Train Loss = 0.5477, Valid Loss = 0.5645\n",
      "Iteration 10/10: Train Loss = 0.5347, Valid Loss = 0.5531\n",
      "Iteration 1/10: Train Loss = 0.6737, Valid Loss = 0.6754\n",
      "Iteration 2/10: Train Loss = 0.6552, Valid Loss = 0.6594\n",
      "Iteration 3/10: Train Loss = 0.6372, Valid Loss = 0.6435\n",
      "Iteration 4/10: Train Loss = 0.6201, Valid Loss = 0.6286\n",
      "Iteration 5/10: Train Loss = 0.6041, Valid Loss = 0.6138\n",
      "Iteration 6/10: Train Loss = 0.5889, Valid Loss = 0.6004\n",
      "Iteration 7/10: Train Loss = 0.5744, Valid Loss = 0.5880\n",
      "Iteration 8/10: Train Loss = 0.5604, Valid Loss = 0.5758\n",
      "Iteration 9/10: Train Loss = 0.5470, Valid Loss = 0.5643\n",
      "Iteration 10/10: Train Loss = 0.5344, Valid Loss = 0.5533\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 800x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "depths = range(1, 30, 2)\n",
    "\n",
    "### ╰( ͡☉ ͜ʖ ͡☉ )つ──☆*:・ﾟ   ฅ^•ﻌ•^ฅ   ʕ•ᴥ•ʔ\n",
    "\n",
    "for d in depths:\n",
    "    boosting = Boosting(\n",
    "        base_model_params={'max_depth': d},\n",
    "        n_estimators=10,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.3,\n",
    "        early_stopping_rounds=None,\n",
    "        plot=False\n",
    "    )\n",
    "\n",
    "    boosting.fit(x_train, y_train, x_valid, y_valid)\n",
    "\n",
    "    train_roc_auc = boosting.score(x_train, y_train)\n",
    "    test_roc_auc = boosting.score(x_test, y_test)\n",
    "\n",
    "    results[d] = (train_roc_auc, test_roc_auc)\n",
    "\n",
    "train_scores = [results[d][0] for d in depths]\n",
    "test_scores = [results[d][1] for d in depths]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(depths, train_scores, marker='o', label='Train ROC-AUC')\n",
    "plt.plot(depths, test_scores, marker='o', label='Test ROC-AUC')\n",
    "plt.xlabel('Max depth')\n",
    "plt.ylabel('ROC-AUC')\n",
    "plt.title('Зависимость качества (ROC-AUC) от максимальной глубины дерева')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bshz0JV6lWlV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Какая из моделей имеет лучшее качество? Как вы можете это объяснить?**\n",
    "\n",
    "╰( ͡☉ ͜ʖ ͡☉ )つ──☆*:・ﾟ   ฅ^•ﻌ•^ฅ   ʕ•ᴥ•ʔ\n",
    "\n",
    "До глубины в 11 деревья учитывают много нелинейностей и взаимодействий признаков - это хорошо. А дальше, модель будто подстраивается под обучающую выборку - оверфиттинг - это видно по тому, что на тестовой после 11 глубины качество падает."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FwaUsmqlWlV"
   },
   "source": [
    "## Задание 3. Подбор гиперпараметров и поиск оптимальной модели [3 балла]\n",
    "\n",
    "Настройте основные гиперпараметры вашей модели градиентного бустинга, используя валидационную выборку. Подберите параметры как для самого бустинга, так и для базовых моделей.\n",
    "\n",
    "**Рекомендации:**\n",
    "- Используйте библиотеки для автоматизированного подбора гиперпараметров, такие как [Hyperopt](https://github.com/hyperopt/hyperopt) или [Optuna](https://optuna.org/).\n",
    "- Подберите все основные параметры, чтобы найти лучшую модель на валидационной выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in d:\\python\\lib\\site-packages (from optuna) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\python\\lib\\site-packages (from optuna) (24.1)\n",
      "Collecting sqlalchemy>=1.4.2 (from optuna)\n",
      "  Downloading SQLAlchemy-2.0.36-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: tqdm in d:\\python\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in d:\\python\\lib\\site-packages (from optuna) (6.0.2)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in d:\\python\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy>=1.4.2->optuna)\n",
      "  Downloading greenlet-3.1.1-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: colorama in d:\\python\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in d:\\python\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
      "Downloading optuna-4.1.0-py3-none-any.whl (364 kB)\n",
      "Downloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
      "Downloading SQLAlchemy-2.0.36-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/2.1 MB 19.4 MB/s eta 0:00:00\n",
      "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading greenlet-3.1.1-cp312-cp312-win_amd64.whl (299 kB)\n",
      "Downloading Mako-1.3.8-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: Mako, greenlet, colorlog, sqlalchemy, alembic, optuna\n",
      "Successfully installed Mako-1.3.8 alembic-1.14.0 colorlog-6.9.0 greenlet-3.1.1 optuna-4.1.0 sqlalchemy-2.0.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-04T21:47:13.705242600Z",
     "start_time": "2025-01-04T21:46:59.656752900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rZq0rKpWlWlV",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-01-04T21:53:21.919317500Z",
     "start_time": "2025-01-04T21:48:22.710803600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:48:23,085] A new study created in memory with name: no-name-6b272efa-15cd-4516-9a37-038402ab0a97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/100: Train Loss = 0.6613, Valid Loss = 0.6622\n",
      "Iteration 2/100: Train Loss = 0.6324, Valid Loss = 0.6340\n",
      "Iteration 3/100: Train Loss = 0.6060, Valid Loss = 0.6080\n",
      "Iteration 4/100: Train Loss = 0.5824, Valid Loss = 0.5849\n",
      "Iteration 5/100: Train Loss = 0.5606, Valid Loss = 0.5642\n",
      "Iteration 6/100: Train Loss = 0.5405, Valid Loss = 0.5448\n",
      "Iteration 7/100: Train Loss = 0.5224, Valid Loss = 0.5273\n",
      "Iteration 8/100: Train Loss = 0.5059, Valid Loss = 0.5113\n",
      "Iteration 9/100: Train Loss = 0.4908, Valid Loss = 0.4966\n",
      "Iteration 10/100: Train Loss = 0.4770, Valid Loss = 0.4832\n",
      "Iteration 11/100: Train Loss = 0.4641, Valid Loss = 0.4707\n",
      "Iteration 12/100: Train Loss = 0.4526, Valid Loss = 0.4594\n",
      "Iteration 13/100: Train Loss = 0.4414, Valid Loss = 0.4486\n",
      "Iteration 14/100: Train Loss = 0.4311, Valid Loss = 0.4385\n",
      "Iteration 15/100: Train Loss = 0.4217, Valid Loss = 0.4292\n",
      "Iteration 16/100: Train Loss = 0.4129, Valid Loss = 0.4205\n",
      "Iteration 17/100: Train Loss = 0.4047, Valid Loss = 0.4129\n",
      "Iteration 18/100: Train Loss = 0.3971, Valid Loss = 0.4055\n",
      "Iteration 19/100: Train Loss = 0.3899, Valid Loss = 0.3987\n",
      "Iteration 20/100: Train Loss = 0.3832, Valid Loss = 0.3927\n",
      "Iteration 21/100: Train Loss = 0.3769, Valid Loss = 0.3867\n",
      "Iteration 22/100: Train Loss = 0.3709, Valid Loss = 0.3812\n",
      "Iteration 23/100: Train Loss = 0.3655, Valid Loss = 0.3762\n",
      "Iteration 24/100: Train Loss = 0.3604, Valid Loss = 0.3714\n",
      "Iteration 25/100: Train Loss = 0.3555, Valid Loss = 0.3668\n",
      "Iteration 26/100: Train Loss = 0.3506, Valid Loss = 0.3621\n",
      "Iteration 27/100: Train Loss = 0.3462, Valid Loss = 0.3578\n",
      "Iteration 28/100: Train Loss = 0.3421, Valid Loss = 0.3537\n",
      "Iteration 29/100: Train Loss = 0.3379, Valid Loss = 0.3501\n",
      "Iteration 30/100: Train Loss = 0.3342, Valid Loss = 0.3465\n",
      "Iteration 31/100: Train Loss = 0.3305, Valid Loss = 0.3432\n",
      "Iteration 32/100: Train Loss = 0.3267, Valid Loss = 0.3399\n",
      "Iteration 33/100: Train Loss = 0.3233, Valid Loss = 0.3369\n",
      "Iteration 34/100: Train Loss = 0.3201, Valid Loss = 0.3340\n",
      "Iteration 35/100: Train Loss = 0.3172, Valid Loss = 0.3311\n",
      "Iteration 36/100: Train Loss = 0.3143, Valid Loss = 0.3283\n",
      "Iteration 37/100: Train Loss = 0.3115, Valid Loss = 0.3257\n",
      "Iteration 38/100: Train Loss = 0.3090, Valid Loss = 0.3233\n",
      "Iteration 39/100: Train Loss = 0.3065, Valid Loss = 0.3210\n",
      "Iteration 40/100: Train Loss = 0.3040, Valid Loss = 0.3188\n",
      "Iteration 41/100: Train Loss = 0.3018, Valid Loss = 0.3169\n",
      "Iteration 42/100: Train Loss = 0.2994, Valid Loss = 0.3149\n",
      "Iteration 43/100: Train Loss = 0.2974, Valid Loss = 0.3132\n",
      "Iteration 44/100: Train Loss = 0.2954, Valid Loss = 0.3113\n",
      "Iteration 45/100: Train Loss = 0.2935, Valid Loss = 0.3096\n",
      "Iteration 46/100: Train Loss = 0.2915, Valid Loss = 0.3079\n",
      "Iteration 47/100: Train Loss = 0.2897, Valid Loss = 0.3065\n",
      "Iteration 48/100: Train Loss = 0.2879, Valid Loss = 0.3045\n",
      "Iteration 49/100: Train Loss = 0.2861, Valid Loss = 0.3030\n",
      "Iteration 50/100: Train Loss = 0.2844, Valid Loss = 0.3015\n",
      "Iteration 51/100: Train Loss = 0.2827, Valid Loss = 0.2999\n",
      "Iteration 52/100: Train Loss = 0.2811, Valid Loss = 0.2985\n",
      "Iteration 53/100: Train Loss = 0.2796, Valid Loss = 0.2972\n",
      "Iteration 54/100: Train Loss = 0.2782, Valid Loss = 0.2959\n",
      "Iteration 55/100: Train Loss = 0.2768, Valid Loss = 0.2946\n",
      "Iteration 56/100: Train Loss = 0.2754, Valid Loss = 0.2934\n",
      "Iteration 57/100: Train Loss = 0.2742, Valid Loss = 0.2923\n",
      "Iteration 58/100: Train Loss = 0.2729, Valid Loss = 0.2912\n",
      "Iteration 59/100: Train Loss = 0.2716, Valid Loss = 0.2901\n",
      "Iteration 60/100: Train Loss = 0.2704, Valid Loss = 0.2892\n",
      "Iteration 61/100: Train Loss = 0.2692, Valid Loss = 0.2881\n",
      "Iteration 62/100: Train Loss = 0.2681, Valid Loss = 0.2874\n",
      "Iteration 63/100: Train Loss = 0.2670, Valid Loss = 0.2864\n",
      "Iteration 64/100: Train Loss = 0.2659, Valid Loss = 0.2854\n",
      "Iteration 65/100: Train Loss = 0.2647, Valid Loss = 0.2845\n",
      "Iteration 66/100: Train Loss = 0.2636, Valid Loss = 0.2835\n",
      "Iteration 67/100: Train Loss = 0.2625, Valid Loss = 0.2827\n",
      "Iteration 68/100: Train Loss = 0.2615, Valid Loss = 0.2818\n",
      "Iteration 69/100: Train Loss = 0.2606, Valid Loss = 0.2811\n",
      "Iteration 70/100: Train Loss = 0.2597, Valid Loss = 0.2804\n",
      "Iteration 71/100: Train Loss = 0.2587, Valid Loss = 0.2796\n",
      "Iteration 72/100: Train Loss = 0.2577, Valid Loss = 0.2786\n",
      "Iteration 73/100: Train Loss = 0.2569, Valid Loss = 0.2781\n",
      "Iteration 74/100: Train Loss = 0.2559, Valid Loss = 0.2773\n",
      "Iteration 75/100: Train Loss = 0.2551, Valid Loss = 0.2767\n",
      "Iteration 76/100: Train Loss = 0.2544, Valid Loss = 0.2760\n",
      "Iteration 77/100: Train Loss = 0.2535, Valid Loss = 0.2754\n",
      "Iteration 78/100: Train Loss = 0.2527, Valid Loss = 0.2747\n",
      "Iteration 79/100: Train Loss = 0.2519, Valid Loss = 0.2742\n",
      "Iteration 80/100: Train Loss = 0.2512, Valid Loss = 0.2736\n",
      "Iteration 81/100: Train Loss = 0.2505, Valid Loss = 0.2731\n",
      "Iteration 82/100: Train Loss = 0.2497, Valid Loss = 0.2724\n",
      "Iteration 83/100: Train Loss = 0.2490, Valid Loss = 0.2719\n",
      "Iteration 84/100: Train Loss = 0.2482, Valid Loss = 0.2713\n",
      "Iteration 85/100: Train Loss = 0.2475, Valid Loss = 0.2706\n",
      "Iteration 86/100: Train Loss = 0.2467, Valid Loss = 0.2701\n",
      "Iteration 87/100: Train Loss = 0.2460, Valid Loss = 0.2696\n",
      "Iteration 88/100: Train Loss = 0.2454, Valid Loss = 0.2691\n",
      "Iteration 89/100: Train Loss = 0.2448, Valid Loss = 0.2685\n",
      "Iteration 90/100: Train Loss = 0.2441, Valid Loss = 0.2681\n",
      "Iteration 91/100: Train Loss = 0.2434, Valid Loss = 0.2677\n",
      "Iteration 92/100: Train Loss = 0.2428, Valid Loss = 0.2672\n",
      "Iteration 93/100: Train Loss = 0.2422, Valid Loss = 0.2667\n",
      "Iteration 94/100: Train Loss = 0.2416, Valid Loss = 0.2664\n",
      "Iteration 95/100: Train Loss = 0.2409, Valid Loss = 0.2659\n",
      "Iteration 96/100: Train Loss = 0.2402, Valid Loss = 0.2654\n",
      "Iteration 97/100: Train Loss = 0.2397, Valid Loss = 0.2650\n",
      "Iteration 98/100: Train Loss = 0.2391, Valid Loss = 0.2645\n",
      "Iteration 99/100: Train Loss = 0.2386, Valid Loss = 0.2641\n",
      "Iteration 100/100: Train Loss = 0.2379, Valid Loss = 0.2638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:48:28,020] Trial 0 finished with value: 0.9634116117421425 and parameters: {'max_depth': 6, 'min_samples_leaf': 15, 'n_estimators': 100, 'learning_rate': 0.1963547445574364, 'subsample': 0.891286506399722}. Best is trial 0 with value: 0.9634116117421425.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/60: Train Loss = 0.6588, Valid Loss = 0.6592\n",
      "Iteration 2/60: Train Loss = 0.6287, Valid Loss = 0.6295\n",
      "Iteration 3/60: Train Loss = 0.6022, Valid Loss = 0.6036\n",
      "Iteration 4/60: Train Loss = 0.5785, Valid Loss = 0.5806\n",
      "Iteration 5/60: Train Loss = 0.5577, Valid Loss = 0.5600\n",
      "Iteration 6/60: Train Loss = 0.5387, Valid Loss = 0.5414\n",
      "Iteration 7/60: Train Loss = 0.5224, Valid Loss = 0.5261\n",
      "Iteration 8/60: Train Loss = 0.5078, Valid Loss = 0.5120\n",
      "Iteration 9/60: Train Loss = 0.4947, Valid Loss = 0.4994\n",
      "Iteration 10/60: Train Loss = 0.4830, Valid Loss = 0.4882\n",
      "Iteration 11/60: Train Loss = 0.4724, Valid Loss = 0.4778\n",
      "Iteration 12/60: Train Loss = 0.4628, Valid Loss = 0.4685\n",
      "Iteration 13/60: Train Loss = 0.4534, Valid Loss = 0.4596\n",
      "Iteration 14/60: Train Loss = 0.4451, Valid Loss = 0.4516\n",
      "Iteration 15/60: Train Loss = 0.4376, Valid Loss = 0.4443\n",
      "Iteration 16/60: Train Loss = 0.4308, Valid Loss = 0.4379\n",
      "Iteration 17/60: Train Loss = 0.4244, Valid Loss = 0.4316\n",
      "Iteration 18/60: Train Loss = 0.4181, Valid Loss = 0.4255\n",
      "Iteration 19/60: Train Loss = 0.4129, Valid Loss = 0.4205\n",
      "Iteration 20/60: Train Loss = 0.4076, Valid Loss = 0.4150\n",
      "Iteration 21/60: Train Loss = 0.4026, Valid Loss = 0.4100\n",
      "Iteration 22/60: Train Loss = 0.3981, Valid Loss = 0.4056\n",
      "Iteration 23/60: Train Loss = 0.3938, Valid Loss = 0.4015\n",
      "Iteration 24/60: Train Loss = 0.3899, Valid Loss = 0.3980\n",
      "Iteration 25/60: Train Loss = 0.3863, Valid Loss = 0.3945\n",
      "Iteration 26/60: Train Loss = 0.3825, Valid Loss = 0.3906\n",
      "Iteration 27/60: Train Loss = 0.3791, Valid Loss = 0.3873\n",
      "Iteration 28/60: Train Loss = 0.3759, Valid Loss = 0.3842\n",
      "Iteration 29/60: Train Loss = 0.3728, Valid Loss = 0.3811\n",
      "Iteration 30/60: Train Loss = 0.3698, Valid Loss = 0.3782\n",
      "Iteration 31/60: Train Loss = 0.3671, Valid Loss = 0.3755\n",
      "Iteration 32/60: Train Loss = 0.3645, Valid Loss = 0.3731\n",
      "Iteration 33/60: Train Loss = 0.3621, Valid Loss = 0.3710\n",
      "Iteration 34/60: Train Loss = 0.3597, Valid Loss = 0.3686\n",
      "Iteration 35/60: Train Loss = 0.3577, Valid Loss = 0.3665\n",
      "Iteration 36/60: Train Loss = 0.3554, Valid Loss = 0.3644\n",
      "Iteration 37/60: Train Loss = 0.3532, Valid Loss = 0.3624\n",
      "Iteration 38/60: Train Loss = 0.3514, Valid Loss = 0.3606\n",
      "Iteration 39/60: Train Loss = 0.3492, Valid Loss = 0.3586\n",
      "Iteration 40/60: Train Loss = 0.3473, Valid Loss = 0.3567\n",
      "Iteration 41/60: Train Loss = 0.3456, Valid Loss = 0.3551\n",
      "Iteration 42/60: Train Loss = 0.3436, Valid Loss = 0.3530\n",
      "Iteration 43/60: Train Loss = 0.3418, Valid Loss = 0.3514\n",
      "Iteration 44/60: Train Loss = 0.3402, Valid Loss = 0.3500\n",
      "Iteration 45/60: Train Loss = 0.3387, Valid Loss = 0.3485\n",
      "Iteration 46/60: Train Loss = 0.3372, Valid Loss = 0.3473\n",
      "Iteration 47/60: Train Loss = 0.3356, Valid Loss = 0.3460\n",
      "Iteration 48/60: Train Loss = 0.3342, Valid Loss = 0.3446\n",
      "Iteration 49/60: Train Loss = 0.3329, Valid Loss = 0.3432\n",
      "Iteration 50/60: Train Loss = 0.3314, Valid Loss = 0.3417\n",
      "Iteration 51/60: Train Loss = 0.3301, Valid Loss = 0.3402\n",
      "Iteration 52/60: Train Loss = 0.3287, Valid Loss = 0.3386\n",
      "Iteration 53/60: Train Loss = 0.3274, Valid Loss = 0.3372\n",
      "Iteration 54/60: Train Loss = 0.3262, Valid Loss = 0.3361\n",
      "Iteration 55/60: Train Loss = 0.3250, Valid Loss = 0.3350\n",
      "Iteration 56/60: Train Loss = 0.3239, Valid Loss = 0.3341\n",
      "Iteration 57/60: Train Loss = 0.3228, Valid Loss = 0.3330\n",
      "Iteration 58/60: Train Loss = 0.3218, Valid Loss = 0.3318\n",
      "Iteration 59/60: Train Loss = 0.3207, Valid Loss = 0.3308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:48:29,350] Trial 1 finished with value: 0.9429677163522948 and parameters: {'max_depth': 2, 'min_samples_leaf': 3, 'n_estimators': 60, 'learning_rate': 0.2666096225887527, 'subsample': 0.3061639888130857}. Best is trial 0 with value: 0.9634116117421425.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 60/60: Train Loss = 0.3197, Valid Loss = 0.3298\n",
      "Iteration 1/90: Train Loss = 0.6855, Valid Loss = 0.6858\n",
      "Iteration 2/90: Train Loss = 0.6780, Valid Loss = 0.6785\n",
      "Iteration 3/90: Train Loss = 0.6707, Valid Loss = 0.6714\n",
      "Iteration 4/90: Train Loss = 0.6635, Valid Loss = 0.6644\n",
      "Iteration 5/90: Train Loss = 0.6565, Valid Loss = 0.6577\n",
      "Iteration 6/90: Train Loss = 0.6497, Valid Loss = 0.6512\n",
      "Iteration 7/90: Train Loss = 0.6429, Valid Loss = 0.6447\n",
      "Iteration 8/90: Train Loss = 0.6364, Valid Loss = 0.6384\n",
      "Iteration 9/90: Train Loss = 0.6300, Valid Loss = 0.6323\n",
      "Iteration 10/90: Train Loss = 0.6237, Valid Loss = 0.6262\n",
      "Iteration 11/90: Train Loss = 0.6175, Valid Loss = 0.6202\n",
      "Iteration 12/90: Train Loss = 0.6114, Valid Loss = 0.6144\n",
      "Iteration 13/90: Train Loss = 0.6054, Valid Loss = 0.6087\n",
      "Iteration 14/90: Train Loss = 0.5996, Valid Loss = 0.6032\n",
      "Iteration 15/90: Train Loss = 0.5939, Valid Loss = 0.5976\n",
      "Iteration 16/90: Train Loss = 0.5884, Valid Loss = 0.5923\n",
      "Iteration 17/90: Train Loss = 0.5830, Valid Loss = 0.5871\n",
      "Iteration 18/90: Train Loss = 0.5776, Valid Loss = 0.5820\n",
      "Iteration 19/90: Train Loss = 0.5723, Valid Loss = 0.5769\n",
      "Iteration 20/90: Train Loss = 0.5672, Valid Loss = 0.5719\n",
      "Iteration 21/90: Train Loss = 0.5621, Valid Loss = 0.5671\n",
      "Iteration 22/90: Train Loss = 0.5572, Valid Loss = 0.5624\n",
      "Iteration 23/90: Train Loss = 0.5523, Valid Loss = 0.5577\n",
      "Iteration 24/90: Train Loss = 0.5476, Valid Loss = 0.5532\n",
      "Iteration 25/90: Train Loss = 0.5429, Valid Loss = 0.5488\n",
      "Iteration 26/90: Train Loss = 0.5384, Valid Loss = 0.5444\n",
      "Iteration 27/90: Train Loss = 0.5339, Valid Loss = 0.5401\n",
      "Iteration 28/90: Train Loss = 0.5295, Valid Loss = 0.5360\n",
      "Iteration 29/90: Train Loss = 0.5252, Valid Loss = 0.5319\n",
      "Iteration 30/90: Train Loss = 0.5210, Valid Loss = 0.5279\n",
      "Iteration 31/90: Train Loss = 0.5169, Valid Loss = 0.5240\n",
      "Iteration 32/90: Train Loss = 0.5128, Valid Loss = 0.5200\n",
      "Iteration 33/90: Train Loss = 0.5088, Valid Loss = 0.5161\n",
      "Iteration 34/90: Train Loss = 0.5049, Valid Loss = 0.5125\n",
      "Iteration 35/90: Train Loss = 0.5011, Valid Loss = 0.5088\n",
      "Iteration 36/90: Train Loss = 0.4973, Valid Loss = 0.5051\n",
      "Iteration 37/90: Train Loss = 0.4937, Valid Loss = 0.5017\n",
      "Iteration 38/90: Train Loss = 0.4900, Valid Loss = 0.4981\n",
      "Iteration 39/90: Train Loss = 0.4865, Valid Loss = 0.4946\n",
      "Iteration 40/90: Train Loss = 0.4829, Valid Loss = 0.4913\n",
      "Iteration 41/90: Train Loss = 0.4795, Valid Loss = 0.4881\n",
      "Iteration 42/90: Train Loss = 0.4761, Valid Loss = 0.4848\n",
      "Iteration 43/90: Train Loss = 0.4728, Valid Loss = 0.4817\n",
      "Iteration 44/90: Train Loss = 0.4695, Valid Loss = 0.4786\n",
      "Iteration 45/90: Train Loss = 0.4663, Valid Loss = 0.4756\n",
      "Iteration 46/90: Train Loss = 0.4632, Valid Loss = 0.4726\n",
      "Iteration 47/90: Train Loss = 0.4601, Valid Loss = 0.4697\n",
      "Iteration 48/90: Train Loss = 0.4571, Valid Loss = 0.4669\n",
      "Iteration 49/90: Train Loss = 0.4542, Valid Loss = 0.4641\n",
      "Iteration 50/90: Train Loss = 0.4512, Valid Loss = 0.4613\n",
      "Iteration 51/90: Train Loss = 0.4483, Valid Loss = 0.4586\n",
      "Iteration 52/90: Train Loss = 0.4456, Valid Loss = 0.4561\n",
      "Iteration 53/90: Train Loss = 0.4428, Valid Loss = 0.4534\n",
      "Iteration 54/90: Train Loss = 0.4401, Valid Loss = 0.4509\n",
      "Iteration 55/90: Train Loss = 0.4374, Valid Loss = 0.4484\n",
      "Iteration 56/90: Train Loss = 0.4348, Valid Loss = 0.4460\n",
      "Iteration 57/90: Train Loss = 0.4322, Valid Loss = 0.4435\n",
      "Iteration 58/90: Train Loss = 0.4296, Valid Loss = 0.4411\n",
      "Iteration 59/90: Train Loss = 0.4271, Valid Loss = 0.4389\n",
      "Iteration 60/90: Train Loss = 0.4246, Valid Loss = 0.4365\n",
      "Iteration 61/90: Train Loss = 0.4222, Valid Loss = 0.4343\n",
      "Iteration 62/90: Train Loss = 0.4198, Valid Loss = 0.4321\n",
      "Iteration 63/90: Train Loss = 0.4175, Valid Loss = 0.4299\n",
      "Iteration 64/90: Train Loss = 0.4152, Valid Loss = 0.4278\n",
      "Iteration 65/90: Train Loss = 0.4129, Valid Loss = 0.4257\n",
      "Iteration 66/90: Train Loss = 0.4107, Valid Loss = 0.4236\n",
      "Iteration 67/90: Train Loss = 0.4085, Valid Loss = 0.4216\n",
      "Iteration 68/90: Train Loss = 0.4063, Valid Loss = 0.4196\n",
      "Iteration 69/90: Train Loss = 0.4041, Valid Loss = 0.4175\n",
      "Iteration 70/90: Train Loss = 0.4020, Valid Loss = 0.4156\n",
      "Iteration 71/90: Train Loss = 0.4000, Valid Loss = 0.4137\n",
      "Iteration 72/90: Train Loss = 0.3980, Valid Loss = 0.4119\n",
      "Iteration 73/90: Train Loss = 0.3960, Valid Loss = 0.4101\n",
      "Iteration 74/90: Train Loss = 0.3940, Valid Loss = 0.4082\n",
      "Iteration 75/90: Train Loss = 0.3921, Valid Loss = 0.4065\n",
      "Iteration 76/90: Train Loss = 0.3902, Valid Loss = 0.4047\n",
      "Iteration 77/90: Train Loss = 0.3883, Valid Loss = 0.4030\n",
      "Iteration 78/90: Train Loss = 0.3864, Valid Loss = 0.4013\n",
      "Iteration 79/90: Train Loss = 0.3846, Valid Loss = 0.3996\n",
      "Iteration 80/90: Train Loss = 0.3828, Valid Loss = 0.3980\n",
      "Iteration 81/90: Train Loss = 0.3810, Valid Loss = 0.3963\n",
      "Iteration 82/90: Train Loss = 0.3793, Valid Loss = 0.3947\n",
      "Iteration 83/90: Train Loss = 0.3775, Valid Loss = 0.3932\n",
      "Iteration 84/90: Train Loss = 0.3758, Valid Loss = 0.3916\n",
      "Iteration 85/90: Train Loss = 0.3742, Valid Loss = 0.3901\n",
      "Iteration 86/90: Train Loss = 0.3725, Valid Loss = 0.3887\n",
      "Iteration 87/90: Train Loss = 0.3709, Valid Loss = 0.3873\n",
      "Iteration 88/90: Train Loss = 0.3692, Valid Loss = 0.3858\n",
      "Iteration 89/90: Train Loss = 0.3677, Valid Loss = 0.3844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:48:36,924] Trial 2 finished with value: 0.9606518498183163 and parameters: {'max_depth': 13, 'min_samples_leaf': 17, 'n_estimators': 90, 'learning_rate': 0.04367287985868966, 'subsample': 0.5617308329885593}. Best is trial 0 with value: 0.9634116117421425.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 90/90: Train Loss = 0.3661, Valid Loss = 0.3830\n",
      "Iteration 1/135: Train Loss = 0.6927, Valid Loss = 0.6927\n",
      "Iteration 2/135: Train Loss = 0.6922, Valid Loss = 0.6922\n",
      "Iteration 3/135: Train Loss = 0.6917, Valid Loss = 0.6917\n",
      "Iteration 4/135: Train Loss = 0.6912, Valid Loss = 0.6912\n",
      "Iteration 5/135: Train Loss = 0.6907, Valid Loss = 0.6908\n",
      "Iteration 6/135: Train Loss = 0.6902, Valid Loss = 0.6903\n",
      "Iteration 7/135: Train Loss = 0.6897, Valid Loss = 0.6898\n",
      "Iteration 8/135: Train Loss = 0.6892, Valid Loss = 0.6893\n",
      "Iteration 9/135: Train Loss = 0.6887, Valid Loss = 0.6888\n",
      "Iteration 10/135: Train Loss = 0.6883, Valid Loss = 0.6884\n",
      "Iteration 11/135: Train Loss = 0.6878, Valid Loss = 0.6879\n",
      "Iteration 12/135: Train Loss = 0.6873, Valid Loss = 0.6874\n",
      "Iteration 13/135: Train Loss = 0.6868, Valid Loss = 0.6869\n",
      "Iteration 14/135: Train Loss = 0.6863, Valid Loss = 0.6865\n",
      "Iteration 15/135: Train Loss = 0.6859, Valid Loss = 0.6860\n",
      "Iteration 16/135: Train Loss = 0.6854, Valid Loss = 0.6855\n",
      "Iteration 17/135: Train Loss = 0.6849, Valid Loss = 0.6851\n",
      "Iteration 18/135: Train Loss = 0.6844, Valid Loss = 0.6846\n",
      "Iteration 19/135: Train Loss = 0.6839, Valid Loss = 0.6841\n",
      "Iteration 20/135: Train Loss = 0.6835, Valid Loss = 0.6837\n",
      "Iteration 21/135: Train Loss = 0.6830, Valid Loss = 0.6832\n",
      "Iteration 22/135: Train Loss = 0.6825, Valid Loss = 0.6827\n",
      "Iteration 23/135: Train Loss = 0.6820, Valid Loss = 0.6823\n",
      "Iteration 24/135: Train Loss = 0.6816, Valid Loss = 0.6818\n",
      "Iteration 25/135: Train Loss = 0.6811, Valid Loss = 0.6813\n",
      "Iteration 26/135: Train Loss = 0.6806, Valid Loss = 0.6809\n",
      "Iteration 27/135: Train Loss = 0.6802, Valid Loss = 0.6804\n",
      "Iteration 28/135: Train Loss = 0.6797, Valid Loss = 0.6800\n",
      "Iteration 29/135: Train Loss = 0.6792, Valid Loss = 0.6795\n",
      "Iteration 30/135: Train Loss = 0.6787, Valid Loss = 0.6790\n",
      "Iteration 31/135: Train Loss = 0.6783, Valid Loss = 0.6786\n",
      "Iteration 32/135: Train Loss = 0.6778, Valid Loss = 0.6781\n",
      "Iteration 33/135: Train Loss = 0.6773, Valid Loss = 0.6777\n",
      "Iteration 34/135: Train Loss = 0.6769, Valid Loss = 0.6772\n",
      "Iteration 35/135: Train Loss = 0.6764, Valid Loss = 0.6768\n",
      "Iteration 36/135: Train Loss = 0.6760, Valid Loss = 0.6763\n",
      "Iteration 37/135: Train Loss = 0.6755, Valid Loss = 0.6759\n",
      "Iteration 38/135: Train Loss = 0.6750, Valid Loss = 0.6754\n",
      "Iteration 39/135: Train Loss = 0.6746, Valid Loss = 0.6750\n",
      "Iteration 40/135: Train Loss = 0.6741, Valid Loss = 0.6745\n",
      "Iteration 41/135: Train Loss = 0.6736, Valid Loss = 0.6741\n",
      "Iteration 42/135: Train Loss = 0.6732, Valid Loss = 0.6736\n",
      "Iteration 43/135: Train Loss = 0.6727, Valid Loss = 0.6732\n",
      "Iteration 44/135: Train Loss = 0.6723, Valid Loss = 0.6727\n",
      "Iteration 45/135: Train Loss = 0.6718, Valid Loss = 0.6723\n",
      "Iteration 46/135: Train Loss = 0.6713, Valid Loss = 0.6718\n",
      "Iteration 47/135: Train Loss = 0.6709, Valid Loss = 0.6714\n",
      "Iteration 48/135: Train Loss = 0.6704, Valid Loss = 0.6709\n",
      "Iteration 49/135: Train Loss = 0.6700, Valid Loss = 0.6705\n",
      "Iteration 50/135: Train Loss = 0.6695, Valid Loss = 0.6700\n",
      "Iteration 51/135: Train Loss = 0.6691, Valid Loss = 0.6696\n",
      "Iteration 52/135: Train Loss = 0.6686, Valid Loss = 0.6691\n",
      "Iteration 53/135: Train Loss = 0.6681, Valid Loss = 0.6687\n",
      "Iteration 54/135: Train Loss = 0.6677, Valid Loss = 0.6683\n",
      "Iteration 55/135: Train Loss = 0.6672, Valid Loss = 0.6678\n",
      "Iteration 56/135: Train Loss = 0.6668, Valid Loss = 0.6674\n",
      "Iteration 57/135: Train Loss = 0.6663, Valid Loss = 0.6669\n",
      "Iteration 58/135: Train Loss = 0.6659, Valid Loss = 0.6665\n",
      "Iteration 59/135: Train Loss = 0.6654, Valid Loss = 0.6661\n",
      "Iteration 60/135: Train Loss = 0.6650, Valid Loss = 0.6656\n",
      "Iteration 61/135: Train Loss = 0.6645, Valid Loss = 0.6652\n",
      "Iteration 62/135: Train Loss = 0.6641, Valid Loss = 0.6648\n",
      "Iteration 63/135: Train Loss = 0.6637, Valid Loss = 0.6643\n",
      "Iteration 64/135: Train Loss = 0.6632, Valid Loss = 0.6639\n",
      "Iteration 65/135: Train Loss = 0.6628, Valid Loss = 0.6634\n",
      "Iteration 66/135: Train Loss = 0.6623, Valid Loss = 0.6630\n",
      "Iteration 67/135: Train Loss = 0.6619, Valid Loss = 0.6626\n",
      "Iteration 68/135: Train Loss = 0.6614, Valid Loss = 0.6621\n",
      "Iteration 69/135: Train Loss = 0.6610, Valid Loss = 0.6617\n",
      "Iteration 70/135: Train Loss = 0.6606, Valid Loss = 0.6613\n",
      "Iteration 71/135: Train Loss = 0.6601, Valid Loss = 0.6609\n",
      "Iteration 72/135: Train Loss = 0.6597, Valid Loss = 0.6604\n",
      "Iteration 73/135: Train Loss = 0.6592, Valid Loss = 0.6600\n",
      "Iteration 74/135: Train Loss = 0.6588, Valid Loss = 0.6596\n",
      "Iteration 75/135: Train Loss = 0.6584, Valid Loss = 0.6591\n",
      "Iteration 76/135: Train Loss = 0.6579, Valid Loss = 0.6587\n",
      "Iteration 77/135: Train Loss = 0.6575, Valid Loss = 0.6583\n",
      "Iteration 78/135: Train Loss = 0.6571, Valid Loss = 0.6579\n",
      "Iteration 79/135: Train Loss = 0.6566, Valid Loss = 0.6574\n",
      "Iteration 80/135: Train Loss = 0.6562, Valid Loss = 0.6570\n",
      "Iteration 81/135: Train Loss = 0.6557, Valid Loss = 0.6566\n",
      "Iteration 82/135: Train Loss = 0.6553, Valid Loss = 0.6562\n",
      "Iteration 83/135: Train Loss = 0.6549, Valid Loss = 0.6557\n",
      "Iteration 84/135: Train Loss = 0.6544, Valid Loss = 0.6553\n",
      "Iteration 85/135: Train Loss = 0.6540, Valid Loss = 0.6549\n",
      "Iteration 86/135: Train Loss = 0.6536, Valid Loss = 0.6545\n",
      "Iteration 87/135: Train Loss = 0.6532, Valid Loss = 0.6541\n",
      "Iteration 88/135: Train Loss = 0.6527, Valid Loss = 0.6536\n",
      "Iteration 89/135: Train Loss = 0.6523, Valid Loss = 0.6532\n",
      "Iteration 90/135: Train Loss = 0.6519, Valid Loss = 0.6528\n",
      "Iteration 91/135: Train Loss = 0.6514, Valid Loss = 0.6524\n",
      "Iteration 92/135: Train Loss = 0.6510, Valid Loss = 0.6520\n",
      "Iteration 93/135: Train Loss = 0.6506, Valid Loss = 0.6515\n",
      "Iteration 94/135: Train Loss = 0.6502, Valid Loss = 0.6511\n",
      "Iteration 95/135: Train Loss = 0.6497, Valid Loss = 0.6507\n",
      "Iteration 96/135: Train Loss = 0.6493, Valid Loss = 0.6503\n",
      "Iteration 97/135: Train Loss = 0.6489, Valid Loss = 0.6499\n",
      "Iteration 98/135: Train Loss = 0.6485, Valid Loss = 0.6495\n",
      "Iteration 99/135: Train Loss = 0.6481, Valid Loss = 0.6491\n",
      "Iteration 100/135: Train Loss = 0.6476, Valid Loss = 0.6486\n",
      "Iteration 101/135: Train Loss = 0.6472, Valid Loss = 0.6482\n",
      "Iteration 102/135: Train Loss = 0.6468, Valid Loss = 0.6478\n",
      "Iteration 103/135: Train Loss = 0.6464, Valid Loss = 0.6474\n",
      "Iteration 104/135: Train Loss = 0.6460, Valid Loss = 0.6470\n",
      "Iteration 105/135: Train Loss = 0.6455, Valid Loss = 0.6466\n",
      "Iteration 106/135: Train Loss = 0.6451, Valid Loss = 0.6462\n",
      "Iteration 107/135: Train Loss = 0.6447, Valid Loss = 0.6458\n",
      "Iteration 108/135: Train Loss = 0.6443, Valid Loss = 0.6454\n",
      "Iteration 109/135: Train Loss = 0.6439, Valid Loss = 0.6450\n",
      "Iteration 110/135: Train Loss = 0.6435, Valid Loss = 0.6446\n",
      "Iteration 111/135: Train Loss = 0.6430, Valid Loss = 0.6442\n",
      "Iteration 112/135: Train Loss = 0.6426, Valid Loss = 0.6437\n",
      "Iteration 113/135: Train Loss = 0.6422, Valid Loss = 0.6433\n",
      "Iteration 114/135: Train Loss = 0.6418, Valid Loss = 0.6429\n",
      "Iteration 115/135: Train Loss = 0.6414, Valid Loss = 0.6425\n",
      "Iteration 116/135: Train Loss = 0.6410, Valid Loss = 0.6421\n",
      "Iteration 117/135: Train Loss = 0.6406, Valid Loss = 0.6417\n",
      "Iteration 118/135: Train Loss = 0.6402, Valid Loss = 0.6413\n",
      "Iteration 119/135: Train Loss = 0.6398, Valid Loss = 0.6409\n",
      "Iteration 120/135: Train Loss = 0.6394, Valid Loss = 0.6405\n",
      "Iteration 121/135: Train Loss = 0.6389, Valid Loss = 0.6401\n",
      "Iteration 122/135: Train Loss = 0.6385, Valid Loss = 0.6397\n",
      "Iteration 123/135: Train Loss = 0.6381, Valid Loss = 0.6393\n",
      "Iteration 124/135: Train Loss = 0.6377, Valid Loss = 0.6389\n",
      "Iteration 125/135: Train Loss = 0.6373, Valid Loss = 0.6385\n",
      "Iteration 126/135: Train Loss = 0.6369, Valid Loss = 0.6381\n",
      "Iteration 127/135: Train Loss = 0.6365, Valid Loss = 0.6378\n",
      "Iteration 128/135: Train Loss = 0.6361, Valid Loss = 0.6374\n",
      "Iteration 129/135: Train Loss = 0.6357, Valid Loss = 0.6370\n",
      "Iteration 130/135: Train Loss = 0.6353, Valid Loss = 0.6366\n",
      "Iteration 131/135: Train Loss = 0.6349, Valid Loss = 0.6362\n",
      "Iteration 132/135: Train Loss = 0.6345, Valid Loss = 0.6358\n",
      "Iteration 133/135: Train Loss = 0.6341, Valid Loss = 0.6354\n",
      "Iteration 134/135: Train Loss = 0.6337, Valid Loss = 0.6350\n",
      "Iteration 135/135: Train Loss = 0.6333, Valid Loss = 0.6346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:48:41,535] Trial 3 finished with value: 0.9423801516763322 and parameters: {'max_depth': 4, 'min_samples_leaf': 9, 'n_estimators': 135, 'learning_rate': 0.0032090841107833903, 'subsample': 0.7855580070630542}. Best is trial 0 with value: 0.9634116117421425.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/70: Train Loss = 0.6915, Valid Loss = 0.6915\n",
      "Iteration 2/70: Train Loss = 0.6899, Valid Loss = 0.6899\n",
      "Iteration 3/70: Train Loss = 0.6882, Valid Loss = 0.6883\n",
      "Iteration 4/70: Train Loss = 0.6866, Valid Loss = 0.6868\n",
      "Iteration 5/70: Train Loss = 0.6850, Valid Loss = 0.6852\n",
      "Iteration 6/70: Train Loss = 0.6833, Valid Loss = 0.6836\n",
      "Iteration 7/70: Train Loss = 0.6817, Valid Loss = 0.6820\n",
      "Iteration 8/70: Train Loss = 0.6801, Valid Loss = 0.6805\n",
      "Iteration 9/70: Train Loss = 0.6786, Valid Loss = 0.6789\n",
      "Iteration 10/70: Train Loss = 0.6770, Valid Loss = 0.6774\n",
      "Iteration 11/70: Train Loss = 0.6754, Valid Loss = 0.6758\n",
      "Iteration 12/70: Train Loss = 0.6739, Valid Loss = 0.6743\n",
      "Iteration 13/70: Train Loss = 0.6723, Valid Loss = 0.6728\n",
      "Iteration 14/70: Train Loss = 0.6708, Valid Loss = 0.6713\n",
      "Iteration 15/70: Train Loss = 0.6692, Valid Loss = 0.6698\n",
      "Iteration 16/70: Train Loss = 0.6677, Valid Loss = 0.6683\n",
      "Iteration 17/70: Train Loss = 0.6662, Valid Loss = 0.6668\n",
      "Iteration 18/70: Train Loss = 0.6647, Valid Loss = 0.6653\n",
      "Iteration 19/70: Train Loss = 0.6631, Valid Loss = 0.6639\n",
      "Iteration 20/70: Train Loss = 0.6616, Valid Loss = 0.6624\n",
      "Iteration 21/70: Train Loss = 0.6601, Valid Loss = 0.6609\n",
      "Iteration 22/70: Train Loss = 0.6586, Valid Loss = 0.6595\n",
      "Iteration 23/70: Train Loss = 0.6571, Valid Loss = 0.6580\n",
      "Iteration 24/70: Train Loss = 0.6556, Valid Loss = 0.6566\n",
      "Iteration 25/70: Train Loss = 0.6542, Valid Loss = 0.6551\n",
      "Iteration 26/70: Train Loss = 0.6527, Valid Loss = 0.6537\n",
      "Iteration 27/70: Train Loss = 0.6512, Valid Loss = 0.6522\n",
      "Iteration 28/70: Train Loss = 0.6498, Valid Loss = 0.6508\n",
      "Iteration 29/70: Train Loss = 0.6484, Valid Loss = 0.6494\n",
      "Iteration 30/70: Train Loss = 0.6469, Valid Loss = 0.6480\n",
      "Iteration 31/70: Train Loss = 0.6455, Valid Loss = 0.6466\n",
      "Iteration 32/70: Train Loss = 0.6440, Valid Loss = 0.6452\n",
      "Iteration 33/70: Train Loss = 0.6426, Valid Loss = 0.6438\n",
      "Iteration 34/70: Train Loss = 0.6412, Valid Loss = 0.6425\n",
      "Iteration 35/70: Train Loss = 0.6398, Valid Loss = 0.6411\n",
      "Iteration 36/70: Train Loss = 0.6384, Valid Loss = 0.6397\n",
      "Iteration 37/70: Train Loss = 0.6370, Valid Loss = 0.6384\n",
      "Iteration 38/70: Train Loss = 0.6356, Valid Loss = 0.6370\n",
      "Iteration 39/70: Train Loss = 0.6342, Valid Loss = 0.6357\n",
      "Iteration 40/70: Train Loss = 0.6329, Valid Loss = 0.6343\n",
      "Iteration 41/70: Train Loss = 0.6315, Valid Loss = 0.6330\n",
      "Iteration 42/70: Train Loss = 0.6302, Valid Loss = 0.6317\n",
      "Iteration 43/70: Train Loss = 0.6288, Valid Loss = 0.6304\n",
      "Iteration 44/70: Train Loss = 0.6275, Valid Loss = 0.6291\n",
      "Iteration 45/70: Train Loss = 0.6261, Valid Loss = 0.6278\n",
      "Iteration 46/70: Train Loss = 0.6248, Valid Loss = 0.6265\n",
      "Iteration 47/70: Train Loss = 0.6235, Valid Loss = 0.6252\n",
      "Iteration 48/70: Train Loss = 0.6221, Valid Loss = 0.6239\n",
      "Iteration 49/70: Train Loss = 0.6208, Valid Loss = 0.6226\n",
      "Iteration 50/70: Train Loss = 0.6195, Valid Loss = 0.6214\n",
      "Iteration 51/70: Train Loss = 0.6182, Valid Loss = 0.6201\n",
      "Iteration 52/70: Train Loss = 0.6169, Valid Loss = 0.6188\n",
      "Iteration 53/70: Train Loss = 0.6156, Valid Loss = 0.6176\n",
      "Iteration 54/70: Train Loss = 0.6143, Valid Loss = 0.6163\n",
      "Iteration 55/70: Train Loss = 0.6131, Valid Loss = 0.6151\n",
      "Iteration 56/70: Train Loss = 0.6118, Valid Loss = 0.6138\n",
      "Iteration 57/70: Train Loss = 0.6105, Valid Loss = 0.6126\n",
      "Iteration 58/70: Train Loss = 0.6093, Valid Loss = 0.6114\n",
      "Iteration 59/70: Train Loss = 0.6080, Valid Loss = 0.6102\n",
      "Iteration 60/70: Train Loss = 0.6068, Valid Loss = 0.6090\n",
      "Iteration 61/70: Train Loss = 0.6055, Valid Loss = 0.6078\n",
      "Iteration 62/70: Train Loss = 0.6043, Valid Loss = 0.6065\n",
      "Iteration 63/70: Train Loss = 0.6031, Valid Loss = 0.6053\n",
      "Iteration 64/70: Train Loss = 0.6019, Valid Loss = 0.6042\n",
      "Iteration 65/70: Train Loss = 0.6006, Valid Loss = 0.6030\n",
      "Iteration 66/70: Train Loss = 0.5994, Valid Loss = 0.6018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:48:45,063] Trial 4 finished with value: 0.9547384555551743 and parameters: {'max_depth': 6, 'min_samples_leaf': 4, 'n_estimators': 70, 'learning_rate': 0.009898303014954486, 'subsample': 0.8056415957658534}. Best is trial 0 with value: 0.9634116117421425.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 67/70: Train Loss = 0.5982, Valid Loss = 0.6006\n",
      "Iteration 68/70: Train Loss = 0.5970, Valid Loss = 0.5994\n",
      "Iteration 69/70: Train Loss = 0.5958, Valid Loss = 0.5983\n",
      "Iteration 70/70: Train Loss = 0.5946, Valid Loss = 0.5971\n",
      "Iteration 1/40: Train Loss = 0.6806, Valid Loss = 0.6814\n",
      "Iteration 2/40: Train Loss = 0.6686, Valid Loss = 0.6700\n",
      "Iteration 3/40: Train Loss = 0.6569, Valid Loss = 0.6591\n",
      "Iteration 4/40: Train Loss = 0.6456, Valid Loss = 0.6485\n",
      "Iteration 5/40: Train Loss = 0.6346, Valid Loss = 0.6385\n",
      "Iteration 6/40: Train Loss = 0.6239, Valid Loss = 0.6285\n",
      "Iteration 7/40: Train Loss = 0.6136, Valid Loss = 0.6189\n",
      "Iteration 8/40: Train Loss = 0.6037, Valid Loss = 0.6097\n",
      "Iteration 9/40: Train Loss = 0.5940, Valid Loss = 0.6008\n",
      "Iteration 10/40: Train Loss = 0.5846, Valid Loss = 0.5920\n",
      "Iteration 11/40: Train Loss = 0.5755, Valid Loss = 0.5836\n",
      "Iteration 12/40: Train Loss = 0.5667, Valid Loss = 0.5757\n",
      "Iteration 13/40: Train Loss = 0.5582, Valid Loss = 0.5677\n",
      "Iteration 14/40: Train Loss = 0.5501, Valid Loss = 0.5600\n",
      "Iteration 15/40: Train Loss = 0.5421, Valid Loss = 0.5526\n",
      "Iteration 16/40: Train Loss = 0.5345, Valid Loss = 0.5454\n",
      "Iteration 17/40: Train Loss = 0.5269, Valid Loss = 0.5385\n",
      "Iteration 18/40: Train Loss = 0.5196, Valid Loss = 0.5318\n",
      "Iteration 19/40: Train Loss = 0.5126, Valid Loss = 0.5252\n",
      "Iteration 20/40: Train Loss = 0.5058, Valid Loss = 0.5189\n",
      "Iteration 21/40: Train Loss = 0.4992, Valid Loss = 0.5129\n",
      "Iteration 22/40: Train Loss = 0.4929, Valid Loss = 0.5072\n",
      "Iteration 23/40: Train Loss = 0.4867, Valid Loss = 0.5014\n",
      "Iteration 24/40: Train Loss = 0.4805, Valid Loss = 0.4960\n",
      "Iteration 25/40: Train Loss = 0.4745, Valid Loss = 0.4907\n",
      "Iteration 26/40: Train Loss = 0.4687, Valid Loss = 0.4856\n",
      "Iteration 27/40: Train Loss = 0.4632, Valid Loss = 0.4806\n",
      "Iteration 28/40: Train Loss = 0.4577, Valid Loss = 0.4757\n",
      "Iteration 29/40: Train Loss = 0.4523, Valid Loss = 0.4706\n",
      "Iteration 30/40: Train Loss = 0.4472, Valid Loss = 0.4660\n",
      "Iteration 31/40: Train Loss = 0.4421, Valid Loss = 0.4616\n",
      "Iteration 32/40: Train Loss = 0.4372, Valid Loss = 0.4574\n",
      "Iteration 33/40: Train Loss = 0.4324, Valid Loss = 0.4529\n",
      "Iteration 34/40: Train Loss = 0.4278, Valid Loss = 0.4486\n",
      "Iteration 35/40: Train Loss = 0.4233, Valid Loss = 0.4445\n",
      "Iteration 36/40: Train Loss = 0.4189, Valid Loss = 0.4406\n",
      "Iteration 37/40: Train Loss = 0.4146, Valid Loss = 0.4367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:48:47,925] Trial 5 finished with value: 0.961980638199711 and parameters: {'max_depth': 11, 'min_samples_leaf': 2, 'n_estimators': 40, 'learning_rate': 0.06805801796704439, 'subsample': 0.4791400074259886}. Best is trial 0 with value: 0.9634116117421425.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38/40: Train Loss = 0.4105, Valid Loss = 0.4331\n",
      "Iteration 39/40: Train Loss = 0.4065, Valid Loss = 0.4295\n",
      "Iteration 40/40: Train Loss = 0.4025, Valid Loss = 0.4258\n",
      "Iteration 1/130: Train Loss = 0.6467, Valid Loss = 0.6476\n",
      "Iteration 2/130: Train Loss = 0.6062, Valid Loss = 0.6085\n",
      "Iteration 3/130: Train Loss = 0.5713, Valid Loss = 0.5744\n",
      "Iteration 4/130: Train Loss = 0.5407, Valid Loss = 0.5447\n",
      "Iteration 5/130: Train Loss = 0.5145, Valid Loss = 0.5194\n",
      "Iteration 6/130: Train Loss = 0.4916, Valid Loss = 0.4971\n",
      "Iteration 7/130: Train Loss = 0.4711, Valid Loss = 0.4770\n",
      "Iteration 8/130: Train Loss = 0.4534, Valid Loss = 0.4600\n",
      "Iteration 9/130: Train Loss = 0.4375, Valid Loss = 0.4449\n",
      "Iteration 10/130: Train Loss = 0.4231, Valid Loss = 0.4310\n",
      "Iteration 11/130: Train Loss = 0.4100, Valid Loss = 0.4186\n",
      "Iteration 12/130: Train Loss = 0.3986, Valid Loss = 0.4073\n",
      "Iteration 13/130: Train Loss = 0.3883, Valid Loss = 0.3974\n",
      "Iteration 14/130: Train Loss = 0.3788, Valid Loss = 0.3880\n",
      "Iteration 15/130: Train Loss = 0.3702, Valid Loss = 0.3799\n",
      "Iteration 16/130: Train Loss = 0.3624, Valid Loss = 0.3725\n",
      "Iteration 17/130: Train Loss = 0.3551, Valid Loss = 0.3656\n",
      "Iteration 18/130: Train Loss = 0.3484, Valid Loss = 0.3594\n",
      "Iteration 19/130: Train Loss = 0.3423, Valid Loss = 0.3538\n",
      "Iteration 20/130: Train Loss = 0.3366, Valid Loss = 0.3482\n",
      "Iteration 21/130: Train Loss = 0.3312, Valid Loss = 0.3436\n",
      "Iteration 22/130: Train Loss = 0.3261, Valid Loss = 0.3395\n",
      "Iteration 23/130: Train Loss = 0.3215, Valid Loss = 0.3350\n",
      "Iteration 24/130: Train Loss = 0.3172, Valid Loss = 0.3309\n",
      "Iteration 25/130: Train Loss = 0.3132, Valid Loss = 0.3272\n",
      "Iteration 26/130: Train Loss = 0.3094, Valid Loss = 0.3237\n",
      "Iteration 27/130: Train Loss = 0.3056, Valid Loss = 0.3205\n",
      "Iteration 28/130: Train Loss = 0.3022, Valid Loss = 0.3177\n",
      "Iteration 29/130: Train Loss = 0.2992, Valid Loss = 0.3149\n",
      "Iteration 30/130: Train Loss = 0.2962, Valid Loss = 0.3121\n",
      "Iteration 31/130: Train Loss = 0.2933, Valid Loss = 0.3094\n",
      "Iteration 32/130: Train Loss = 0.2906, Valid Loss = 0.3068\n",
      "Iteration 33/130: Train Loss = 0.2880, Valid Loss = 0.3048\n",
      "Iteration 34/130: Train Loss = 0.2857, Valid Loss = 0.3026\n",
      "Iteration 35/130: Train Loss = 0.2831, Valid Loss = 0.3004\n",
      "Iteration 36/130: Train Loss = 0.2807, Valid Loss = 0.2984\n",
      "Iteration 37/130: Train Loss = 0.2786, Valid Loss = 0.2965\n",
      "Iteration 38/130: Train Loss = 0.2766, Valid Loss = 0.2946\n",
      "Iteration 39/130: Train Loss = 0.2747, Valid Loss = 0.2928\n",
      "Iteration 40/130: Train Loss = 0.2729, Valid Loss = 0.2914\n",
      "Iteration 41/130: Train Loss = 0.2712, Valid Loss = 0.2898\n",
      "Iteration 42/130: Train Loss = 0.2691, Valid Loss = 0.2879\n",
      "Iteration 43/130: Train Loss = 0.2675, Valid Loss = 0.2863\n",
      "Iteration 44/130: Train Loss = 0.2660, Valid Loss = 0.2851\n",
      "Iteration 45/130: Train Loss = 0.2644, Valid Loss = 0.2837\n",
      "Iteration 46/130: Train Loss = 0.2630, Valid Loss = 0.2827\n",
      "Iteration 47/130: Train Loss = 0.2616, Valid Loss = 0.2815\n",
      "Iteration 48/130: Train Loss = 0.2604, Valid Loss = 0.2805\n",
      "Iteration 49/130: Train Loss = 0.2591, Valid Loss = 0.2793\n",
      "Iteration 50/130: Train Loss = 0.2578, Valid Loss = 0.2785\n",
      "Iteration 51/130: Train Loss = 0.2566, Valid Loss = 0.2774\n",
      "Iteration 52/130: Train Loss = 0.2552, Valid Loss = 0.2764\n",
      "Iteration 53/130: Train Loss = 0.2541, Valid Loss = 0.2753\n",
      "Iteration 54/130: Train Loss = 0.2530, Valid Loss = 0.2745\n",
      "Iteration 55/130: Train Loss = 0.2520, Valid Loss = 0.2737\n",
      "Iteration 56/130: Train Loss = 0.2510, Valid Loss = 0.2729\n",
      "Iteration 57/130: Train Loss = 0.2500, Valid Loss = 0.2719\n",
      "Iteration 58/130: Train Loss = 0.2489, Valid Loss = 0.2712\n",
      "Iteration 59/130: Train Loss = 0.2480, Valid Loss = 0.2704\n",
      "Iteration 60/130: Train Loss = 0.2471, Valid Loss = 0.2699\n",
      "Iteration 61/130: Train Loss = 0.2463, Valid Loss = 0.2692\n",
      "Iteration 62/130: Train Loss = 0.2452, Valid Loss = 0.2684\n",
      "Iteration 63/130: Train Loss = 0.2444, Valid Loss = 0.2677\n",
      "Iteration 64/130: Train Loss = 0.2435, Valid Loss = 0.2670\n",
      "Iteration 65/130: Train Loss = 0.2427, Valid Loss = 0.2663\n",
      "Iteration 66/130: Train Loss = 0.2418, Valid Loss = 0.2654\n",
      "Iteration 67/130: Train Loss = 0.2410, Valid Loss = 0.2646\n",
      "Iteration 68/130: Train Loss = 0.2401, Valid Loss = 0.2639\n",
      "Iteration 69/130: Train Loss = 0.2394, Valid Loss = 0.2634\n",
      "Iteration 70/130: Train Loss = 0.2386, Valid Loss = 0.2628\n",
      "Iteration 71/130: Train Loss = 0.2378, Valid Loss = 0.2622\n",
      "Iteration 72/130: Train Loss = 0.2372, Valid Loss = 0.2619\n",
      "Iteration 73/130: Train Loss = 0.2366, Valid Loss = 0.2615\n",
      "Iteration 74/130: Train Loss = 0.2359, Valid Loss = 0.2613\n",
      "Iteration 75/130: Train Loss = 0.2353, Valid Loss = 0.2609\n",
      "Iteration 76/130: Train Loss = 0.2346, Valid Loss = 0.2605\n",
      "Iteration 77/130: Train Loss = 0.2340, Valid Loss = 0.2599\n",
      "Iteration 78/130: Train Loss = 0.2333, Valid Loss = 0.2596\n",
      "Iteration 79/130: Train Loss = 0.2327, Valid Loss = 0.2591\n",
      "Iteration 80/130: Train Loss = 0.2322, Valid Loss = 0.2586\n",
      "Iteration 81/130: Train Loss = 0.2316, Valid Loss = 0.2582\n",
      "Iteration 82/130: Train Loss = 0.2310, Valid Loss = 0.2576\n",
      "Iteration 83/130: Train Loss = 0.2305, Valid Loss = 0.2571\n",
      "Iteration 84/130: Train Loss = 0.2300, Valid Loss = 0.2569\n",
      "Iteration 85/130: Train Loss = 0.2295, Valid Loss = 0.2566\n",
      "Iteration 86/130: Train Loss = 0.2288, Valid Loss = 0.2561\n",
      "Iteration 87/130: Train Loss = 0.2283, Valid Loss = 0.2560\n",
      "Iteration 88/130: Train Loss = 0.2279, Valid Loss = 0.2555\n",
      "Iteration 89/130: Train Loss = 0.2273, Valid Loss = 0.2552\n",
      "Iteration 90/130: Train Loss = 0.2268, Valid Loss = 0.2548\n",
      "Iteration 91/130: Train Loss = 0.2262, Valid Loss = 0.2544\n",
      "Iteration 92/130: Train Loss = 0.2257, Valid Loss = 0.2540\n",
      "Iteration 93/130: Train Loss = 0.2252, Valid Loss = 0.2536\n",
      "Iteration 94/130: Train Loss = 0.2248, Valid Loss = 0.2533\n",
      "Iteration 95/130: Train Loss = 0.2244, Valid Loss = 0.2531\n",
      "Iteration 96/130: Train Loss = 0.2239, Valid Loss = 0.2526\n",
      "Iteration 97/130: Train Loss = 0.2235, Valid Loss = 0.2523\n",
      "Iteration 98/130: Train Loss = 0.2231, Valid Loss = 0.2521\n",
      "Iteration 99/130: Train Loss = 0.2227, Valid Loss = 0.2516\n",
      "Iteration 100/130: Train Loss = 0.2223, Valid Loss = 0.2511\n",
      "Iteration 101/130: Train Loss = 0.2220, Valid Loss = 0.2509\n",
      "Iteration 102/130: Train Loss = 0.2215, Valid Loss = 0.2505\n",
      "Iteration 103/130: Train Loss = 0.2211, Valid Loss = 0.2501\n",
      "Iteration 104/130: Train Loss = 0.2207, Valid Loss = 0.2500\n",
      "Iteration 105/130: Train Loss = 0.2203, Valid Loss = 0.2498\n",
      "Iteration 106/130: Train Loss = 0.2199, Valid Loss = 0.2496\n",
      "Iteration 107/130: Train Loss = 0.2195, Valid Loss = 0.2493\n",
      "Iteration 108/130: Train Loss = 0.2191, Valid Loss = 0.2492\n",
      "Iteration 109/130: Train Loss = 0.2188, Valid Loss = 0.2488\n",
      "Iteration 110/130: Train Loss = 0.2184, Valid Loss = 0.2484\n",
      "Iteration 111/130: Train Loss = 0.2180, Valid Loss = 0.2483\n",
      "Iteration 112/130: Train Loss = 0.2176, Valid Loss = 0.2479\n",
      "Iteration 113/130: Train Loss = 0.2173, Valid Loss = 0.2477\n",
      "Iteration 114/130: Train Loss = 0.2170, Valid Loss = 0.2476\n",
      "Iteration 115/130: Train Loss = 0.2167, Valid Loss = 0.2475\n",
      "Iteration 116/130: Train Loss = 0.2164, Valid Loss = 0.2473\n",
      "Iteration 117/130: Train Loss = 0.2161, Valid Loss = 0.2470\n",
      "Iteration 118/130: Train Loss = 0.2157, Valid Loss = 0.2468\n",
      "Iteration 119/130: Train Loss = 0.2154, Valid Loss = 0.2466\n",
      "Iteration 120/130: Train Loss = 0.2151, Valid Loss = 0.2464\n",
      "Iteration 121/130: Train Loss = 0.2148, Valid Loss = 0.2462\n",
      "Iteration 122/130: Train Loss = 0.2144, Valid Loss = 0.2460\n",
      "Iteration 123/130: Train Loss = 0.2142, Valid Loss = 0.2459\n",
      "Iteration 124/130: Train Loss = 0.2140, Valid Loss = 0.2457\n",
      "Iteration 125/130: Train Loss = 0.2136, Valid Loss = 0.2454\n",
      "Iteration 126/130: Train Loss = 0.2134, Valid Loss = 0.2453\n",
      "Iteration 127/130: Train Loss = 0.2131, Valid Loss = 0.2452\n",
      "Iteration 128/130: Train Loss = 0.2128, Valid Loss = 0.2449\n",
      "Iteration 129/130: Train Loss = 0.2126, Valid Loss = 0.2448\n",
      "Iteration 130/130: Train Loss = 0.2124, Valid Loss = 0.2446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:48:54,434] Trial 6 finished with value: 0.965235693124235 and parameters: {'max_depth': 6, 'min_samples_leaf': 20, 'n_estimators': 130, 'learning_rate': 0.2923279505403704, 'subsample': 0.8205317126256255}. Best is trial 6 with value: 0.965235693124235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/15: Train Loss = 0.6499, Valid Loss = 0.6506\n",
      "Iteration 2/15: Train Loss = 0.6117, Valid Loss = 0.6132\n",
      "Iteration 3/15: Train Loss = 0.5788, Valid Loss = 0.5814\n",
      "Iteration 4/15: Train Loss = 0.5495, Valid Loss = 0.5528\n",
      "Iteration 5/15: Train Loss = 0.5241, Valid Loss = 0.5281\n",
      "Iteration 6/15: Train Loss = 0.5018, Valid Loss = 0.5064\n",
      "Iteration 7/15: Train Loss = 0.4820, Valid Loss = 0.4870\n",
      "Iteration 8/15: Train Loss = 0.4645, Valid Loss = 0.4703\n",
      "Iteration 9/15: Train Loss = 0.4491, Valid Loss = 0.4553\n",
      "Iteration 10/15: Train Loss = 0.4350, Valid Loss = 0.4421\n",
      "Iteration 11/15: Train Loss = 0.4226, Valid Loss = 0.4304\n",
      "Iteration 12/15: Train Loss = 0.4111, Valid Loss = 0.4193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:48:54,994] Trial 7 finished with value: 0.9531583984382447 and parameters: {'max_depth': 5, 'min_samples_leaf': 2, 'n_estimators': 15, 'learning_rate': 0.28060934083460354, 'subsample': 0.6232983217747247}. Best is trial 6 with value: 0.965235693124235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13/15: Train Loss = 0.4007, Valid Loss = 0.4090\n",
      "Iteration 14/15: Train Loss = 0.3912, Valid Loss = 0.3999\n",
      "Iteration 15/15: Train Loss = 0.3825, Valid Loss = 0.3919\n",
      "Iteration 1/25: Train Loss = 0.6924, Valid Loss = 0.6924\n",
      "Iteration 2/25: Train Loss = 0.6916, Valid Loss = 0.6916\n",
      "Iteration 3/25: Train Loss = 0.6908, Valid Loss = 0.6909\n",
      "Iteration 4/25: Train Loss = 0.6901, Valid Loss = 0.6901\n",
      "Iteration 5/25: Train Loss = 0.6893, Valid Loss = 0.6894\n",
      "Iteration 6/25: Train Loss = 0.6886, Valid Loss = 0.6887\n",
      "Iteration 7/25: Train Loss = 0.6878, Valid Loss = 0.6879\n",
      "Iteration 8/25: Train Loss = 0.6870, Valid Loss = 0.6872\n",
      "Iteration 9/25: Train Loss = 0.6863, Valid Loss = 0.6864\n",
      "Iteration 10/25: Train Loss = 0.6855, Valid Loss = 0.6857\n",
      "Iteration 11/25: Train Loss = 0.6848, Valid Loss = 0.6850\n",
      "Iteration 12/25: Train Loss = 0.6840, Valid Loss = 0.6842\n",
      "Iteration 13/25: Train Loss = 0.6833, Valid Loss = 0.6835\n",
      "Iteration 14/25: Train Loss = 0.6825, Valid Loss = 0.6828\n",
      "Iteration 15/25: Train Loss = 0.6818, Valid Loss = 0.6820\n",
      "Iteration 16/25: Train Loss = 0.6810, Valid Loss = 0.6813\n",
      "Iteration 17/25: Train Loss = 0.6803, Valid Loss = 0.6806\n",
      "Iteration 18/25: Train Loss = 0.6796, Valid Loss = 0.6799\n",
      "Iteration 19/25: Train Loss = 0.6788, Valid Loss = 0.6791\n",
      "Iteration 20/25: Train Loss = 0.6781, Valid Loss = 0.6784\n",
      "Iteration 21/25: Train Loss = 0.6774, Valid Loss = 0.6777\n",
      "Iteration 22/25: Train Loss = 0.6767, Valid Loss = 0.6770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:48:55,859] Trial 8 finished with value: 0.9522135669865673 and parameters: {'max_depth': 6, 'min_samples_leaf': 15, 'n_estimators': 25, 'learning_rate': 0.004664387916906928, 'subsample': 0.35150799819676903}. Best is trial 6 with value: 0.965235693124235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23/25: Train Loss = 0.6759, Valid Loss = 0.6763\n",
      "Iteration 24/25: Train Loss = 0.6752, Valid Loss = 0.6756\n",
      "Iteration 25/25: Train Loss = 0.6745, Valid Loss = 0.6748\n",
      "Iteration 1/135: Train Loss = 0.6836, Valid Loss = 0.6842\n",
      "Iteration 2/135: Train Loss = 0.6743, Valid Loss = 0.6755\n",
      "Iteration 3/135: Train Loss = 0.6653, Valid Loss = 0.6670\n",
      "Iteration 4/135: Train Loss = 0.6564, Valid Loss = 0.6586\n",
      "Iteration 5/135: Train Loss = 0.6478, Valid Loss = 0.6504\n",
      "Iteration 6/135: Train Loss = 0.6394, Valid Loss = 0.6425\n",
      "Iteration 7/135: Train Loss = 0.6313, Valid Loss = 0.6348\n",
      "Iteration 8/135: Train Loss = 0.6232, Valid Loss = 0.6272\n",
      "Iteration 9/135: Train Loss = 0.6155, Valid Loss = 0.6199\n",
      "Iteration 10/135: Train Loss = 0.6079, Valid Loss = 0.6128\n",
      "Iteration 11/135: Train Loss = 0.6005, Valid Loss = 0.6060\n",
      "Iteration 12/135: Train Loss = 0.5934, Valid Loss = 0.5993\n",
      "Iteration 13/135: Train Loss = 0.5863, Valid Loss = 0.5926\n",
      "Iteration 14/135: Train Loss = 0.5794, Valid Loss = 0.5861\n",
      "Iteration 15/135: Train Loss = 0.5727, Valid Loss = 0.5799\n",
      "Iteration 16/135: Train Loss = 0.5661, Valid Loss = 0.5737\n",
      "Iteration 17/135: Train Loss = 0.5597, Valid Loss = 0.5676\n",
      "Iteration 18/135: Train Loss = 0.5534, Valid Loss = 0.5618\n",
      "Iteration 19/135: Train Loss = 0.5473, Valid Loss = 0.5562\n",
      "Iteration 20/135: Train Loss = 0.5412, Valid Loss = 0.5506\n",
      "Iteration 21/135: Train Loss = 0.5353, Valid Loss = 0.5453\n",
      "Iteration 22/135: Train Loss = 0.5297, Valid Loss = 0.5400\n",
      "Iteration 23/135: Train Loss = 0.5241, Valid Loss = 0.5349\n",
      "Iteration 24/135: Train Loss = 0.5186, Valid Loss = 0.5299\n",
      "Iteration 25/135: Train Loss = 0.5132, Valid Loss = 0.5249\n",
      "Iteration 26/135: Train Loss = 0.5080, Valid Loss = 0.5202\n",
      "Iteration 27/135: Train Loss = 0.5029, Valid Loss = 0.5154\n",
      "Iteration 28/135: Train Loss = 0.4980, Valid Loss = 0.5108\n",
      "Iteration 29/135: Train Loss = 0.4931, Valid Loss = 0.5062\n",
      "Iteration 30/135: Train Loss = 0.4883, Valid Loss = 0.5019\n",
      "Iteration 31/135: Train Loss = 0.4836, Valid Loss = 0.4975\n",
      "Iteration 32/135: Train Loss = 0.4791, Valid Loss = 0.4933\n",
      "Iteration 33/135: Train Loss = 0.4746, Valid Loss = 0.4893\n",
      "Iteration 34/135: Train Loss = 0.4702, Valid Loss = 0.4853\n",
      "Iteration 35/135: Train Loss = 0.4659, Valid Loss = 0.4813\n",
      "Iteration 36/135: Train Loss = 0.4618, Valid Loss = 0.4776\n",
      "Iteration 37/135: Train Loss = 0.4577, Valid Loss = 0.4739\n",
      "Iteration 38/135: Train Loss = 0.4537, Valid Loss = 0.4703\n",
      "Iteration 39/135: Train Loss = 0.4498, Valid Loss = 0.4670\n",
      "Iteration 40/135: Train Loss = 0.4460, Valid Loss = 0.4636\n",
      "Iteration 41/135: Train Loss = 0.4421, Valid Loss = 0.4602\n",
      "Iteration 42/135: Train Loss = 0.4384, Valid Loss = 0.4568\n",
      "Iteration 43/135: Train Loss = 0.4348, Valid Loss = 0.4536\n",
      "Iteration 44/135: Train Loss = 0.4313, Valid Loss = 0.4504\n",
      "Iteration 45/135: Train Loss = 0.4278, Valid Loss = 0.4474\n",
      "Iteration 46/135: Train Loss = 0.4244, Valid Loss = 0.4444\n",
      "Iteration 47/135: Train Loss = 0.4211, Valid Loss = 0.4413\n",
      "Iteration 48/135: Train Loss = 0.4178, Valid Loss = 0.4385\n",
      "Iteration 49/135: Train Loss = 0.4147, Valid Loss = 0.4356\n",
      "Iteration 50/135: Train Loss = 0.4116, Valid Loss = 0.4328\n",
      "Iteration 51/135: Train Loss = 0.4085, Valid Loss = 0.4301\n",
      "Iteration 52/135: Train Loss = 0.4055, Valid Loss = 0.4275\n",
      "Iteration 53/135: Train Loss = 0.4026, Valid Loss = 0.4248\n",
      "Iteration 54/135: Train Loss = 0.3997, Valid Loss = 0.4222\n",
      "Iteration 55/135: Train Loss = 0.3968, Valid Loss = 0.4196\n",
      "Iteration 56/135: Train Loss = 0.3941, Valid Loss = 0.4171\n",
      "Iteration 57/135: Train Loss = 0.3913, Valid Loss = 0.4146\n",
      "Iteration 58/135: Train Loss = 0.3886, Valid Loss = 0.4122\n",
      "Iteration 59/135: Train Loss = 0.3860, Valid Loss = 0.4100\n",
      "Iteration 60/135: Train Loss = 0.3834, Valid Loss = 0.4078\n",
      "Iteration 61/135: Train Loss = 0.3808, Valid Loss = 0.4056\n",
      "Iteration 62/135: Train Loss = 0.3783, Valid Loss = 0.4035\n",
      "Iteration 63/135: Train Loss = 0.3759, Valid Loss = 0.4013\n",
      "Iteration 64/135: Train Loss = 0.3735, Valid Loss = 0.3993\n",
      "Iteration 65/135: Train Loss = 0.3711, Valid Loss = 0.3972\n",
      "Iteration 66/135: Train Loss = 0.3688, Valid Loss = 0.3951\n",
      "Iteration 67/135: Train Loss = 0.3665, Valid Loss = 0.3932\n",
      "Iteration 68/135: Train Loss = 0.3643, Valid Loss = 0.3912\n",
      "Iteration 69/135: Train Loss = 0.3620, Valid Loss = 0.3893\n",
      "Iteration 70/135: Train Loss = 0.3599, Valid Loss = 0.3873\n",
      "Iteration 71/135: Train Loss = 0.3577, Valid Loss = 0.3855\n",
      "Iteration 72/135: Train Loss = 0.3556, Valid Loss = 0.3837\n",
      "Iteration 73/135: Train Loss = 0.3535, Valid Loss = 0.3820\n",
      "Iteration 74/135: Train Loss = 0.3515, Valid Loss = 0.3801\n",
      "Iteration 75/135: Train Loss = 0.3495, Valid Loss = 0.3784\n",
      "Iteration 76/135: Train Loss = 0.3475, Valid Loss = 0.3767\n",
      "Iteration 77/135: Train Loss = 0.3456, Valid Loss = 0.3751\n",
      "Iteration 78/135: Train Loss = 0.3437, Valid Loss = 0.3735\n",
      "Iteration 79/135: Train Loss = 0.3418, Valid Loss = 0.3719\n",
      "Iteration 80/135: Train Loss = 0.3399, Valid Loss = 0.3703\n",
      "Iteration 81/135: Train Loss = 0.3381, Valid Loss = 0.3688\n",
      "Iteration 82/135: Train Loss = 0.3364, Valid Loss = 0.3673\n",
      "Iteration 83/135: Train Loss = 0.3345, Valid Loss = 0.3658\n",
      "Iteration 84/135: Train Loss = 0.3328, Valid Loss = 0.3643\n",
      "Iteration 85/135: Train Loss = 0.3311, Valid Loss = 0.3628\n",
      "Iteration 86/135: Train Loss = 0.3294, Valid Loss = 0.3615\n",
      "Iteration 87/135: Train Loss = 0.3278, Valid Loss = 0.3601\n",
      "Iteration 88/135: Train Loss = 0.3262, Valid Loss = 0.3588\n",
      "Iteration 89/135: Train Loss = 0.3246, Valid Loss = 0.3575\n",
      "Iteration 90/135: Train Loss = 0.3230, Valid Loss = 0.3562\n",
      "Iteration 91/135: Train Loss = 0.3214, Valid Loss = 0.3549\n",
      "Iteration 92/135: Train Loss = 0.3198, Valid Loss = 0.3537\n",
      "Iteration 93/135: Train Loss = 0.3183, Valid Loss = 0.3524\n",
      "Iteration 94/135: Train Loss = 0.3167, Valid Loss = 0.3511\n",
      "Iteration 95/135: Train Loss = 0.3153, Valid Loss = 0.3499\n",
      "Iteration 96/135: Train Loss = 0.3138, Valid Loss = 0.3488\n",
      "Iteration 97/135: Train Loss = 0.3124, Valid Loss = 0.3476\n",
      "Iteration 98/135: Train Loss = 0.3109, Valid Loss = 0.3464\n",
      "Iteration 99/135: Train Loss = 0.3095, Valid Loss = 0.3452\n",
      "Iteration 100/135: Train Loss = 0.3081, Valid Loss = 0.3441\n",
      "Iteration 101/135: Train Loss = 0.3067, Valid Loss = 0.3431\n",
      "Iteration 102/135: Train Loss = 0.3053, Valid Loss = 0.3420\n",
      "Iteration 103/135: Train Loss = 0.3040, Valid Loss = 0.3410\n",
      "Iteration 104/135: Train Loss = 0.3026, Valid Loss = 0.3399\n",
      "Iteration 105/135: Train Loss = 0.3014, Valid Loss = 0.3387\n",
      "Iteration 106/135: Train Loss = 0.3001, Valid Loss = 0.3377\n",
      "Iteration 107/135: Train Loss = 0.2988, Valid Loss = 0.3367\n",
      "Iteration 108/135: Train Loss = 0.2976, Valid Loss = 0.3357\n",
      "Iteration 109/135: Train Loss = 0.2964, Valid Loss = 0.3348\n",
      "Iteration 110/135: Train Loss = 0.2952, Valid Loss = 0.3339\n",
      "Iteration 111/135: Train Loss = 0.2940, Valid Loss = 0.3329\n",
      "Iteration 112/135: Train Loss = 0.2928, Valid Loss = 0.3321\n",
      "Iteration 113/135: Train Loss = 0.2916, Valid Loss = 0.3312\n",
      "Iteration 114/135: Train Loss = 0.2905, Valid Loss = 0.3305\n",
      "Iteration 115/135: Train Loss = 0.2893, Valid Loss = 0.3296\n",
      "Iteration 116/135: Train Loss = 0.2882, Valid Loss = 0.3287\n",
      "Iteration 117/135: Train Loss = 0.2871, Valid Loss = 0.3278\n",
      "Iteration 118/135: Train Loss = 0.2860, Valid Loss = 0.3269\n",
      "Iteration 119/135: Train Loss = 0.2849, Valid Loss = 0.3260\n",
      "Iteration 120/135: Train Loss = 0.2838, Valid Loss = 0.3252\n",
      "Iteration 121/135: Train Loss = 0.2827, Valid Loss = 0.3243\n",
      "Iteration 122/135: Train Loss = 0.2817, Valid Loss = 0.3235\n",
      "Iteration 123/135: Train Loss = 0.2807, Valid Loss = 0.3227\n",
      "Iteration 124/135: Train Loss = 0.2796, Valid Loss = 0.3219\n",
      "Iteration 125/135: Train Loss = 0.2787, Valid Loss = 0.3211\n",
      "Iteration 126/135: Train Loss = 0.2777, Valid Loss = 0.3205\n",
      "Iteration 127/135: Train Loss = 0.2767, Valid Loss = 0.3198\n",
      "Iteration 128/135: Train Loss = 0.2758, Valid Loss = 0.3190\n",
      "Iteration 129/135: Train Loss = 0.2748, Valid Loss = 0.3183\n",
      "Iteration 130/135: Train Loss = 0.2738, Valid Loss = 0.3175\n",
      "Iteration 131/135: Train Loss = 0.2728, Valid Loss = 0.3168\n",
      "Iteration 132/135: Train Loss = 0.2719, Valid Loss = 0.3161\n",
      "Iteration 133/135: Train Loss = 0.2710, Valid Loss = 0.3154\n",
      "Iteration 134/135: Train Loss = 0.2701, Valid Loss = 0.3147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:49:14,916] Trial 9 finished with value: 0.9624156877099466 and parameters: {'max_depth': 14, 'min_samples_leaf': 10, 'n_estimators': 135, 'learning_rate': 0.05174124324522668, 'subsample': 0.8661324243377773}. Best is trial 6 with value: 0.965235693124235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 135/135: Train Loss = 0.2692, Valid Loss = 0.3140\n",
      "Iteration 1/150: Train Loss = 0.5633, Valid Loss = 0.5678\n",
      "Iteration 2/150: Train Loss = 0.4780, Valid Loss = 0.4858\n",
      "Iteration 3/150: Train Loss = 0.4206, Valid Loss = 0.4323\n",
      "Iteration 4/150: Train Loss = 0.3804, Valid Loss = 0.3953\n",
      "Iteration 5/150: Train Loss = 0.3505, Valid Loss = 0.3683\n",
      "Iteration 6/150: Train Loss = 0.3280, Valid Loss = 0.3488\n",
      "Iteration 7/150: Train Loss = 0.3102, Valid Loss = 0.3335\n",
      "Iteration 8/150: Train Loss = 0.2956, Valid Loss = 0.3210\n",
      "Iteration 9/150: Train Loss = 0.2834, Valid Loss = 0.3103\n",
      "Iteration 10/150: Train Loss = 0.2731, Valid Loss = 0.3013\n",
      "Iteration 11/150: Train Loss = 0.2645, Valid Loss = 0.2941\n",
      "Iteration 12/150: Train Loss = 0.2575, Valid Loss = 0.2894\n",
      "Iteration 13/150: Train Loss = 0.2510, Valid Loss = 0.2843\n",
      "Iteration 14/150: Train Loss = 0.2453, Valid Loss = 0.2795\n",
      "Iteration 15/150: Train Loss = 0.2399, Valid Loss = 0.2755\n",
      "Iteration 16/150: Train Loss = 0.2352, Valid Loss = 0.2719\n",
      "Iteration 17/150: Train Loss = 0.2307, Valid Loss = 0.2694\n",
      "Iteration 18/150: Train Loss = 0.2269, Valid Loss = 0.2670\n",
      "Iteration 19/150: Train Loss = 0.2233, Valid Loss = 0.2645\n",
      "Iteration 20/150: Train Loss = 0.2201, Valid Loss = 0.2630\n",
      "Iteration 21/150: Train Loss = 0.2171, Valid Loss = 0.2607\n",
      "Iteration 22/150: Train Loss = 0.2145, Valid Loss = 0.2591\n",
      "Iteration 23/150: Train Loss = 0.2116, Valid Loss = 0.2575\n",
      "Iteration 24/150: Train Loss = 0.2095, Valid Loss = 0.2563\n",
      "Iteration 25/150: Train Loss = 0.2074, Valid Loss = 0.2547\n",
      "Iteration 26/150: Train Loss = 0.2053, Valid Loss = 0.2538\n",
      "Iteration 27/150: Train Loss = 0.2033, Valid Loss = 0.2528\n",
      "Iteration 28/150: Train Loss = 0.2010, Valid Loss = 0.2521\n",
      "Iteration 29/150: Train Loss = 0.1995, Valid Loss = 0.2516\n",
      "Iteration 30/150: Train Loss = 0.1977, Valid Loss = 0.2505\n",
      "Iteration 31/150: Train Loss = 0.1964, Valid Loss = 0.2497\n",
      "Iteration 32/150: Train Loss = 0.1949, Valid Loss = 0.2493\n",
      "Iteration 33/150: Train Loss = 0.1933, Valid Loss = 0.2483\n",
      "Iteration 34/150: Train Loss = 0.1917, Valid Loss = 0.2473\n",
      "Iteration 35/150: Train Loss = 0.1904, Valid Loss = 0.2472\n",
      "Iteration 36/150: Train Loss = 0.1892, Valid Loss = 0.2471\n",
      "Iteration 37/150: Train Loss = 0.1879, Valid Loss = 0.2473\n",
      "Iteration 38/150: Train Loss = 0.1868, Valid Loss = 0.2470\n",
      "Iteration 39/150: Train Loss = 0.1855, Valid Loss = 0.2468\n",
      "Iteration 40/150: Train Loss = 0.1844, Valid Loss = 0.2461\n",
      "Iteration 41/150: Train Loss = 0.1833, Valid Loss = 0.2456\n",
      "Iteration 42/150: Train Loss = 0.1822, Valid Loss = 0.2451\n",
      "Iteration 43/150: Train Loss = 0.1813, Valid Loss = 0.2450\n",
      "Iteration 44/150: Train Loss = 0.1806, Valid Loss = 0.2448\n",
      "Iteration 45/150: Train Loss = 0.1797, Valid Loss = 0.2446\n",
      "Iteration 46/150: Train Loss = 0.1790, Valid Loss = 0.2447\n",
      "Iteration 47/150: Train Loss = 0.1781, Valid Loss = 0.2447\n",
      "Iteration 48/150: Train Loss = 0.1773, Valid Loss = 0.2446\n",
      "Iteration 49/150: Train Loss = 0.1764, Valid Loss = 0.2443\n",
      "Iteration 50/150: Train Loss = 0.1758, Valid Loss = 0.2440\n",
      "Iteration 51/150: Train Loss = 0.1748, Valid Loss = 0.2437\n",
      "Iteration 52/150: Train Loss = 0.1739, Valid Loss = 0.2432\n",
      "Iteration 53/150: Train Loss = 0.1732, Valid Loss = 0.2426\n",
      "Iteration 54/150: Train Loss = 0.1726, Valid Loss = 0.2429\n",
      "Iteration 55/150: Train Loss = 0.1718, Valid Loss = 0.2430\n",
      "Iteration 56/150: Train Loss = 0.1712, Valid Loss = 0.2423\n",
      "Iteration 57/150: Train Loss = 0.1708, Valid Loss = 0.2419\n",
      "Iteration 58/150: Train Loss = 0.1701, Valid Loss = 0.2422\n",
      "Iteration 59/150: Train Loss = 0.1695, Valid Loss = 0.2421\n",
      "Iteration 60/150: Train Loss = 0.1687, Valid Loss = 0.2418\n",
      "Iteration 61/150: Train Loss = 0.1683, Valid Loss = 0.2412\n",
      "Iteration 62/150: Train Loss = 0.1677, Valid Loss = 0.2411\n",
      "Iteration 63/150: Train Loss = 0.1669, Valid Loss = 0.2412\n",
      "Iteration 64/150: Train Loss = 0.1662, Valid Loss = 0.2413\n",
      "Iteration 65/150: Train Loss = 0.1656, Valid Loss = 0.2413\n",
      "Iteration 66/150: Train Loss = 0.1651, Valid Loss = 0.2412\n",
      "Iteration 67/150: Train Loss = 0.1646, Valid Loss = 0.2413\n",
      "Iteration 68/150: Train Loss = 0.1640, Valid Loss = 0.2411\n",
      "Iteration 69/150: Train Loss = 0.1634, Valid Loss = 0.2414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:49:20,869] Trial 10 finished with value: 0.964195158406064 and parameters: {'max_depth': 10, 'min_samples_leaf': 20, 'n_estimators': 150, 'learning_rate': 0.8208343171870824, 'subsample': 0.9668398938364334}. Best is trial 6 with value: 0.965235693124235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70/150: Train Loss = 0.1629, Valid Loss = 0.2411\n",
      "Iteration 71/150: Train Loss = 0.1622, Valid Loss = 0.2417\n",
      "Early stopping triggered.\n",
      "Iteration 1/150: Train Loss = 0.5504, Valid Loss = 0.5565\n",
      "Iteration 2/150: Train Loss = 0.4612, Valid Loss = 0.4719\n",
      "Iteration 3/150: Train Loss = 0.4033, Valid Loss = 0.4176\n",
      "Iteration 4/150: Train Loss = 0.3638, Valid Loss = 0.3815\n",
      "Iteration 5/150: Train Loss = 0.3357, Valid Loss = 0.3553\n",
      "Iteration 6/150: Train Loss = 0.3142, Valid Loss = 0.3369\n",
      "Iteration 7/150: Train Loss = 0.2977, Valid Loss = 0.3228\n",
      "Iteration 8/150: Train Loss = 0.2841, Valid Loss = 0.3117\n",
      "Iteration 9/150: Train Loss = 0.2719, Valid Loss = 0.3027\n",
      "Iteration 10/150: Train Loss = 0.2626, Valid Loss = 0.2948\n",
      "Iteration 11/150: Train Loss = 0.2547, Valid Loss = 0.2891\n",
      "Iteration 12/150: Train Loss = 0.2479, Valid Loss = 0.2843\n",
      "Iteration 13/150: Train Loss = 0.2419, Valid Loss = 0.2801\n",
      "Iteration 14/150: Train Loss = 0.2369, Valid Loss = 0.2759\n",
      "Iteration 15/150: Train Loss = 0.2319, Valid Loss = 0.2728\n",
      "Iteration 16/150: Train Loss = 0.2282, Valid Loss = 0.2696\n",
      "Iteration 17/150: Train Loss = 0.2246, Valid Loss = 0.2668\n",
      "Iteration 18/150: Train Loss = 0.2206, Valid Loss = 0.2647\n",
      "Iteration 19/150: Train Loss = 0.2173, Valid Loss = 0.2629\n",
      "Iteration 20/150: Train Loss = 0.2139, Valid Loss = 0.2608\n",
      "Iteration 21/150: Train Loss = 0.2110, Valid Loss = 0.2596\n",
      "Iteration 22/150: Train Loss = 0.2083, Valid Loss = 0.2586\n",
      "Iteration 23/150: Train Loss = 0.2059, Valid Loss = 0.2571\n",
      "Iteration 24/150: Train Loss = 0.2036, Valid Loss = 0.2559\n",
      "Iteration 25/150: Train Loss = 0.2018, Valid Loss = 0.2556\n",
      "Iteration 26/150: Train Loss = 0.2003, Valid Loss = 0.2547\n",
      "Iteration 27/150: Train Loss = 0.1986, Valid Loss = 0.2547\n",
      "Iteration 28/150: Train Loss = 0.1970, Valid Loss = 0.2546\n",
      "Iteration 29/150: Train Loss = 0.1956, Valid Loss = 0.2539\n",
      "Iteration 30/150: Train Loss = 0.1940, Valid Loss = 0.2535\n",
      "Iteration 31/150: Train Loss = 0.1927, Valid Loss = 0.2530\n",
      "Iteration 32/150: Train Loss = 0.1908, Valid Loss = 0.2529\n",
      "Iteration 33/150: Train Loss = 0.1896, Valid Loss = 0.2527\n",
      "Iteration 34/150: Train Loss = 0.1884, Valid Loss = 0.2523\n",
      "Iteration 35/150: Train Loss = 0.1872, Valid Loss = 0.2516\n",
      "Iteration 36/150: Train Loss = 0.1857, Valid Loss = 0.2514\n",
      "Iteration 37/150: Train Loss = 0.1845, Valid Loss = 0.2511\n",
      "Iteration 38/150: Train Loss = 0.1833, Valid Loss = 0.2507\n",
      "Iteration 39/150: Train Loss = 0.1820, Valid Loss = 0.2511\n",
      "Iteration 40/150: Train Loss = 0.1808, Valid Loss = 0.2513\n",
      "Iteration 41/150: Train Loss = 0.1801, Valid Loss = 0.2508\n",
      "Iteration 42/150: Train Loss = 0.1792, Valid Loss = 0.2503\n",
      "Iteration 43/150: Train Loss = 0.1783, Valid Loss = 0.2501\n",
      "Iteration 44/150: Train Loss = 0.1774, Valid Loss = 0.2492\n",
      "Iteration 45/150: Train Loss = 0.1763, Valid Loss = 0.2497\n",
      "Iteration 46/150: Train Loss = 0.1752, Valid Loss = 0.2489\n",
      "Iteration 47/150: Train Loss = 0.1746, Valid Loss = 0.2490\n",
      "Iteration 48/150: Train Loss = 0.1739, Valid Loss = 0.2487\n",
      "Iteration 49/150: Train Loss = 0.1730, Valid Loss = 0.2484\n",
      "Iteration 50/150: Train Loss = 0.1718, Valid Loss = 0.2483\n",
      "Iteration 51/150: Train Loss = 0.1711, Valid Loss = 0.2487\n",
      "Iteration 52/150: Train Loss = 0.1707, Valid Loss = 0.2487\n",
      "Iteration 53/150: Train Loss = 0.1699, Valid Loss = 0.2484\n",
      "Iteration 54/150: Train Loss = 0.1691, Valid Loss = 0.2480\n",
      "Iteration 55/150: Train Loss = 0.1685, Valid Loss = 0.2477\n",
      "Iteration 56/150: Train Loss = 0.1675, Valid Loss = 0.2471\n",
      "Iteration 57/150: Train Loss = 0.1668, Valid Loss = 0.2469\n",
      "Iteration 58/150: Train Loss = 0.1659, Valid Loss = 0.2463\n",
      "Iteration 59/150: Train Loss = 0.1652, Valid Loss = 0.2464\n",
      "Iteration 60/150: Train Loss = 0.1646, Valid Loss = 0.2467\n",
      "Iteration 61/150: Train Loss = 0.1643, Valid Loss = 0.2470\n",
      "Iteration 62/150: Train Loss = 0.1635, Valid Loss = 0.2469\n",
      "Iteration 63/150: Train Loss = 0.1628, Valid Loss = 0.2473\n",
      "Iteration 64/150: Train Loss = 0.1623, Valid Loss = 0.2474\n",
      "Iteration 65/150: Train Loss = 0.1617, Valid Loss = 0.2473\n",
      "Iteration 66/150: Train Loss = 0.1611, Valid Loss = 0.2470\n",
      "Iteration 67/150: Train Loss = 0.1606, Valid Loss = 0.2471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:49:26,647] Trial 11 finished with value: 0.962629208941964 and parameters: {'max_depth': 10, 'min_samples_leaf': 20, 'n_estimators': 150, 'learning_rate': 0.9231962103991376, 'subsample': 0.9793108424646119}. Best is trial 6 with value: 0.965235693124235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 68/150: Train Loss = 0.1601, Valid Loss = 0.2474\n",
      "Early stopping triggered.\n",
      "Iteration 1/115: Train Loss = 0.5416, Valid Loss = 0.5466\n",
      "Iteration 2/115: Train Loss = 0.4509, Valid Loss = 0.4609\n",
      "Iteration 3/115: Train Loss = 0.3945, Valid Loss = 0.4074\n",
      "Iteration 4/115: Train Loss = 0.3561, Valid Loss = 0.3714\n",
      "Iteration 5/115: Train Loss = 0.3280, Valid Loss = 0.3464\n",
      "Iteration 6/115: Train Loss = 0.3085, Valid Loss = 0.3281\n",
      "Iteration 7/115: Train Loss = 0.2919, Valid Loss = 0.3146\n",
      "Iteration 8/115: Train Loss = 0.2802, Valid Loss = 0.3058\n",
      "Iteration 9/115: Train Loss = 0.2692, Valid Loss = 0.2979\n",
      "Iteration 10/115: Train Loss = 0.2604, Valid Loss = 0.2899\n",
      "Iteration 11/115: Train Loss = 0.2529, Valid Loss = 0.2836\n",
      "Iteration 12/115: Train Loss = 0.2465, Valid Loss = 0.2799\n",
      "Iteration 13/115: Train Loss = 0.2409, Valid Loss = 0.2758\n",
      "Iteration 14/115: Train Loss = 0.2356, Valid Loss = 0.2722\n",
      "Iteration 15/115: Train Loss = 0.2312, Valid Loss = 0.2700\n",
      "Iteration 16/115: Train Loss = 0.2274, Valid Loss = 0.2682\n",
      "Iteration 17/115: Train Loss = 0.2242, Valid Loss = 0.2657\n",
      "Iteration 18/115: Train Loss = 0.2210, Valid Loss = 0.2646\n",
      "Iteration 19/115: Train Loss = 0.2178, Valid Loss = 0.2627\n",
      "Iteration 20/115: Train Loss = 0.2148, Valid Loss = 0.2609\n",
      "Iteration 21/115: Train Loss = 0.2123, Valid Loss = 0.2601\n",
      "Iteration 22/115: Train Loss = 0.2095, Valid Loss = 0.2588\n",
      "Iteration 23/115: Train Loss = 0.2072, Valid Loss = 0.2575\n",
      "Iteration 24/115: Train Loss = 0.2049, Valid Loss = 0.2564\n",
      "Iteration 25/115: Train Loss = 0.2031, Valid Loss = 0.2557\n",
      "Iteration 26/115: Train Loss = 0.2016, Valid Loss = 0.2545\n",
      "Iteration 27/115: Train Loss = 0.1998, Valid Loss = 0.2534\n",
      "Iteration 28/115: Train Loss = 0.1983, Valid Loss = 0.2528\n",
      "Iteration 29/115: Train Loss = 0.1971, Valid Loss = 0.2526\n",
      "Iteration 30/115: Train Loss = 0.1956, Valid Loss = 0.2517\n",
      "Iteration 31/115: Train Loss = 0.1945, Valid Loss = 0.2508\n",
      "Iteration 32/115: Train Loss = 0.1933, Valid Loss = 0.2495\n",
      "Iteration 33/115: Train Loss = 0.1922, Valid Loss = 0.2496\n",
      "Iteration 34/115: Train Loss = 0.1911, Valid Loss = 0.2495\n",
      "Iteration 35/115: Train Loss = 0.1900, Valid Loss = 0.2491\n",
      "Iteration 36/115: Train Loss = 0.1887, Valid Loss = 0.2489\n",
      "Iteration 37/115: Train Loss = 0.1876, Valid Loss = 0.2483\n",
      "Iteration 38/115: Train Loss = 0.1867, Valid Loss = 0.2488\n",
      "Iteration 39/115: Train Loss = 0.1860, Valid Loss = 0.2483\n",
      "Iteration 40/115: Train Loss = 0.1852, Valid Loss = 0.2477\n",
      "Iteration 41/115: Train Loss = 0.1843, Valid Loss = 0.2478\n",
      "Iteration 42/115: Train Loss = 0.1834, Valid Loss = 0.2475\n",
      "Iteration 43/115: Train Loss = 0.1825, Valid Loss = 0.2476\n",
      "Iteration 44/115: Train Loss = 0.1816, Valid Loss = 0.2471\n",
      "Iteration 45/115: Train Loss = 0.1805, Valid Loss = 0.2471\n",
      "Iteration 46/115: Train Loss = 0.1796, Valid Loss = 0.2470\n",
      "Iteration 47/115: Train Loss = 0.1791, Valid Loss = 0.2471\n",
      "Iteration 48/115: Train Loss = 0.1782, Valid Loss = 0.2466\n",
      "Iteration 49/115: Train Loss = 0.1774, Valid Loss = 0.2465\n",
      "Iteration 50/115: Train Loss = 0.1766, Valid Loss = 0.2462\n",
      "Iteration 51/115: Train Loss = 0.1760, Valid Loss = 0.2463\n",
      "Iteration 52/115: Train Loss = 0.1748, Valid Loss = 0.2466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:49:31,044] Trial 12 finished with value: 0.962553332647015 and parameters: {'max_depth': 9, 'min_samples_leaf': 20, 'n_estimators': 115, 'learning_rate': 0.9941715670538087, 'subsample': 0.9929793647270659}. Best is trial 6 with value: 0.965235693124235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 53/115: Train Loss = 0.1743, Valid Loss = 0.2464\n",
      "Iteration 54/115: Train Loss = 0.1736, Valid Loss = 0.2472\n",
      "Early stopping triggered.\n",
      "Iteration 1/120: Train Loss = 0.6174, Valid Loss = 0.6197\n",
      "Iteration 2/120: Train Loss = 0.5567, Valid Loss = 0.5617\n",
      "Iteration 3/120: Train Loss = 0.5079, Valid Loss = 0.5145\n",
      "Iteration 4/120: Train Loss = 0.4693, Valid Loss = 0.4771\n",
      "Iteration 5/120: Train Loss = 0.4386, Valid Loss = 0.4482\n",
      "Iteration 6/120: Train Loss = 0.4129, Valid Loss = 0.4242\n",
      "Iteration 7/120: Train Loss = 0.3916, Valid Loss = 0.4046\n",
      "Iteration 8/120: Train Loss = 0.3740, Valid Loss = 0.3881\n",
      "Iteration 9/120: Train Loss = 0.3586, Valid Loss = 0.3736\n",
      "Iteration 10/120: Train Loss = 0.3450, Valid Loss = 0.3612\n",
      "Iteration 11/120: Train Loss = 0.3337, Valid Loss = 0.3504\n",
      "Iteration 12/120: Train Loss = 0.3235, Valid Loss = 0.3413\n",
      "Iteration 13/120: Train Loss = 0.3146, Valid Loss = 0.3331\n",
      "Iteration 14/120: Train Loss = 0.3064, Valid Loss = 0.3253\n",
      "Iteration 15/120: Train Loss = 0.2993, Valid Loss = 0.3190\n",
      "Iteration 16/120: Train Loss = 0.2930, Valid Loss = 0.3130\n",
      "Iteration 17/120: Train Loss = 0.2874, Valid Loss = 0.3084\n",
      "Iteration 18/120: Train Loss = 0.2821, Valid Loss = 0.3037\n",
      "Iteration 19/120: Train Loss = 0.2768, Valid Loss = 0.2999\n",
      "Iteration 20/120: Train Loss = 0.2724, Valid Loss = 0.2964\n",
      "Iteration 21/120: Train Loss = 0.2685, Valid Loss = 0.2934\n",
      "Iteration 22/120: Train Loss = 0.2646, Valid Loss = 0.2904\n",
      "Iteration 23/120: Train Loss = 0.2612, Valid Loss = 0.2876\n",
      "Iteration 24/120: Train Loss = 0.2580, Valid Loss = 0.2845\n",
      "Iteration 25/120: Train Loss = 0.2547, Valid Loss = 0.2820\n",
      "Iteration 26/120: Train Loss = 0.2519, Valid Loss = 0.2796\n",
      "Iteration 27/120: Train Loss = 0.2495, Valid Loss = 0.2773\n",
      "Iteration 28/120: Train Loss = 0.2469, Valid Loss = 0.2752\n",
      "Iteration 29/120: Train Loss = 0.2446, Valid Loss = 0.2733\n",
      "Iteration 30/120: Train Loss = 0.2422, Valid Loss = 0.2719\n",
      "Iteration 31/120: Train Loss = 0.2400, Valid Loss = 0.2705\n",
      "Iteration 32/120: Train Loss = 0.2379, Valid Loss = 0.2690\n",
      "Iteration 33/120: Train Loss = 0.2359, Valid Loss = 0.2677\n",
      "Iteration 34/120: Train Loss = 0.2342, Valid Loss = 0.2663\n",
      "Iteration 35/120: Train Loss = 0.2325, Valid Loss = 0.2650\n",
      "Iteration 36/120: Train Loss = 0.2307, Valid Loss = 0.2636\n",
      "Iteration 37/120: Train Loss = 0.2291, Valid Loss = 0.2629\n",
      "Iteration 38/120: Train Loss = 0.2277, Valid Loss = 0.2620\n",
      "Iteration 39/120: Train Loss = 0.2264, Valid Loss = 0.2612\n",
      "Iteration 40/120: Train Loss = 0.2251, Valid Loss = 0.2601\n",
      "Iteration 41/120: Train Loss = 0.2240, Valid Loss = 0.2588\n",
      "Iteration 42/120: Train Loss = 0.2226, Valid Loss = 0.2583\n",
      "Iteration 43/120: Train Loss = 0.2215, Valid Loss = 0.2576\n",
      "Iteration 44/120: Train Loss = 0.2203, Valid Loss = 0.2565\n",
      "Iteration 45/120: Train Loss = 0.2191, Valid Loss = 0.2560\n",
      "Iteration 46/120: Train Loss = 0.2179, Valid Loss = 0.2554\n",
      "Iteration 47/120: Train Loss = 0.2168, Valid Loss = 0.2546\n",
      "Iteration 48/120: Train Loss = 0.2158, Valid Loss = 0.2540\n",
      "Iteration 49/120: Train Loss = 0.2149, Valid Loss = 0.2532\n",
      "Iteration 50/120: Train Loss = 0.2140, Valid Loss = 0.2528\n",
      "Iteration 51/120: Train Loss = 0.2131, Valid Loss = 0.2522\n",
      "Iteration 52/120: Train Loss = 0.2121, Valid Loss = 0.2513\n",
      "Iteration 53/120: Train Loss = 0.2113, Valid Loss = 0.2508\n",
      "Iteration 54/120: Train Loss = 0.2103, Valid Loss = 0.2504\n",
      "Iteration 55/120: Train Loss = 0.2095, Valid Loss = 0.2502\n",
      "Iteration 56/120: Train Loss = 0.2087, Valid Loss = 0.2499\n",
      "Iteration 57/120: Train Loss = 0.2080, Valid Loss = 0.2494\n",
      "Iteration 58/120: Train Loss = 0.2073, Valid Loss = 0.2486\n",
      "Iteration 59/120: Train Loss = 0.2064, Valid Loss = 0.2481\n",
      "Iteration 60/120: Train Loss = 0.2056, Valid Loss = 0.2478\n",
      "Iteration 61/120: Train Loss = 0.2050, Valid Loss = 0.2476\n",
      "Iteration 62/120: Train Loss = 0.2044, Valid Loss = 0.2472\n",
      "Iteration 63/120: Train Loss = 0.2038, Valid Loss = 0.2468\n",
      "Iteration 64/120: Train Loss = 0.2032, Valid Loss = 0.2465\n",
      "Iteration 65/120: Train Loss = 0.2023, Valid Loss = 0.2463\n",
      "Iteration 66/120: Train Loss = 0.2018, Valid Loss = 0.2458\n",
      "Iteration 67/120: Train Loss = 0.2011, Valid Loss = 0.2452\n",
      "Iteration 68/120: Train Loss = 0.2006, Valid Loss = 0.2450\n",
      "Iteration 69/120: Train Loss = 0.2002, Valid Loss = 0.2449\n",
      "Iteration 70/120: Train Loss = 0.1996, Valid Loss = 0.2445\n",
      "Iteration 71/120: Train Loss = 0.1990, Valid Loss = 0.2440\n",
      "Iteration 72/120: Train Loss = 0.1985, Valid Loss = 0.2437\n",
      "Iteration 73/120: Train Loss = 0.1981, Valid Loss = 0.2437\n",
      "Iteration 74/120: Train Loss = 0.1974, Valid Loss = 0.2435\n",
      "Iteration 75/120: Train Loss = 0.1969, Valid Loss = 0.2432\n",
      "Iteration 76/120: Train Loss = 0.1964, Valid Loss = 0.2429\n",
      "Iteration 77/120: Train Loss = 0.1961, Valid Loss = 0.2428\n",
      "Iteration 78/120: Train Loss = 0.1956, Valid Loss = 0.2426\n",
      "Iteration 79/120: Train Loss = 0.1952, Valid Loss = 0.2428\n",
      "Iteration 80/120: Train Loss = 0.1947, Valid Loss = 0.2430\n",
      "Iteration 81/120: Train Loss = 0.1943, Valid Loss = 0.2428\n",
      "Iteration 82/120: Train Loss = 0.1937, Valid Loss = 0.2430\n",
      "Iteration 83/120: Train Loss = 0.1931, Valid Loss = 0.2431\n",
      "Iteration 84/120: Train Loss = 0.1927, Valid Loss = 0.2430\n",
      "Iteration 85/120: Train Loss = 0.1923, Valid Loss = 0.2427\n",
      "Iteration 86/120: Train Loss = 0.1919, Valid Loss = 0.2428\n",
      "Iteration 87/120: Train Loss = 0.1915, Valid Loss = 0.2428\n",
      "Iteration 88/120: Train Loss = 0.1911, Valid Loss = 0.2427\n",
      "Iteration 89/120: Train Loss = 0.1908, Valid Loss = 0.2425\n",
      "Iteration 90/120: Train Loss = 0.1904, Valid Loss = 0.2424\n",
      "Iteration 91/120: Train Loss = 0.1899, Valid Loss = 0.2423\n",
      "Iteration 92/120: Train Loss = 0.1895, Valid Loss = 0.2420\n",
      "Iteration 93/120: Train Loss = 0.1893, Valid Loss = 0.2417\n",
      "Iteration 94/120: Train Loss = 0.1889, Valid Loss = 0.2416\n",
      "Iteration 95/120: Train Loss = 0.1886, Valid Loss = 0.2414\n",
      "Iteration 96/120: Train Loss = 0.1884, Valid Loss = 0.2413\n",
      "Iteration 97/120: Train Loss = 0.1881, Valid Loss = 0.2412\n",
      "Iteration 98/120: Train Loss = 0.1878, Valid Loss = 0.2413\n",
      "Iteration 99/120: Train Loss = 0.1874, Valid Loss = 0.2413\n",
      "Iteration 100/120: Train Loss = 0.1871, Valid Loss = 0.2411\n",
      "Iteration 101/120: Train Loss = 0.1867, Valid Loss = 0.2411\n",
      "Iteration 102/120: Train Loss = 0.1862, Valid Loss = 0.2409\n",
      "Iteration 103/120: Train Loss = 0.1858, Valid Loss = 0.2411\n",
      "Iteration 104/120: Train Loss = 0.1854, Valid Loss = 0.2412\n",
      "Iteration 105/120: Train Loss = 0.1851, Valid Loss = 0.2412\n",
      "Iteration 106/120: Train Loss = 0.1846, Valid Loss = 0.2411\n",
      "Iteration 107/120: Train Loss = 0.1843, Valid Loss = 0.2409\n",
      "Iteration 108/120: Train Loss = 0.1839, Valid Loss = 0.2411\n",
      "Iteration 109/120: Train Loss = 0.1837, Valid Loss = 0.2407\n",
      "Iteration 110/120: Train Loss = 0.1835, Valid Loss = 0.2406\n",
      "Iteration 111/120: Train Loss = 0.1832, Valid Loss = 0.2408\n",
      "Iteration 112/120: Train Loss = 0.1829, Valid Loss = 0.2404\n",
      "Iteration 113/120: Train Loss = 0.1826, Valid Loss = 0.2404\n",
      "Iteration 114/120: Train Loss = 0.1822, Valid Loss = 0.2404\n",
      "Iteration 115/120: Train Loss = 0.1819, Valid Loss = 0.2403\n",
      "Iteration 116/120: Train Loss = 0.1817, Valid Loss = 0.2404\n",
      "Iteration 117/120: Train Loss = 0.1814, Valid Loss = 0.2402\n",
      "Iteration 118/120: Train Loss = 0.1811, Valid Loss = 0.2401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:49:37,888] Trial 13 finished with value: 0.9651003359146524 and parameters: {'max_depth': 8, 'min_samples_leaf': 16, 'n_estimators': 120, 'learning_rate': 0.4730918224161353, 'subsample': 0.7348167539791466}. Best is trial 6 with value: 0.965235693124235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 119/120: Train Loss = 0.1809, Valid Loss = 0.2404\n",
      "Iteration 120/120: Train Loss = 0.1805, Valid Loss = 0.2407\n",
      "Iteration 1/115: Train Loss = 0.6800, Valid Loss = 0.6804\n",
      "Iteration 2/115: Train Loss = 0.6675, Valid Loss = 0.6683\n",
      "Iteration 3/115: Train Loss = 0.6557, Valid Loss = 0.6570\n",
      "Iteration 4/115: Train Loss = 0.6445, Valid Loss = 0.6461\n",
      "Iteration 5/115: Train Loss = 0.6340, Valid Loss = 0.6361\n",
      "Iteration 6/115: Train Loss = 0.6241, Valid Loss = 0.6265\n",
      "Iteration 7/115: Train Loss = 0.6146, Valid Loss = 0.6175\n",
      "Iteration 8/115: Train Loss = 0.6059, Valid Loss = 0.6092\n",
      "Iteration 9/115: Train Loss = 0.5974, Valid Loss = 0.6010\n",
      "Iteration 10/115: Train Loss = 0.5895, Valid Loss = 0.5934\n",
      "Iteration 11/115: Train Loss = 0.5821, Valid Loss = 0.5864\n",
      "Iteration 12/115: Train Loss = 0.5749, Valid Loss = 0.5795\n",
      "Iteration 13/115: Train Loss = 0.5681, Valid Loss = 0.5730\n",
      "Iteration 14/115: Train Loss = 0.5617, Valid Loss = 0.5668\n",
      "Iteration 15/115: Train Loss = 0.5555, Valid Loss = 0.5608\n",
      "Iteration 16/115: Train Loss = 0.5498, Valid Loss = 0.5553\n",
      "Iteration 17/115: Train Loss = 0.5444, Valid Loss = 0.5501\n",
      "Iteration 18/115: Train Loss = 0.5391, Valid Loss = 0.5452\n",
      "Iteration 19/115: Train Loss = 0.5342, Valid Loss = 0.5404\n",
      "Iteration 20/115: Train Loss = 0.5295, Valid Loss = 0.5357\n",
      "Iteration 21/115: Train Loss = 0.5249, Valid Loss = 0.5313\n",
      "Iteration 22/115: Train Loss = 0.5205, Valid Loss = 0.5271\n",
      "Iteration 23/115: Train Loss = 0.5163, Valid Loss = 0.5233\n",
      "Iteration 24/115: Train Loss = 0.5124, Valid Loss = 0.5194\n",
      "Iteration 25/115: Train Loss = 0.5087, Valid Loss = 0.5157\n",
      "Iteration 26/115: Train Loss = 0.5051, Valid Loss = 0.5122\n",
      "Iteration 27/115: Train Loss = 0.5017, Valid Loss = 0.5088\n",
      "Iteration 28/115: Train Loss = 0.4983, Valid Loss = 0.5057\n",
      "Iteration 29/115: Train Loss = 0.4951, Valid Loss = 0.5026\n",
      "Iteration 30/115: Train Loss = 0.4920, Valid Loss = 0.4997\n",
      "Iteration 31/115: Train Loss = 0.4890, Valid Loss = 0.4971\n",
      "Iteration 32/115: Train Loss = 0.4861, Valid Loss = 0.4941\n",
      "Iteration 33/115: Train Loss = 0.4833, Valid Loss = 0.4916\n",
      "Iteration 34/115: Train Loss = 0.4806, Valid Loss = 0.4889\n",
      "Iteration 35/115: Train Loss = 0.4779, Valid Loss = 0.4863\n",
      "Iteration 36/115: Train Loss = 0.4754, Valid Loss = 0.4839\n",
      "Iteration 37/115: Train Loss = 0.4728, Valid Loss = 0.4814\n",
      "Iteration 38/115: Train Loss = 0.4703, Valid Loss = 0.4790\n",
      "Iteration 39/115: Train Loss = 0.4679, Valid Loss = 0.4768\n",
      "Iteration 40/115: Train Loss = 0.4656, Valid Loss = 0.4744\n",
      "Iteration 41/115: Train Loss = 0.4633, Valid Loss = 0.4723\n",
      "Iteration 42/115: Train Loss = 0.4611, Valid Loss = 0.4703\n",
      "Iteration 43/115: Train Loss = 0.4590, Valid Loss = 0.4681\n",
      "Iteration 44/115: Train Loss = 0.4569, Valid Loss = 0.4660\n",
      "Iteration 45/115: Train Loss = 0.4548, Valid Loss = 0.4641\n",
      "Iteration 46/115: Train Loss = 0.4528, Valid Loss = 0.4621\n",
      "Iteration 47/115: Train Loss = 0.4509, Valid Loss = 0.4602\n",
      "Iteration 48/115: Train Loss = 0.4489, Valid Loss = 0.4585\n",
      "Iteration 49/115: Train Loss = 0.4470, Valid Loss = 0.4568\n",
      "Iteration 50/115: Train Loss = 0.4452, Valid Loss = 0.4551\n",
      "Iteration 51/115: Train Loss = 0.4435, Valid Loss = 0.4533\n",
      "Iteration 52/115: Train Loss = 0.4417, Valid Loss = 0.4518\n",
      "Iteration 53/115: Train Loss = 0.4400, Valid Loss = 0.4499\n",
      "Iteration 54/115: Train Loss = 0.4383, Valid Loss = 0.4484\n",
      "Iteration 55/115: Train Loss = 0.4367, Valid Loss = 0.4470\n",
      "Iteration 56/115: Train Loss = 0.4351, Valid Loss = 0.4454\n",
      "Iteration 57/115: Train Loss = 0.4335, Valid Loss = 0.4438\n",
      "Iteration 58/115: Train Loss = 0.4320, Valid Loss = 0.4425\n",
      "Iteration 59/115: Train Loss = 0.4305, Valid Loss = 0.4409\n",
      "Iteration 60/115: Train Loss = 0.4290, Valid Loss = 0.4393\n",
      "Iteration 61/115: Train Loss = 0.4276, Valid Loss = 0.4380\n",
      "Iteration 62/115: Train Loss = 0.4262, Valid Loss = 0.4366\n",
      "Iteration 63/115: Train Loss = 0.4248, Valid Loss = 0.4353\n",
      "Iteration 64/115: Train Loss = 0.4235, Valid Loss = 0.4341\n",
      "Iteration 65/115: Train Loss = 0.4222, Valid Loss = 0.4327\n",
      "Iteration 66/115: Train Loss = 0.4209, Valid Loss = 0.4312\n",
      "Iteration 67/115: Train Loss = 0.4196, Valid Loss = 0.4301\n",
      "Iteration 68/115: Train Loss = 0.4184, Valid Loss = 0.4290\n",
      "Iteration 69/115: Train Loss = 0.4171, Valid Loss = 0.4278\n",
      "Iteration 70/115: Train Loss = 0.4160, Valid Loss = 0.4267\n",
      "Iteration 71/115: Train Loss = 0.4148, Valid Loss = 0.4254\n",
      "Iteration 72/115: Train Loss = 0.4136, Valid Loss = 0.4243\n",
      "Iteration 73/115: Train Loss = 0.4125, Valid Loss = 0.4233\n",
      "Iteration 74/115: Train Loss = 0.4114, Valid Loss = 0.4222\n",
      "Iteration 75/115: Train Loss = 0.4103, Valid Loss = 0.4210\n",
      "Iteration 76/115: Train Loss = 0.4092, Valid Loss = 0.4198\n",
      "Iteration 77/115: Train Loss = 0.4082, Valid Loss = 0.4189\n",
      "Iteration 78/115: Train Loss = 0.4072, Valid Loss = 0.4179\n",
      "Iteration 79/115: Train Loss = 0.4062, Valid Loss = 0.4168\n",
      "Iteration 80/115: Train Loss = 0.4052, Valid Loss = 0.4159\n",
      "Iteration 81/115: Train Loss = 0.4042, Valid Loss = 0.4150\n",
      "Iteration 82/115: Train Loss = 0.4032, Valid Loss = 0.4140\n",
      "Iteration 83/115: Train Loss = 0.4023, Valid Loss = 0.4132\n",
      "Iteration 84/115: Train Loss = 0.4014, Valid Loss = 0.4123\n",
      "Iteration 85/115: Train Loss = 0.4005, Valid Loss = 0.4114\n",
      "Iteration 86/115: Train Loss = 0.3996, Valid Loss = 0.4106\n",
      "Iteration 87/115: Train Loss = 0.3988, Valid Loss = 0.4098\n",
      "Iteration 88/115: Train Loss = 0.3979, Valid Loss = 0.4089\n",
      "Iteration 89/115: Train Loss = 0.3970, Valid Loss = 0.4081\n",
      "Iteration 90/115: Train Loss = 0.3962, Valid Loss = 0.4073\n",
      "Iteration 91/115: Train Loss = 0.3953, Valid Loss = 0.4063\n",
      "Iteration 92/115: Train Loss = 0.3945, Valid Loss = 0.4056\n",
      "Iteration 93/115: Train Loss = 0.3937, Valid Loss = 0.4049\n",
      "Iteration 94/115: Train Loss = 0.3929, Valid Loss = 0.4040\n",
      "Iteration 95/115: Train Loss = 0.3921, Valid Loss = 0.4032\n",
      "Iteration 96/115: Train Loss = 0.3914, Valid Loss = 0.4026\n",
      "Iteration 97/115: Train Loss = 0.3906, Valid Loss = 0.4019\n",
      "Iteration 98/115: Train Loss = 0.3898, Valid Loss = 0.4011\n",
      "Iteration 99/115: Train Loss = 0.3891, Valid Loss = 0.4004\n",
      "Iteration 100/115: Train Loss = 0.3883, Valid Loss = 0.3997\n",
      "Iteration 101/115: Train Loss = 0.3876, Valid Loss = 0.3990\n",
      "Iteration 102/115: Train Loss = 0.3869, Valid Loss = 0.3983\n",
      "Iteration 103/115: Train Loss = 0.3862, Valid Loss = 0.3976\n",
      "Iteration 104/115: Train Loss = 0.3855, Valid Loss = 0.3970\n",
      "Iteration 105/115: Train Loss = 0.3848, Valid Loss = 0.3962\n",
      "Iteration 106/115: Train Loss = 0.3841, Valid Loss = 0.3956\n",
      "Iteration 107/115: Train Loss = 0.3834, Valid Loss = 0.3949\n",
      "Iteration 108/115: Train Loss = 0.3827, Valid Loss = 0.3942\n",
      "Iteration 109/115: Train Loss = 0.3821, Valid Loss = 0.3936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:49:40,537] Trial 14 finished with value: 0.924732240562171 and parameters: {'max_depth': 1, 'min_samples_leaf': 13, 'n_estimators': 115, 'learning_rate': 0.11831499299537956, 'subsample': 0.7238635172477894}. Best is trial 6 with value: 0.965235693124235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 110/115: Train Loss = 0.3814, Valid Loss = 0.3928\n",
      "Iteration 111/115: Train Loss = 0.3808, Valid Loss = 0.3922\n",
      "Iteration 112/115: Train Loss = 0.3801, Valid Loss = 0.3916\n",
      "Iteration 113/115: Train Loss = 0.3795, Valid Loss = 0.3908\n",
      "Iteration 114/115: Train Loss = 0.3788, Valid Loss = 0.3902\n",
      "Iteration 115/115: Train Loss = 0.3782, Valid Loss = 0.3895\n",
      "Iteration 1/115: Train Loss = 0.6896, Valid Loss = 0.6897\n",
      "Iteration 2/115: Train Loss = 0.6860, Valid Loss = 0.6863\n",
      "Iteration 3/115: Train Loss = 0.6825, Valid Loss = 0.6829\n",
      "Iteration 4/115: Train Loss = 0.6791, Valid Loss = 0.6795\n",
      "Iteration 5/115: Train Loss = 0.6756, Valid Loss = 0.6762\n",
      "Iteration 6/115: Train Loss = 0.6722, Valid Loss = 0.6728\n",
      "Iteration 7/115: Train Loss = 0.6689, Valid Loss = 0.6696\n",
      "Iteration 8/115: Train Loss = 0.6655, Valid Loss = 0.6663\n",
      "Iteration 9/115: Train Loss = 0.6622, Valid Loss = 0.6631\n",
      "Iteration 10/115: Train Loss = 0.6589, Valid Loss = 0.6599\n",
      "Iteration 11/115: Train Loss = 0.6557, Valid Loss = 0.6567\n",
      "Iteration 12/115: Train Loss = 0.6525, Valid Loss = 0.6536\n",
      "Iteration 13/115: Train Loss = 0.6493, Valid Loss = 0.6506\n",
      "Iteration 14/115: Train Loss = 0.6462, Valid Loss = 0.6475\n",
      "Iteration 15/115: Train Loss = 0.6431, Valid Loss = 0.6445\n",
      "Iteration 16/115: Train Loss = 0.6400, Valid Loss = 0.6415\n",
      "Iteration 17/115: Train Loss = 0.6369, Valid Loss = 0.6386\n",
      "Iteration 18/115: Train Loss = 0.6339, Valid Loss = 0.6356\n",
      "Iteration 19/115: Train Loss = 0.6309, Valid Loss = 0.6327\n",
      "Iteration 20/115: Train Loss = 0.6280, Valid Loss = 0.6298\n",
      "Iteration 21/115: Train Loss = 0.6251, Valid Loss = 0.6270\n",
      "Iteration 22/115: Train Loss = 0.6222, Valid Loss = 0.6242\n",
      "Iteration 23/115: Train Loss = 0.6193, Valid Loss = 0.6214\n",
      "Iteration 24/115: Train Loss = 0.6165, Valid Loss = 0.6186\n",
      "Iteration 25/115: Train Loss = 0.6137, Valid Loss = 0.6159\n",
      "Iteration 26/115: Train Loss = 0.6108, Valid Loss = 0.6131\n",
      "Iteration 27/115: Train Loss = 0.6080, Valid Loss = 0.6104\n",
      "Iteration 28/115: Train Loss = 0.6053, Valid Loss = 0.6078\n",
      "Iteration 29/115: Train Loss = 0.6026, Valid Loss = 0.6052\n",
      "Iteration 30/115: Train Loss = 0.5999, Valid Loss = 0.6026\n",
      "Iteration 31/115: Train Loss = 0.5973, Valid Loss = 0.6000\n",
      "Iteration 32/115: Train Loss = 0.5947, Valid Loss = 0.5975\n",
      "Iteration 33/115: Train Loss = 0.5921, Valid Loss = 0.5950\n",
      "Iteration 34/115: Train Loss = 0.5895, Valid Loss = 0.5925\n",
      "Iteration 35/115: Train Loss = 0.5870, Valid Loss = 0.5900\n",
      "Iteration 36/115: Train Loss = 0.5844, Valid Loss = 0.5876\n",
      "Iteration 37/115: Train Loss = 0.5819, Valid Loss = 0.5852\n",
      "Iteration 38/115: Train Loss = 0.5795, Valid Loss = 0.5828\n",
      "Iteration 39/115: Train Loss = 0.5770, Valid Loss = 0.5805\n",
      "Iteration 40/115: Train Loss = 0.5746, Valid Loss = 0.5781\n",
      "Iteration 41/115: Train Loss = 0.5722, Valid Loss = 0.5758\n",
      "Iteration 42/115: Train Loss = 0.5698, Valid Loss = 0.5735\n",
      "Iteration 43/115: Train Loss = 0.5675, Valid Loss = 0.5713\n",
      "Iteration 44/115: Train Loss = 0.5651, Valid Loss = 0.5690\n",
      "Iteration 45/115: Train Loss = 0.5628, Valid Loss = 0.5668\n",
      "Iteration 46/115: Train Loss = 0.5605, Valid Loss = 0.5645\n",
      "Iteration 47/115: Train Loss = 0.5583, Valid Loss = 0.5623\n",
      "Iteration 48/115: Train Loss = 0.5560, Valid Loss = 0.5601\n",
      "Iteration 49/115: Train Loss = 0.5538, Valid Loss = 0.5580\n",
      "Iteration 50/115: Train Loss = 0.5516, Valid Loss = 0.5558\n",
      "Iteration 51/115: Train Loss = 0.5494, Valid Loss = 0.5538\n",
      "Iteration 52/115: Train Loss = 0.5472, Valid Loss = 0.5516\n",
      "Iteration 53/115: Train Loss = 0.5451, Valid Loss = 0.5496\n",
      "Iteration 54/115: Train Loss = 0.5430, Valid Loss = 0.5475\n",
      "Iteration 55/115: Train Loss = 0.5409, Valid Loss = 0.5455\n",
      "Iteration 56/115: Train Loss = 0.5388, Valid Loss = 0.5435\n",
      "Iteration 57/115: Train Loss = 0.5367, Valid Loss = 0.5416\n",
      "Iteration 58/115: Train Loss = 0.5347, Valid Loss = 0.5396\n",
      "Iteration 59/115: Train Loss = 0.5326, Valid Loss = 0.5376\n",
      "Iteration 60/115: Train Loss = 0.5307, Valid Loss = 0.5357\n",
      "Iteration 61/115: Train Loss = 0.5287, Valid Loss = 0.5338\n",
      "Iteration 62/115: Train Loss = 0.5267, Valid Loss = 0.5319\n",
      "Iteration 63/115: Train Loss = 0.5248, Valid Loss = 0.5300\n",
      "Iteration 64/115: Train Loss = 0.5228, Valid Loss = 0.5281\n",
      "Iteration 65/115: Train Loss = 0.5209, Valid Loss = 0.5263\n",
      "Iteration 66/115: Train Loss = 0.5190, Valid Loss = 0.5245\n",
      "Iteration 67/115: Train Loss = 0.5171, Valid Loss = 0.5227\n",
      "Iteration 68/115: Train Loss = 0.5152, Valid Loss = 0.5208\n",
      "Iteration 69/115: Train Loss = 0.5134, Valid Loss = 0.5190\n",
      "Iteration 70/115: Train Loss = 0.5116, Valid Loss = 0.5173\n",
      "Iteration 71/115: Train Loss = 0.5098, Valid Loss = 0.5155\n",
      "Iteration 72/115: Train Loss = 0.5080, Valid Loss = 0.5138\n",
      "Iteration 73/115: Train Loss = 0.5062, Valid Loss = 0.5121\n",
      "Iteration 74/115: Train Loss = 0.5044, Valid Loss = 0.5104\n",
      "Iteration 75/115: Train Loss = 0.5026, Valid Loss = 0.5087\n",
      "Iteration 76/115: Train Loss = 0.5009, Valid Loss = 0.5071\n",
      "Iteration 77/115: Train Loss = 0.4992, Valid Loss = 0.5054\n",
      "Iteration 78/115: Train Loss = 0.4975, Valid Loss = 0.5038\n",
      "Iteration 79/115: Train Loss = 0.4958, Valid Loss = 0.5022\n",
      "Iteration 80/115: Train Loss = 0.4941, Valid Loss = 0.5006\n",
      "Iteration 81/115: Train Loss = 0.4925, Valid Loss = 0.4990\n",
      "Iteration 82/115: Train Loss = 0.4909, Valid Loss = 0.4975\n",
      "Iteration 83/115: Train Loss = 0.4892, Valid Loss = 0.4959\n",
      "Iteration 84/115: Train Loss = 0.4876, Valid Loss = 0.4943\n",
      "Iteration 85/115: Train Loss = 0.4860, Valid Loss = 0.4928\n",
      "Iteration 86/115: Train Loss = 0.4844, Valid Loss = 0.4913\n",
      "Iteration 87/115: Train Loss = 0.4829, Valid Loss = 0.4898\n",
      "Iteration 88/115: Train Loss = 0.4813, Valid Loss = 0.4883\n",
      "Iteration 89/115: Train Loss = 0.4798, Valid Loss = 0.4868\n",
      "Iteration 90/115: Train Loss = 0.4782, Valid Loss = 0.4853\n",
      "Iteration 91/115: Train Loss = 0.4768, Valid Loss = 0.4839\n",
      "Iteration 92/115: Train Loss = 0.4753, Valid Loss = 0.4824\n",
      "Iteration 93/115: Train Loss = 0.4738, Valid Loss = 0.4810\n",
      "Iteration 94/115: Train Loss = 0.4723, Valid Loss = 0.4796\n",
      "Iteration 95/115: Train Loss = 0.4708, Valid Loss = 0.4782\n",
      "Iteration 96/115: Train Loss = 0.4694, Valid Loss = 0.4768\n",
      "Iteration 97/115: Train Loss = 0.4680, Valid Loss = 0.4754\n",
      "Iteration 98/115: Train Loss = 0.4665, Valid Loss = 0.4741\n",
      "Iteration 99/115: Train Loss = 0.4651, Valid Loss = 0.4727\n",
      "Iteration 100/115: Train Loss = 0.4637, Valid Loss = 0.4714\n",
      "Iteration 101/115: Train Loss = 0.4623, Valid Loss = 0.4700\n",
      "Iteration 102/115: Train Loss = 0.4609, Valid Loss = 0.4687\n",
      "Iteration 103/115: Train Loss = 0.4595, Valid Loss = 0.4674\n",
      "Iteration 104/115: Train Loss = 0.4582, Valid Loss = 0.4662\n",
      "Iteration 105/115: Train Loss = 0.4568, Valid Loss = 0.4649\n",
      "Iteration 106/115: Train Loss = 0.4555, Valid Loss = 0.4636\n",
      "Iteration 107/115: Train Loss = 0.4542, Valid Loss = 0.4624\n",
      "Iteration 108/115: Train Loss = 0.4529, Valid Loss = 0.4612\n",
      "Iteration 109/115: Train Loss = 0.4516, Valid Loss = 0.4600\n",
      "Iteration 110/115: Train Loss = 0.4503, Valid Loss = 0.4587\n",
      "Iteration 111/115: Train Loss = 0.4490, Valid Loss = 0.4575\n",
      "Iteration 112/115: Train Loss = 0.4478, Valid Loss = 0.4563\n",
      "Iteration 113/115: Train Loss = 0.4465, Valid Loss = 0.4551\n",
      "Iteration 114/115: Train Loss = 0.4453, Valid Loss = 0.4540\n",
      "Iteration 115/115: Train Loss = 0.4440, Valid Loss = 0.4528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:49:48,105] Trial 15 finished with value: 0.958800315706393 and parameters: {'max_depth': 8, 'min_samples_leaf': 17, 'n_estimators': 115, 'learning_rate': 0.020860412871374332, 'subsample': 0.7088780128535194}. Best is trial 6 with value: 0.965235693124235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/100: Train Loss = 0.6929, Valid Loss = 0.6929\n",
      "Iteration 2/100: Train Loss = 0.6927, Valid Loss = 0.6927\n",
      "Iteration 3/100: Train Loss = 0.6925, Valid Loss = 0.6925\n",
      "Iteration 4/100: Train Loss = 0.6923, Valid Loss = 0.6923\n",
      "Iteration 5/100: Train Loss = 0.6920, Valid Loss = 0.6921\n",
      "Iteration 6/100: Train Loss = 0.6918, Valid Loss = 0.6919\n",
      "Iteration 7/100: Train Loss = 0.6916, Valid Loss = 0.6916\n",
      "Iteration 8/100: Train Loss = 0.6914, Valid Loss = 0.6914\n",
      "Iteration 9/100: Train Loss = 0.6911, Valid Loss = 0.6912\n",
      "Iteration 10/100: Train Loss = 0.6909, Valid Loss = 0.6910\n",
      "Iteration 11/100: Train Loss = 0.6907, Valid Loss = 0.6908\n",
      "Iteration 12/100: Train Loss = 0.6905, Valid Loss = 0.6906\n",
      "Iteration 13/100: Train Loss = 0.6903, Valid Loss = 0.6904\n",
      "Iteration 14/100: Train Loss = 0.6900, Valid Loss = 0.6901\n",
      "Iteration 15/100: Train Loss = 0.6898, Valid Loss = 0.6899\n",
      "Iteration 16/100: Train Loss = 0.6896, Valid Loss = 0.6897\n",
      "Iteration 17/100: Train Loss = 0.6894, Valid Loss = 0.6895\n",
      "Iteration 18/100: Train Loss = 0.6891, Valid Loss = 0.6893\n",
      "Iteration 19/100: Train Loss = 0.6889, Valid Loss = 0.6891\n",
      "Iteration 20/100: Train Loss = 0.6887, Valid Loss = 0.6889\n",
      "Iteration 21/100: Train Loss = 0.6885, Valid Loss = 0.6886\n",
      "Iteration 22/100: Train Loss = 0.6883, Valid Loss = 0.6884\n",
      "Iteration 23/100: Train Loss = 0.6880, Valid Loss = 0.6882\n",
      "Iteration 24/100: Train Loss = 0.6878, Valid Loss = 0.6880\n",
      "Iteration 25/100: Train Loss = 0.6876, Valid Loss = 0.6878\n",
      "Iteration 26/100: Train Loss = 0.6874, Valid Loss = 0.6876\n",
      "Iteration 27/100: Train Loss = 0.6872, Valid Loss = 0.6874\n",
      "Iteration 28/100: Train Loss = 0.6869, Valid Loss = 0.6871\n",
      "Iteration 29/100: Train Loss = 0.6867, Valid Loss = 0.6869\n",
      "Iteration 30/100: Train Loss = 0.6865, Valid Loss = 0.6867\n",
      "Iteration 31/100: Train Loss = 0.6863, Valid Loss = 0.6865\n",
      "Iteration 32/100: Train Loss = 0.6861, Valid Loss = 0.6863\n",
      "Iteration 33/100: Train Loss = 0.6858, Valid Loss = 0.6861\n",
      "Iteration 34/100: Train Loss = 0.6856, Valid Loss = 0.6859\n",
      "Iteration 35/100: Train Loss = 0.6854, Valid Loss = 0.6857\n",
      "Iteration 36/100: Train Loss = 0.6852, Valid Loss = 0.6854\n",
      "Iteration 37/100: Train Loss = 0.6850, Valid Loss = 0.6852\n",
      "Iteration 38/100: Train Loss = 0.6847, Valid Loss = 0.6850\n",
      "Iteration 39/100: Train Loss = 0.6845, Valid Loss = 0.6848\n",
      "Iteration 40/100: Train Loss = 0.6843, Valid Loss = 0.6846\n",
      "Iteration 41/100: Train Loss = 0.6841, Valid Loss = 0.6844\n",
      "Iteration 42/100: Train Loss = 0.6839, Valid Loss = 0.6842\n",
      "Iteration 43/100: Train Loss = 0.6837, Valid Loss = 0.6840\n",
      "Iteration 44/100: Train Loss = 0.6834, Valid Loss = 0.6838\n",
      "Iteration 45/100: Train Loss = 0.6832, Valid Loss = 0.6836\n",
      "Iteration 46/100: Train Loss = 0.6830, Valid Loss = 0.6834\n",
      "Iteration 47/100: Train Loss = 0.6828, Valid Loss = 0.6831\n",
      "Iteration 48/100: Train Loss = 0.6826, Valid Loss = 0.6829\n",
      "Iteration 49/100: Train Loss = 0.6824, Valid Loss = 0.6827\n",
      "Iteration 50/100: Train Loss = 0.6821, Valid Loss = 0.6825\n",
      "Iteration 51/100: Train Loss = 0.6819, Valid Loss = 0.6823\n",
      "Iteration 52/100: Train Loss = 0.6817, Valid Loss = 0.6821\n",
      "Iteration 53/100: Train Loss = 0.6815, Valid Loss = 0.6819\n",
      "Iteration 54/100: Train Loss = 0.6813, Valid Loss = 0.6817\n",
      "Iteration 55/100: Train Loss = 0.6811, Valid Loss = 0.6815\n",
      "Iteration 56/100: Train Loss = 0.6808, Valid Loss = 0.6813\n",
      "Iteration 57/100: Train Loss = 0.6806, Valid Loss = 0.6810\n",
      "Iteration 58/100: Train Loss = 0.6804, Valid Loss = 0.6808\n",
      "Iteration 59/100: Train Loss = 0.6802, Valid Loss = 0.6806\n",
      "Iteration 60/100: Train Loss = 0.6800, Valid Loss = 0.6804\n",
      "Iteration 61/100: Train Loss = 0.6798, Valid Loss = 0.6802\n",
      "Iteration 62/100: Train Loss = 0.6796, Valid Loss = 0.6800\n",
      "Iteration 63/100: Train Loss = 0.6793, Valid Loss = 0.6798\n",
      "Iteration 64/100: Train Loss = 0.6791, Valid Loss = 0.6796\n",
      "Iteration 65/100: Train Loss = 0.6789, Valid Loss = 0.6794\n",
      "Iteration 66/100: Train Loss = 0.6787, Valid Loss = 0.6792\n",
      "Iteration 67/100: Train Loss = 0.6785, Valid Loss = 0.6790\n",
      "Iteration 68/100: Train Loss = 0.6783, Valid Loss = 0.6788\n",
      "Iteration 69/100: Train Loss = 0.6781, Valid Loss = 0.6786\n",
      "Iteration 70/100: Train Loss = 0.6778, Valid Loss = 0.6784\n",
      "Iteration 71/100: Train Loss = 0.6776, Valid Loss = 0.6782\n",
      "Iteration 72/100: Train Loss = 0.6774, Valid Loss = 0.6779\n",
      "Iteration 73/100: Train Loss = 0.6772, Valid Loss = 0.6777\n",
      "Iteration 74/100: Train Loss = 0.6770, Valid Loss = 0.6775\n",
      "Iteration 75/100: Train Loss = 0.6768, Valid Loss = 0.6773\n",
      "Iteration 76/100: Train Loss = 0.6766, Valid Loss = 0.6771\n",
      "Iteration 77/100: Train Loss = 0.6764, Valid Loss = 0.6769\n",
      "Iteration 78/100: Train Loss = 0.6761, Valid Loss = 0.6767\n",
      "Iteration 79/100: Train Loss = 0.6759, Valid Loss = 0.6765\n",
      "Iteration 80/100: Train Loss = 0.6757, Valid Loss = 0.6763\n",
      "Iteration 81/100: Train Loss = 0.6755, Valid Loss = 0.6761\n",
      "Iteration 82/100: Train Loss = 0.6753, Valid Loss = 0.6759\n",
      "Iteration 83/100: Train Loss = 0.6751, Valid Loss = 0.6757\n",
      "Iteration 84/100: Train Loss = 0.6749, Valid Loss = 0.6755\n",
      "Iteration 85/100: Train Loss = 0.6747, Valid Loss = 0.6753\n",
      "Iteration 86/100: Train Loss = 0.6744, Valid Loss = 0.6751\n",
      "Iteration 87/100: Train Loss = 0.6742, Valid Loss = 0.6749\n",
      "Iteration 88/100: Train Loss = 0.6740, Valid Loss = 0.6747\n",
      "Iteration 89/100: Train Loss = 0.6738, Valid Loss = 0.6745\n",
      "Iteration 90/100: Train Loss = 0.6736, Valid Loss = 0.6743\n",
      "Iteration 91/100: Train Loss = 0.6734, Valid Loss = 0.6741\n",
      "Iteration 92/100: Train Loss = 0.6732, Valid Loss = 0.6739\n",
      "Iteration 93/100: Train Loss = 0.6730, Valid Loss = 0.6736\n",
      "Iteration 94/100: Train Loss = 0.6728, Valid Loss = 0.6734\n",
      "Iteration 95/100: Train Loss = 0.6726, Valid Loss = 0.6732\n",
      "Iteration 96/100: Train Loss = 0.6723, Valid Loss = 0.6730\n",
      "Iteration 97/100: Train Loss = 0.6721, Valid Loss = 0.6728\n",
      "Iteration 98/100: Train Loss = 0.6719, Valid Loss = 0.6726\n",
      "Iteration 99/100: Train Loss = 0.6717, Valid Loss = 0.6724\n",
      "Iteration 100/100: Train Loss = 0.6715, Valid Loss = 0.6722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:49:54,385] Trial 16 finished with value: 0.958767906233676 and parameters: {'max_depth': 8, 'min_samples_leaf': 7, 'n_estimators': 100, 'learning_rate': 0.0012796368307581559, 'subsample': 0.6345180478300262}. Best is trial 6 with value: 0.965235693124235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/130: Train Loss = 0.6370, Valid Loss = 0.6380\n",
      "Iteration 2/130: Train Loss = 0.5903, Valid Loss = 0.5923\n",
      "Iteration 3/130: Train Loss = 0.5516, Valid Loss = 0.5547\n",
      "Iteration 4/130: Train Loss = 0.5199, Valid Loss = 0.5244\n",
      "Iteration 5/130: Train Loss = 0.4936, Valid Loss = 0.4989\n",
      "Iteration 6/130: Train Loss = 0.4712, Valid Loss = 0.4767\n",
      "Iteration 7/130: Train Loss = 0.4528, Valid Loss = 0.4589\n",
      "Iteration 8/130: Train Loss = 0.4364, Valid Loss = 0.4429\n",
      "Iteration 9/130: Train Loss = 0.4223, Valid Loss = 0.4288\n",
      "Iteration 10/130: Train Loss = 0.4101, Valid Loss = 0.4171\n",
      "Iteration 11/130: Train Loss = 0.3995, Valid Loss = 0.4070\n",
      "Iteration 12/130: Train Loss = 0.3905, Valid Loss = 0.3982\n",
      "Iteration 13/130: Train Loss = 0.3823, Valid Loss = 0.3901\n",
      "Iteration 14/130: Train Loss = 0.3743, Valid Loss = 0.3821\n",
      "Iteration 15/130: Train Loss = 0.3679, Valid Loss = 0.3761\n",
      "Iteration 16/130: Train Loss = 0.3610, Valid Loss = 0.3695\n",
      "Iteration 17/130: Train Loss = 0.3554, Valid Loss = 0.3641\n",
      "Iteration 18/130: Train Loss = 0.3506, Valid Loss = 0.3596\n",
      "Iteration 19/130: Train Loss = 0.3462, Valid Loss = 0.3554\n",
      "Iteration 20/130: Train Loss = 0.3416, Valid Loss = 0.3512\n",
      "Iteration 21/130: Train Loss = 0.3373, Valid Loss = 0.3471\n",
      "Iteration 22/130: Train Loss = 0.3335, Valid Loss = 0.3435\n",
      "Iteration 23/130: Train Loss = 0.3301, Valid Loss = 0.3405\n",
      "Iteration 24/130: Train Loss = 0.3270, Valid Loss = 0.3374\n",
      "Iteration 25/130: Train Loss = 0.3238, Valid Loss = 0.3345\n",
      "Iteration 26/130: Train Loss = 0.3207, Valid Loss = 0.3314\n",
      "Iteration 27/130: Train Loss = 0.3177, Valid Loss = 0.3286\n",
      "Iteration 28/130: Train Loss = 0.3152, Valid Loss = 0.3261\n",
      "Iteration 29/130: Train Loss = 0.3129, Valid Loss = 0.3238\n",
      "Iteration 30/130: Train Loss = 0.3107, Valid Loss = 0.3219\n",
      "Iteration 31/130: Train Loss = 0.3088, Valid Loss = 0.3200\n",
      "Iteration 32/130: Train Loss = 0.3070, Valid Loss = 0.3182\n",
      "Iteration 33/130: Train Loss = 0.3053, Valid Loss = 0.3165\n",
      "Iteration 34/130: Train Loss = 0.3034, Valid Loss = 0.3145\n",
      "Iteration 35/130: Train Loss = 0.3015, Valid Loss = 0.3126\n",
      "Iteration 36/130: Train Loss = 0.2997, Valid Loss = 0.3108\n",
      "Iteration 37/130: Train Loss = 0.2978, Valid Loss = 0.3094\n",
      "Iteration 38/130: Train Loss = 0.2963, Valid Loss = 0.3082\n",
      "Iteration 39/130: Train Loss = 0.2945, Valid Loss = 0.3064\n",
      "Iteration 40/130: Train Loss = 0.2929, Valid Loss = 0.3048\n",
      "Iteration 41/130: Train Loss = 0.2914, Valid Loss = 0.3035\n",
      "Iteration 42/130: Train Loss = 0.2900, Valid Loss = 0.3019\n",
      "Iteration 43/130: Train Loss = 0.2886, Valid Loss = 0.3004\n",
      "Iteration 44/130: Train Loss = 0.2872, Valid Loss = 0.2991\n",
      "Iteration 45/130: Train Loss = 0.2860, Valid Loss = 0.2978\n",
      "Iteration 46/130: Train Loss = 0.2848, Valid Loss = 0.2968\n",
      "Iteration 47/130: Train Loss = 0.2837, Valid Loss = 0.2953\n",
      "Iteration 48/130: Train Loss = 0.2824, Valid Loss = 0.2942\n",
      "Iteration 49/130: Train Loss = 0.2812, Valid Loss = 0.2928\n",
      "Iteration 50/130: Train Loss = 0.2801, Valid Loss = 0.2917\n",
      "Iteration 51/130: Train Loss = 0.2790, Valid Loss = 0.2907\n",
      "Iteration 52/130: Train Loss = 0.2780, Valid Loss = 0.2894\n",
      "Iteration 53/130: Train Loss = 0.2772, Valid Loss = 0.2887\n",
      "Iteration 54/130: Train Loss = 0.2762, Valid Loss = 0.2878\n",
      "Iteration 55/130: Train Loss = 0.2752, Valid Loss = 0.2869\n",
      "Iteration 56/130: Train Loss = 0.2744, Valid Loss = 0.2860\n",
      "Iteration 57/130: Train Loss = 0.2735, Valid Loss = 0.2851\n",
      "Iteration 58/130: Train Loss = 0.2727, Valid Loss = 0.2842\n",
      "Iteration 59/130: Train Loss = 0.2719, Valid Loss = 0.2836\n",
      "Iteration 60/130: Train Loss = 0.2711, Valid Loss = 0.2827\n",
      "Iteration 61/130: Train Loss = 0.2703, Valid Loss = 0.2818\n",
      "Iteration 62/130: Train Loss = 0.2695, Valid Loss = 0.2811\n",
      "Iteration 63/130: Train Loss = 0.2689, Valid Loss = 0.2805\n",
      "Iteration 64/130: Train Loss = 0.2682, Valid Loss = 0.2797\n",
      "Iteration 65/130: Train Loss = 0.2675, Valid Loss = 0.2790\n",
      "Iteration 66/130: Train Loss = 0.2670, Valid Loss = 0.2785\n",
      "Iteration 67/130: Train Loss = 0.2662, Valid Loss = 0.2778\n",
      "Iteration 68/130: Train Loss = 0.2656, Valid Loss = 0.2772\n",
      "Iteration 69/130: Train Loss = 0.2650, Valid Loss = 0.2765\n",
      "Iteration 70/130: Train Loss = 0.2644, Valid Loss = 0.2759\n",
      "Iteration 71/130: Train Loss = 0.2638, Valid Loss = 0.2756\n",
      "Iteration 72/130: Train Loss = 0.2630, Valid Loss = 0.2749\n",
      "Iteration 73/130: Train Loss = 0.2625, Valid Loss = 0.2743\n",
      "Iteration 74/130: Train Loss = 0.2619, Valid Loss = 0.2736\n",
      "Iteration 75/130: Train Loss = 0.2614, Valid Loss = 0.2733\n",
      "Iteration 76/130: Train Loss = 0.2608, Valid Loss = 0.2727\n",
      "Iteration 77/130: Train Loss = 0.2603, Valid Loss = 0.2720\n",
      "Iteration 78/130: Train Loss = 0.2598, Valid Loss = 0.2717\n",
      "Iteration 79/130: Train Loss = 0.2593, Valid Loss = 0.2713\n",
      "Iteration 80/130: Train Loss = 0.2588, Valid Loss = 0.2709\n",
      "Iteration 81/130: Train Loss = 0.2583, Valid Loss = 0.2704\n",
      "Iteration 82/130: Train Loss = 0.2578, Valid Loss = 0.2699\n",
      "Iteration 83/130: Train Loss = 0.2574, Valid Loss = 0.2696\n",
      "Iteration 84/130: Train Loss = 0.2570, Valid Loss = 0.2691\n",
      "Iteration 85/130: Train Loss = 0.2564, Valid Loss = 0.2685\n",
      "Iteration 86/130: Train Loss = 0.2559, Valid Loss = 0.2679\n",
      "Iteration 87/130: Train Loss = 0.2553, Valid Loss = 0.2673\n",
      "Iteration 88/130: Train Loss = 0.2550, Valid Loss = 0.2672\n",
      "Iteration 89/130: Train Loss = 0.2545, Valid Loss = 0.2664\n",
      "Iteration 90/130: Train Loss = 0.2542, Valid Loss = 0.2661\n",
      "Iteration 91/130: Train Loss = 0.2538, Valid Loss = 0.2660\n",
      "Iteration 92/130: Train Loss = 0.2533, Valid Loss = 0.2656\n",
      "Iteration 93/130: Train Loss = 0.2528, Valid Loss = 0.2652\n",
      "Iteration 94/130: Train Loss = 0.2525, Valid Loss = 0.2649\n",
      "Iteration 95/130: Train Loss = 0.2520, Valid Loss = 0.2645\n",
      "Iteration 96/130: Train Loss = 0.2516, Valid Loss = 0.2644\n",
      "Iteration 97/130: Train Loss = 0.2513, Valid Loss = 0.2642\n",
      "Iteration 98/130: Train Loss = 0.2509, Valid Loss = 0.2639\n",
      "Iteration 99/130: Train Loss = 0.2506, Valid Loss = 0.2634\n",
      "Iteration 100/130: Train Loss = 0.2502, Valid Loss = 0.2631\n",
      "Iteration 101/130: Train Loss = 0.2499, Valid Loss = 0.2627\n",
      "Iteration 102/130: Train Loss = 0.2494, Valid Loss = 0.2625\n",
      "Iteration 103/130: Train Loss = 0.2490, Valid Loss = 0.2625\n",
      "Iteration 104/130: Train Loss = 0.2486, Valid Loss = 0.2621\n",
      "Iteration 105/130: Train Loss = 0.2483, Valid Loss = 0.2619\n",
      "Iteration 106/130: Train Loss = 0.2480, Valid Loss = 0.2617\n",
      "Iteration 107/130: Train Loss = 0.2476, Valid Loss = 0.2613\n",
      "Iteration 108/130: Train Loss = 0.2472, Valid Loss = 0.2610\n",
      "Iteration 109/130: Train Loss = 0.2469, Valid Loss = 0.2606\n",
      "Iteration 110/130: Train Loss = 0.2466, Valid Loss = 0.2602\n",
      "Iteration 111/130: Train Loss = 0.2463, Valid Loss = 0.2600\n",
      "Iteration 112/130: Train Loss = 0.2459, Valid Loss = 0.2598\n",
      "Iteration 113/130: Train Loss = 0.2455, Valid Loss = 0.2596\n",
      "Iteration 114/130: Train Loss = 0.2452, Valid Loss = 0.2594\n",
      "Iteration 115/130: Train Loss = 0.2449, Valid Loss = 0.2591\n",
      "Iteration 116/130: Train Loss = 0.2446, Valid Loss = 0.2589\n",
      "Iteration 117/130: Train Loss = 0.2443, Valid Loss = 0.2583\n",
      "Iteration 118/130: Train Loss = 0.2440, Valid Loss = 0.2581\n",
      "Iteration 119/130: Train Loss = 0.2436, Valid Loss = 0.2575\n",
      "Iteration 120/130: Train Loss = 0.2433, Valid Loss = 0.2573\n",
      "Iteration 121/130: Train Loss = 0.2430, Valid Loss = 0.2569\n",
      "Iteration 122/130: Train Loss = 0.2428, Valid Loss = 0.2566\n",
      "Iteration 123/130: Train Loss = 0.2425, Valid Loss = 0.2564\n",
      "Iteration 124/130: Train Loss = 0.2423, Valid Loss = 0.2562\n",
      "Iteration 125/130: Train Loss = 0.2420, Valid Loss = 0.2561\n",
      "Iteration 126/130: Train Loss = 0.2418, Valid Loss = 0.2560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:49:58,298] Trial 17 finished with value: 0.9617812246205233 and parameters: {'max_depth': 3, 'min_samples_leaf': 13, 'n_estimators': 130, 'learning_rate': 0.412176276269258, 'subsample': 0.4866367765057925}. Best is trial 6 with value: 0.965235693124235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 127/130: Train Loss = 0.2417, Valid Loss = 0.2559\n",
      "Iteration 128/130: Train Loss = 0.2414, Valid Loss = 0.2558\n",
      "Iteration 129/130: Train Loss = 0.2412, Valid Loss = 0.2556\n",
      "Iteration 130/130: Train Loss = 0.2408, Valid Loss = 0.2555\n",
      "Iteration 1/80: Train Loss = 0.6743, Valid Loss = 0.6748\n",
      "Iteration 2/80: Train Loss = 0.6563, Valid Loss = 0.6574\n",
      "Iteration 3/80: Train Loss = 0.6393, Valid Loss = 0.6410\n",
      "Iteration 4/80: Train Loss = 0.6231, Valid Loss = 0.6255\n",
      "Iteration 5/80: Train Loss = 0.6079, Valid Loss = 0.6110\n",
      "Iteration 6/80: Train Loss = 0.5934, Valid Loss = 0.5970\n",
      "Iteration 7/80: Train Loss = 0.5798, Valid Loss = 0.5838\n",
      "Iteration 8/80: Train Loss = 0.5666, Valid Loss = 0.5715\n",
      "Iteration 9/80: Train Loss = 0.5539, Valid Loss = 0.5597\n",
      "Iteration 10/80: Train Loss = 0.5420, Valid Loss = 0.5483\n",
      "Iteration 11/80: Train Loss = 0.5307, Valid Loss = 0.5375\n",
      "Iteration 12/80: Train Loss = 0.5199, Valid Loss = 0.5271\n",
      "Iteration 13/80: Train Loss = 0.5097, Valid Loss = 0.5175\n",
      "Iteration 14/80: Train Loss = 0.4999, Valid Loss = 0.5080\n",
      "Iteration 15/80: Train Loss = 0.4906, Valid Loss = 0.4992\n",
      "Iteration 16/80: Train Loss = 0.4816, Valid Loss = 0.4907\n",
      "Iteration 17/80: Train Loss = 0.4731, Valid Loss = 0.4825\n",
      "Iteration 18/80: Train Loss = 0.4650, Valid Loss = 0.4749\n",
      "Iteration 19/80: Train Loss = 0.4572, Valid Loss = 0.4676\n",
      "Iteration 20/80: Train Loss = 0.4498, Valid Loss = 0.4605\n",
      "Iteration 21/80: Train Loss = 0.4426, Valid Loss = 0.4537\n",
      "Iteration 22/80: Train Loss = 0.4359, Valid Loss = 0.4473\n",
      "Iteration 23/80: Train Loss = 0.4294, Valid Loss = 0.4413\n",
      "Iteration 24/80: Train Loss = 0.4232, Valid Loss = 0.4354\n",
      "Iteration 25/80: Train Loss = 0.4173, Valid Loss = 0.4301\n",
      "Iteration 26/80: Train Loss = 0.4116, Valid Loss = 0.4251\n",
      "Iteration 27/80: Train Loss = 0.4060, Valid Loss = 0.4201\n",
      "Iteration 28/80: Train Loss = 0.4007, Valid Loss = 0.4153\n",
      "Iteration 29/80: Train Loss = 0.3955, Valid Loss = 0.4105\n",
      "Iteration 30/80: Train Loss = 0.3907, Valid Loss = 0.4061\n",
      "Iteration 31/80: Train Loss = 0.3858, Valid Loss = 0.4017\n",
      "Iteration 32/80: Train Loss = 0.3813, Valid Loss = 0.3976\n",
      "Iteration 33/80: Train Loss = 0.3769, Valid Loss = 0.3935\n",
      "Iteration 34/80: Train Loss = 0.3726, Valid Loss = 0.3898\n",
      "Iteration 35/80: Train Loss = 0.3684, Valid Loss = 0.3860\n",
      "Iteration 36/80: Train Loss = 0.3645, Valid Loss = 0.3825\n",
      "Iteration 37/80: Train Loss = 0.3607, Valid Loss = 0.3790\n",
      "Iteration 38/80: Train Loss = 0.3570, Valid Loss = 0.3756\n",
      "Iteration 39/80: Train Loss = 0.3534, Valid Loss = 0.3724\n",
      "Iteration 40/80: Train Loss = 0.3499, Valid Loss = 0.3693\n",
      "Iteration 41/80: Train Loss = 0.3465, Valid Loss = 0.3663\n",
      "Iteration 42/80: Train Loss = 0.3433, Valid Loss = 0.3633\n",
      "Iteration 43/80: Train Loss = 0.3401, Valid Loss = 0.3607\n",
      "Iteration 44/80: Train Loss = 0.3369, Valid Loss = 0.3581\n",
      "Iteration 45/80: Train Loss = 0.3340, Valid Loss = 0.3554\n",
      "Iteration 46/80: Train Loss = 0.3311, Valid Loss = 0.3529\n",
      "Iteration 47/80: Train Loss = 0.3283, Valid Loss = 0.3504\n",
      "Iteration 48/80: Train Loss = 0.3257, Valid Loss = 0.3479\n",
      "Iteration 49/80: Train Loss = 0.3230, Valid Loss = 0.3458\n",
      "Iteration 50/80: Train Loss = 0.3205, Valid Loss = 0.3434\n",
      "Iteration 51/80: Train Loss = 0.3179, Valid Loss = 0.3412\n",
      "Iteration 52/80: Train Loss = 0.3155, Valid Loss = 0.3390\n",
      "Iteration 53/80: Train Loss = 0.3132, Valid Loss = 0.3371\n",
      "Iteration 54/80: Train Loss = 0.3108, Valid Loss = 0.3350\n",
      "Iteration 55/80: Train Loss = 0.3086, Valid Loss = 0.3331\n",
      "Iteration 56/80: Train Loss = 0.3064, Valid Loss = 0.3313\n",
      "Iteration 57/80: Train Loss = 0.3044, Valid Loss = 0.3295\n",
      "Iteration 58/80: Train Loss = 0.3023, Valid Loss = 0.3278\n",
      "Iteration 59/80: Train Loss = 0.3002, Valid Loss = 0.3260\n",
      "Iteration 60/80: Train Loss = 0.2983, Valid Loss = 0.3243\n",
      "Iteration 61/80: Train Loss = 0.2964, Valid Loss = 0.3228\n",
      "Iteration 62/80: Train Loss = 0.2945, Valid Loss = 0.3214\n",
      "Iteration 63/80: Train Loss = 0.2927, Valid Loss = 0.3199\n",
      "Iteration 64/80: Train Loss = 0.2909, Valid Loss = 0.3185\n",
      "Iteration 65/80: Train Loss = 0.2892, Valid Loss = 0.3170\n",
      "Iteration 66/80: Train Loss = 0.2875, Valid Loss = 0.3157\n",
      "Iteration 67/80: Train Loss = 0.2859, Valid Loss = 0.3142\n",
      "Iteration 68/80: Train Loss = 0.2842, Valid Loss = 0.3130\n",
      "Iteration 69/80: Train Loss = 0.2826, Valid Loss = 0.3118\n",
      "Iteration 70/80: Train Loss = 0.2811, Valid Loss = 0.3106\n",
      "Iteration 71/80: Train Loss = 0.2795, Valid Loss = 0.3093\n",
      "Iteration 72/80: Train Loss = 0.2780, Valid Loss = 0.3081\n",
      "Iteration 73/80: Train Loss = 0.2765, Valid Loss = 0.3071\n",
      "Iteration 74/80: Train Loss = 0.2751, Valid Loss = 0.3058\n",
      "Iteration 75/80: Train Loss = 0.2738, Valid Loss = 0.3048\n",
      "Iteration 76/80: Train Loss = 0.2724, Valid Loss = 0.3038\n",
      "Iteration 77/80: Train Loss = 0.2711, Valid Loss = 0.3028\n",
      "Iteration 78/80: Train Loss = 0.2698, Valid Loss = 0.3018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:50:06,018] Trial 18 finished with value: 0.9629967704913658 and parameters: {'max_depth': 12, 'min_samples_leaf': 18, 'n_estimators': 80, 'learning_rate': 0.1080243913996377, 'subsample': 0.7377507909474819}. Best is trial 6 with value: 0.965235693124235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 79/80: Train Loss = 0.2686, Valid Loss = 0.3008\n",
      "Iteration 80/80: Train Loss = 0.2673, Valid Loss = 0.2998\n",
      "Iteration 1/120: Train Loss = 0.6900, Valid Loss = 0.6901\n",
      "Iteration 2/120: Train Loss = 0.6868, Valid Loss = 0.6871\n",
      "Iteration 3/120: Train Loss = 0.6837, Valid Loss = 0.6841\n",
      "Iteration 4/120: Train Loss = 0.6806, Valid Loss = 0.6812\n",
      "Iteration 5/120: Train Loss = 0.6775, Valid Loss = 0.6782\n",
      "Iteration 6/120: Train Loss = 0.6744, Valid Loss = 0.6753\n",
      "Iteration 7/120: Train Loss = 0.6714, Valid Loss = 0.6724\n",
      "Iteration 8/120: Train Loss = 0.6684, Valid Loss = 0.6696\n",
      "Iteration 9/120: Train Loss = 0.6654, Valid Loss = 0.6668\n",
      "Iteration 10/120: Train Loss = 0.6625, Valid Loss = 0.6640\n",
      "Iteration 11/120: Train Loss = 0.6596, Valid Loss = 0.6612\n",
      "Iteration 12/120: Train Loss = 0.6566, Valid Loss = 0.6585\n",
      "Iteration 13/120: Train Loss = 0.6538, Valid Loss = 0.6557\n",
      "Iteration 14/120: Train Loss = 0.6509, Valid Loss = 0.6530\n",
      "Iteration 15/120: Train Loss = 0.6481, Valid Loss = 0.6504\n",
      "Iteration 16/120: Train Loss = 0.6453, Valid Loss = 0.6477\n",
      "Iteration 17/120: Train Loss = 0.6425, Valid Loss = 0.6451\n",
      "Iteration 18/120: Train Loss = 0.6397, Valid Loss = 0.6424\n",
      "Iteration 19/120: Train Loss = 0.6370, Valid Loss = 0.6398\n",
      "Iteration 20/120: Train Loss = 0.6343, Valid Loss = 0.6373\n",
      "Iteration 21/120: Train Loss = 0.6316, Valid Loss = 0.6347\n",
      "Iteration 22/120: Train Loss = 0.6289, Valid Loss = 0.6322\n",
      "Iteration 23/120: Train Loss = 0.6263, Valid Loss = 0.6297\n",
      "Iteration 24/120: Train Loss = 0.6236, Valid Loss = 0.6272\n",
      "Iteration 25/120: Train Loss = 0.6210, Valid Loss = 0.6247\n",
      "Iteration 26/120: Train Loss = 0.6184, Valid Loss = 0.6223\n",
      "Iteration 27/120: Train Loss = 0.6158, Valid Loss = 0.6198\n",
      "Iteration 28/120: Train Loss = 0.6133, Valid Loss = 0.6174\n",
      "Iteration 29/120: Train Loss = 0.6108, Valid Loss = 0.6151\n",
      "Iteration 30/120: Train Loss = 0.6083, Valid Loss = 0.6127\n",
      "Iteration 31/120: Train Loss = 0.6058, Valid Loss = 0.6104\n",
      "Iteration 32/120: Train Loss = 0.6033, Valid Loss = 0.6081\n",
      "Iteration 33/120: Train Loss = 0.6009, Valid Loss = 0.6058\n",
      "Iteration 34/120: Train Loss = 0.5984, Valid Loss = 0.6035\n",
      "Iteration 35/120: Train Loss = 0.5960, Valid Loss = 0.6012\n",
      "Iteration 36/120: Train Loss = 0.5937, Valid Loss = 0.5989\n",
      "Iteration 37/120: Train Loss = 0.5913, Valid Loss = 0.5967\n",
      "Iteration 38/120: Train Loss = 0.5889, Valid Loss = 0.5945\n",
      "Iteration 39/120: Train Loss = 0.5866, Valid Loss = 0.5924\n",
      "Iteration 40/120: Train Loss = 0.5843, Valid Loss = 0.5902\n",
      "Iteration 41/120: Train Loss = 0.5820, Valid Loss = 0.5880\n",
      "Iteration 42/120: Train Loss = 0.5797, Valid Loss = 0.5859\n",
      "Iteration 43/120: Train Loss = 0.5775, Valid Loss = 0.5838\n",
      "Iteration 44/120: Train Loss = 0.5752, Valid Loss = 0.5816\n",
      "Iteration 45/120: Train Loss = 0.5730, Valid Loss = 0.5795\n",
      "Iteration 46/120: Train Loss = 0.5708, Valid Loss = 0.5775\n",
      "Iteration 47/120: Train Loss = 0.5686, Valid Loss = 0.5754\n",
      "Iteration 48/120: Train Loss = 0.5664, Valid Loss = 0.5734\n",
      "Iteration 49/120: Train Loss = 0.5643, Valid Loss = 0.5714\n",
      "Iteration 50/120: Train Loss = 0.5621, Valid Loss = 0.5694\n",
      "Iteration 51/120: Train Loss = 0.5600, Valid Loss = 0.5675\n",
      "Iteration 52/120: Train Loss = 0.5579, Valid Loss = 0.5655\n",
      "Iteration 53/120: Train Loss = 0.5558, Valid Loss = 0.5635\n",
      "Iteration 54/120: Train Loss = 0.5538, Valid Loss = 0.5616\n",
      "Iteration 55/120: Train Loss = 0.5517, Valid Loss = 0.5596\n",
      "Iteration 56/120: Train Loss = 0.5497, Valid Loss = 0.5577\n",
      "Iteration 57/120: Train Loss = 0.5477, Valid Loss = 0.5558\n",
      "Iteration 58/120: Train Loss = 0.5457, Valid Loss = 0.5540\n",
      "Iteration 59/120: Train Loss = 0.5437, Valid Loss = 0.5521\n",
      "Iteration 60/120: Train Loss = 0.5417, Valid Loss = 0.5503\n",
      "Iteration 61/120: Train Loss = 0.5398, Valid Loss = 0.5485\n",
      "Iteration 62/120: Train Loss = 0.5378, Valid Loss = 0.5467\n",
      "Iteration 63/120: Train Loss = 0.5359, Valid Loss = 0.5448\n",
      "Iteration 64/120: Train Loss = 0.5340, Valid Loss = 0.5431\n",
      "Iteration 65/120: Train Loss = 0.5321, Valid Loss = 0.5413\n",
      "Iteration 66/120: Train Loss = 0.5302, Valid Loss = 0.5395\n",
      "Iteration 67/120: Train Loss = 0.5283, Valid Loss = 0.5378\n",
      "Iteration 68/120: Train Loss = 0.5265, Valid Loss = 0.5360\n",
      "Iteration 69/120: Train Loss = 0.5247, Valid Loss = 0.5343\n",
      "Iteration 70/120: Train Loss = 0.5229, Valid Loss = 0.5327\n",
      "Iteration 71/120: Train Loss = 0.5210, Valid Loss = 0.5310\n",
      "Iteration 72/120: Train Loss = 0.5192, Valid Loss = 0.5293\n",
      "Iteration 73/120: Train Loss = 0.5175, Valid Loss = 0.5277\n",
      "Iteration 74/120: Train Loss = 0.5157, Valid Loss = 0.5261\n",
      "Iteration 75/120: Train Loss = 0.5139, Valid Loss = 0.5245\n",
      "Iteration 76/120: Train Loss = 0.5122, Valid Loss = 0.5229\n",
      "Iteration 77/120: Train Loss = 0.5105, Valid Loss = 0.5213\n",
      "Iteration 78/120: Train Loss = 0.5088, Valid Loss = 0.5197\n",
      "Iteration 79/120: Train Loss = 0.5071, Valid Loss = 0.5181\n",
      "Iteration 80/120: Train Loss = 0.5054, Valid Loss = 0.5165\n",
      "Iteration 81/120: Train Loss = 0.5037, Valid Loss = 0.5150\n",
      "Iteration 82/120: Train Loss = 0.5021, Valid Loss = 0.5135\n",
      "Iteration 83/120: Train Loss = 0.5004, Valid Loss = 0.5120\n",
      "Iteration 84/120: Train Loss = 0.4988, Valid Loss = 0.5105\n",
      "Iteration 85/120: Train Loss = 0.4972, Valid Loss = 0.5090\n",
      "Iteration 86/120: Train Loss = 0.4956, Valid Loss = 0.5074\n",
      "Iteration 87/120: Train Loss = 0.4940, Valid Loss = 0.5060\n",
      "Iteration 88/120: Train Loss = 0.4924, Valid Loss = 0.5045\n",
      "Iteration 89/120: Train Loss = 0.4908, Valid Loss = 0.5030\n",
      "Iteration 90/120: Train Loss = 0.4892, Valid Loss = 0.5016\n",
      "Iteration 91/120: Train Loss = 0.4877, Valid Loss = 0.5001\n",
      "Iteration 92/120: Train Loss = 0.4861, Valid Loss = 0.4987\n",
      "Iteration 93/120: Train Loss = 0.4846, Valid Loss = 0.4973\n",
      "Iteration 94/120: Train Loss = 0.4831, Valid Loss = 0.4959\n",
      "Iteration 95/120: Train Loss = 0.4816, Valid Loss = 0.4945\n",
      "Iteration 96/120: Train Loss = 0.4801, Valid Loss = 0.4931\n",
      "Iteration 97/120: Train Loss = 0.4786, Valid Loss = 0.4917\n",
      "Iteration 98/120: Train Loss = 0.4771, Valid Loss = 0.4904\n",
      "Iteration 99/120: Train Loss = 0.4756, Valid Loss = 0.4890\n",
      "Iteration 100/120: Train Loss = 0.4742, Valid Loss = 0.4877\n",
      "Iteration 101/120: Train Loss = 0.4727, Valid Loss = 0.4863\n",
      "Iteration 102/120: Train Loss = 0.4713, Valid Loss = 0.4850\n",
      "Iteration 103/120: Train Loss = 0.4699, Valid Loss = 0.4837\n",
      "Iteration 104/120: Train Loss = 0.4685, Valid Loss = 0.4824\n",
      "Iteration 105/120: Train Loss = 0.4671, Valid Loss = 0.4811\n",
      "Iteration 106/120: Train Loss = 0.4657, Valid Loss = 0.4798\n",
      "Iteration 107/120: Train Loss = 0.4643, Valid Loss = 0.4786\n",
      "Iteration 108/120: Train Loss = 0.4629, Valid Loss = 0.4773\n",
      "Iteration 109/120: Train Loss = 0.4615, Valid Loss = 0.4761\n",
      "Iteration 110/120: Train Loss = 0.4602, Valid Loss = 0.4748\n",
      "Iteration 111/120: Train Loss = 0.4588, Valid Loss = 0.4736\n",
      "Iteration 112/120: Train Loss = 0.4575, Valid Loss = 0.4724\n",
      "Iteration 113/120: Train Loss = 0.4562, Valid Loss = 0.4712\n",
      "Iteration 114/120: Train Loss = 0.4549, Valid Loss = 0.4700\n",
      "Iteration 115/120: Train Loss = 0.4536, Valid Loss = 0.4687\n",
      "Iteration 116/120: Train Loss = 0.4523, Valid Loss = 0.4676\n",
      "Iteration 117/120: Train Loss = 0.4510, Valid Loss = 0.4664\n",
      "Iteration 118/120: Train Loss = 0.4497, Valid Loss = 0.4653\n",
      "Iteration 119/120: Train Loss = 0.4485, Valid Loss = 0.4641\n",
      "Iteration 120/120: Train Loss = 0.4472, Valid Loss = 0.4630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:50:25,202] Trial 19 finished with value: 0.9617164056750892 and parameters: {'max_depth': 15, 'min_samples_leaf': 13, 'n_estimators': 120, 'learning_rate': 0.01747174211422389, 'subsample': 0.8819848713894572}. Best is trial 6 with value: 0.965235693124235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/95: Train Loss = 0.6069, Valid Loss = 0.6096\n",
      "Iteration 2/95: Train Loss = 0.5418, Valid Loss = 0.5469\n",
      "Iteration 3/95: Train Loss = 0.4916, Valid Loss = 0.5000\n",
      "Iteration 4/95: Train Loss = 0.4525, Valid Loss = 0.4627\n",
      "Iteration 5/95: Train Loss = 0.4206, Valid Loss = 0.4321\n",
      "Iteration 6/95: Train Loss = 0.3955, Valid Loss = 0.4082\n",
      "Iteration 7/95: Train Loss = 0.3742, Valid Loss = 0.3883\n",
      "Iteration 8/95: Train Loss = 0.3572, Valid Loss = 0.3722\n",
      "Iteration 9/95: Train Loss = 0.3420, Valid Loss = 0.3576\n",
      "Iteration 10/95: Train Loss = 0.3290, Valid Loss = 0.3459\n",
      "Iteration 11/95: Train Loss = 0.3178, Valid Loss = 0.3367\n",
      "Iteration 12/95: Train Loss = 0.3086, Valid Loss = 0.3287\n",
      "Iteration 13/95: Train Loss = 0.2997, Valid Loss = 0.3217\n",
      "Iteration 14/95: Train Loss = 0.2923, Valid Loss = 0.3156\n",
      "Iteration 15/95: Train Loss = 0.2856, Valid Loss = 0.3097\n",
      "Iteration 16/95: Train Loss = 0.2795, Valid Loss = 0.3048\n",
      "Iteration 17/95: Train Loss = 0.2741, Valid Loss = 0.3003\n",
      "Iteration 18/95: Train Loss = 0.2694, Valid Loss = 0.2960\n",
      "Iteration 19/95: Train Loss = 0.2650, Valid Loss = 0.2927\n",
      "Iteration 20/95: Train Loss = 0.2607, Valid Loss = 0.2898\n",
      "Iteration 21/95: Train Loss = 0.2570, Valid Loss = 0.2863\n",
      "Iteration 22/95: Train Loss = 0.2532, Valid Loss = 0.2833\n",
      "Iteration 23/95: Train Loss = 0.2501, Valid Loss = 0.2811\n",
      "Iteration 24/95: Train Loss = 0.2470, Valid Loss = 0.2789\n",
      "Iteration 25/95: Train Loss = 0.2441, Valid Loss = 0.2770\n",
      "Iteration 26/95: Train Loss = 0.2414, Valid Loss = 0.2755\n",
      "Iteration 27/95: Train Loss = 0.2390, Valid Loss = 0.2736\n",
      "Iteration 28/95: Train Loss = 0.2366, Valid Loss = 0.2715\n",
      "Iteration 29/95: Train Loss = 0.2346, Valid Loss = 0.2698\n",
      "Iteration 30/95: Train Loss = 0.2323, Valid Loss = 0.2687\n",
      "Iteration 31/95: Train Loss = 0.2305, Valid Loss = 0.2679\n",
      "Iteration 32/95: Train Loss = 0.2285, Valid Loss = 0.2662\n",
      "Iteration 33/95: Train Loss = 0.2269, Valid Loss = 0.2651\n",
      "Iteration 34/95: Train Loss = 0.2252, Valid Loss = 0.2637\n",
      "Iteration 35/95: Train Loss = 0.2238, Valid Loss = 0.2623\n",
      "Iteration 36/95: Train Loss = 0.2224, Valid Loss = 0.2609\n",
      "Iteration 37/95: Train Loss = 0.2209, Valid Loss = 0.2600\n",
      "Iteration 38/95: Train Loss = 0.2196, Valid Loss = 0.2593\n",
      "Iteration 39/95: Train Loss = 0.2180, Valid Loss = 0.2584\n",
      "Iteration 40/95: Train Loss = 0.2167, Valid Loss = 0.2581\n",
      "Iteration 41/95: Train Loss = 0.2154, Valid Loss = 0.2575\n",
      "Iteration 42/95: Train Loss = 0.2143, Valid Loss = 0.2568\n",
      "Iteration 43/95: Train Loss = 0.2129, Valid Loss = 0.2556\n",
      "Iteration 44/95: Train Loss = 0.2118, Valid Loss = 0.2550\n",
      "Iteration 45/95: Train Loss = 0.2108, Valid Loss = 0.2543\n",
      "Iteration 46/95: Train Loss = 0.2099, Valid Loss = 0.2537\n",
      "Iteration 47/95: Train Loss = 0.2090, Valid Loss = 0.2535\n",
      "Iteration 48/95: Train Loss = 0.2082, Valid Loss = 0.2532\n",
      "Iteration 49/95: Train Loss = 0.2070, Valid Loss = 0.2526\n",
      "Iteration 50/95: Train Loss = 0.2062, Valid Loss = 0.2524\n",
      "Iteration 51/95: Train Loss = 0.2054, Valid Loss = 0.2519\n",
      "Iteration 52/95: Train Loss = 0.2044, Valid Loss = 0.2511\n",
      "Iteration 53/95: Train Loss = 0.2037, Valid Loss = 0.2508\n",
      "Iteration 54/95: Train Loss = 0.2029, Valid Loss = 0.2501\n",
      "Iteration 55/95: Train Loss = 0.2021, Valid Loss = 0.2495\n",
      "Iteration 56/95: Train Loss = 0.2015, Valid Loss = 0.2492\n",
      "Iteration 57/95: Train Loss = 0.2007, Valid Loss = 0.2487\n",
      "Iteration 58/95: Train Loss = 0.1999, Valid Loss = 0.2483\n",
      "Iteration 59/95: Train Loss = 0.1992, Valid Loss = 0.2482\n",
      "Iteration 60/95: Train Loss = 0.1986, Valid Loss = 0.2479\n",
      "Iteration 61/95: Train Loss = 0.1980, Valid Loss = 0.2477\n",
      "Iteration 62/95: Train Loss = 0.1974, Valid Loss = 0.2475\n",
      "Iteration 63/95: Train Loss = 0.1968, Valid Loss = 0.2471\n",
      "Iteration 64/95: Train Loss = 0.1963, Valid Loss = 0.2469\n",
      "Iteration 65/95: Train Loss = 0.1957, Valid Loss = 0.2470\n",
      "Iteration 66/95: Train Loss = 0.1952, Valid Loss = 0.2470\n",
      "Iteration 67/95: Train Loss = 0.1947, Valid Loss = 0.2466\n",
      "Iteration 68/95: Train Loss = 0.1941, Valid Loss = 0.2466\n",
      "Iteration 69/95: Train Loss = 0.1936, Valid Loss = 0.2466\n",
      "Iteration 70/95: Train Loss = 0.1931, Valid Loss = 0.2466\n",
      "Iteration 71/95: Train Loss = 0.1926, Valid Loss = 0.2461\n",
      "Iteration 72/95: Train Loss = 0.1922, Valid Loss = 0.2459\n",
      "Iteration 73/95: Train Loss = 0.1918, Valid Loss = 0.2458\n",
      "Iteration 74/95: Train Loss = 0.1913, Valid Loss = 0.2456\n",
      "Iteration 75/95: Train Loss = 0.1909, Valid Loss = 0.2457\n",
      "Iteration 76/95: Train Loss = 0.1902, Valid Loss = 0.2459\n",
      "Iteration 77/95: Train Loss = 0.1897, Valid Loss = 0.2457\n",
      "Iteration 78/95: Train Loss = 0.1890, Valid Loss = 0.2457\n",
      "Iteration 79/95: Train Loss = 0.1887, Valid Loss = 0.2455\n",
      "Iteration 80/95: Train Loss = 0.1884, Valid Loss = 0.2454\n",
      "Iteration 81/95: Train Loss = 0.1877, Valid Loss = 0.2454\n",
      "Iteration 82/95: Train Loss = 0.1872, Valid Loss = 0.2452\n",
      "Iteration 83/95: Train Loss = 0.1867, Valid Loss = 0.2451\n",
      "Iteration 84/95: Train Loss = 0.1863, Valid Loss = 0.2452\n",
      "Iteration 85/95: Train Loss = 0.1860, Valid Loss = 0.2448\n",
      "Iteration 86/95: Train Loss = 0.1856, Valid Loss = 0.2445\n",
      "Iteration 87/95: Train Loss = 0.1853, Valid Loss = 0.2442\n",
      "Iteration 88/95: Train Loss = 0.1848, Valid Loss = 0.2441\n",
      "Iteration 89/95: Train Loss = 0.1846, Valid Loss = 0.2439\n",
      "Iteration 90/95: Train Loss = 0.1843, Valid Loss = 0.2435\n",
      "Iteration 91/95: Train Loss = 0.1839, Valid Loss = 0.2435\n",
      "Iteration 92/95: Train Loss = 0.1836, Valid Loss = 0.2436\n",
      "Iteration 93/95: Train Loss = 0.1833, Valid Loss = 0.2435\n",
      "Iteration 94/95: Train Loss = 0.1828, Valid Loss = 0.2437\n",
      "Iteration 95/95: Train Loss = 0.1825, Valid Loss = 0.2433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:50:31,002] Trial 20 finished with value: 0.964221848560066 and parameters: {'max_depth': 8, 'min_samples_leaf': 15, 'n_estimators': 95, 'learning_rate': 0.5311993294957216, 'subsample': 0.8107743656887347}. Best is trial 6 with value: 0.965235693124235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/100: Train Loss = 0.6131, Valid Loss = 0.6158\n",
      "Iteration 2/100: Train Loss = 0.5509, Valid Loss = 0.5561\n",
      "Iteration 3/100: Train Loss = 0.5021, Valid Loss = 0.5086\n",
      "Iteration 4/100: Train Loss = 0.4636, Valid Loss = 0.4720\n",
      "Iteration 5/100: Train Loss = 0.4326, Valid Loss = 0.4419\n",
      "Iteration 6/100: Train Loss = 0.4075, Valid Loss = 0.4181\n",
      "Iteration 7/100: Train Loss = 0.3871, Valid Loss = 0.3981\n",
      "Iteration 8/100: Train Loss = 0.3700, Valid Loss = 0.3819\n",
      "Iteration 9/100: Train Loss = 0.3554, Valid Loss = 0.3683\n",
      "Iteration 10/100: Train Loss = 0.3430, Valid Loss = 0.3569\n",
      "Iteration 11/100: Train Loss = 0.3318, Valid Loss = 0.3464\n",
      "Iteration 12/100: Train Loss = 0.3222, Valid Loss = 0.3374\n",
      "Iteration 13/100: Train Loss = 0.3136, Valid Loss = 0.3297\n",
      "Iteration 14/100: Train Loss = 0.3060, Valid Loss = 0.3231\n",
      "Iteration 15/100: Train Loss = 0.2992, Valid Loss = 0.3178\n",
      "Iteration 16/100: Train Loss = 0.2931, Valid Loss = 0.3122\n",
      "Iteration 17/100: Train Loss = 0.2878, Valid Loss = 0.3071\n",
      "Iteration 18/100: Train Loss = 0.2831, Valid Loss = 0.3027\n",
      "Iteration 19/100: Train Loss = 0.2783, Valid Loss = 0.2985\n",
      "Iteration 20/100: Train Loss = 0.2742, Valid Loss = 0.2947\n",
      "Iteration 21/100: Train Loss = 0.2702, Valid Loss = 0.2908\n",
      "Iteration 22/100: Train Loss = 0.2667, Valid Loss = 0.2879\n",
      "Iteration 23/100: Train Loss = 0.2636, Valid Loss = 0.2853\n",
      "Iteration 24/100: Train Loss = 0.2608, Valid Loss = 0.2828\n",
      "Iteration 25/100: Train Loss = 0.2577, Valid Loss = 0.2809\n",
      "Iteration 26/100: Train Loss = 0.2548, Valid Loss = 0.2787\n",
      "Iteration 27/100: Train Loss = 0.2521, Valid Loss = 0.2767\n",
      "Iteration 28/100: Train Loss = 0.2496, Valid Loss = 0.2746\n",
      "Iteration 29/100: Train Loss = 0.2476, Valid Loss = 0.2734\n",
      "Iteration 30/100: Train Loss = 0.2455, Valid Loss = 0.2719\n",
      "Iteration 31/100: Train Loss = 0.2433, Valid Loss = 0.2705\n",
      "Iteration 32/100: Train Loss = 0.2413, Valid Loss = 0.2690\n",
      "Iteration 33/100: Train Loss = 0.2393, Valid Loss = 0.2674\n",
      "Iteration 34/100: Train Loss = 0.2377, Valid Loss = 0.2660\n",
      "Iteration 35/100: Train Loss = 0.2359, Valid Loss = 0.2649\n",
      "Iteration 36/100: Train Loss = 0.2343, Valid Loss = 0.2642\n",
      "Iteration 37/100: Train Loss = 0.2328, Valid Loss = 0.2631\n",
      "Iteration 38/100: Train Loss = 0.2314, Valid Loss = 0.2618\n",
      "Iteration 39/100: Train Loss = 0.2300, Valid Loss = 0.2612\n",
      "Iteration 40/100: Train Loss = 0.2285, Valid Loss = 0.2604\n",
      "Iteration 41/100: Train Loss = 0.2274, Valid Loss = 0.2598\n",
      "Iteration 42/100: Train Loss = 0.2260, Valid Loss = 0.2593\n",
      "Iteration 43/100: Train Loss = 0.2248, Valid Loss = 0.2582\n",
      "Iteration 44/100: Train Loss = 0.2238, Valid Loss = 0.2571\n",
      "Iteration 45/100: Train Loss = 0.2226, Valid Loss = 0.2564\n",
      "Iteration 46/100: Train Loss = 0.2216, Valid Loss = 0.2553\n",
      "Iteration 47/100: Train Loss = 0.2206, Valid Loss = 0.2548\n",
      "Iteration 48/100: Train Loss = 0.2195, Valid Loss = 0.2542\n",
      "Iteration 49/100: Train Loss = 0.2186, Valid Loss = 0.2538\n",
      "Iteration 50/100: Train Loss = 0.2177, Valid Loss = 0.2532\n",
      "Iteration 51/100: Train Loss = 0.2168, Valid Loss = 0.2526\n",
      "Iteration 52/100: Train Loss = 0.2160, Valid Loss = 0.2519\n",
      "Iteration 53/100: Train Loss = 0.2152, Valid Loss = 0.2511\n",
      "Iteration 54/100: Train Loss = 0.2145, Valid Loss = 0.2508\n",
      "Iteration 55/100: Train Loss = 0.2138, Valid Loss = 0.2502\n",
      "Iteration 56/100: Train Loss = 0.2132, Valid Loss = 0.2498\n",
      "Iteration 57/100: Train Loss = 0.2123, Valid Loss = 0.2497\n",
      "Iteration 58/100: Train Loss = 0.2114, Valid Loss = 0.2491\n",
      "Iteration 59/100: Train Loss = 0.2107, Valid Loss = 0.2488\n",
      "Iteration 60/100: Train Loss = 0.2101, Valid Loss = 0.2484\n",
      "Iteration 61/100: Train Loss = 0.2095, Valid Loss = 0.2481\n",
      "Iteration 62/100: Train Loss = 0.2089, Valid Loss = 0.2477\n",
      "Iteration 63/100: Train Loss = 0.2082, Valid Loss = 0.2472\n",
      "Iteration 64/100: Train Loss = 0.2075, Valid Loss = 0.2472\n",
      "Iteration 65/100: Train Loss = 0.2070, Valid Loss = 0.2467\n",
      "Iteration 66/100: Train Loss = 0.2065, Valid Loss = 0.2463\n",
      "Iteration 67/100: Train Loss = 0.2059, Valid Loss = 0.2462\n",
      "Iteration 68/100: Train Loss = 0.2052, Valid Loss = 0.2462\n",
      "Iteration 69/100: Train Loss = 0.2047, Valid Loss = 0.2460\n",
      "Iteration 70/100: Train Loss = 0.2041, Valid Loss = 0.2458\n",
      "Iteration 71/100: Train Loss = 0.2033, Valid Loss = 0.2456\n",
      "Iteration 72/100: Train Loss = 0.2030, Valid Loss = 0.2454\n",
      "Iteration 73/100: Train Loss = 0.2022, Valid Loss = 0.2450\n",
      "Iteration 74/100: Train Loss = 0.2019, Valid Loss = 0.2448\n",
      "Iteration 75/100: Train Loss = 0.2014, Valid Loss = 0.2445\n",
      "Iteration 76/100: Train Loss = 0.2010, Valid Loss = 0.2443\n",
      "Iteration 77/100: Train Loss = 0.2005, Valid Loss = 0.2440\n",
      "Iteration 78/100: Train Loss = 0.2002, Valid Loss = 0.2439\n",
      "Iteration 79/100: Train Loss = 0.1998, Valid Loss = 0.2437\n",
      "Iteration 80/100: Train Loss = 0.1994, Valid Loss = 0.2435\n",
      "Iteration 81/100: Train Loss = 0.1988, Valid Loss = 0.2433\n",
      "Iteration 82/100: Train Loss = 0.1983, Valid Loss = 0.2433\n",
      "Iteration 83/100: Train Loss = 0.1979, Valid Loss = 0.2432\n",
      "Iteration 84/100: Train Loss = 0.1973, Valid Loss = 0.2433\n",
      "Iteration 85/100: Train Loss = 0.1970, Valid Loss = 0.2432\n",
      "Iteration 86/100: Train Loss = 0.1966, Valid Loss = 0.2431\n",
      "Iteration 87/100: Train Loss = 0.1962, Valid Loss = 0.2428\n",
      "Iteration 88/100: Train Loss = 0.1959, Valid Loss = 0.2427\n",
      "Iteration 89/100: Train Loss = 0.1955, Valid Loss = 0.2427\n",
      "Iteration 90/100: Train Loss = 0.1951, Valid Loss = 0.2428\n",
      "Iteration 91/100: Train Loss = 0.1947, Valid Loss = 0.2429\n",
      "Iteration 92/100: Train Loss = 0.1943, Valid Loss = 0.2430\n",
      "Iteration 93/100: Train Loss = 0.1940, Valid Loss = 0.2425\n",
      "Iteration 94/100: Train Loss = 0.1937, Valid Loss = 0.2425\n",
      "Iteration 95/100: Train Loss = 0.1931, Valid Loss = 0.2427\n",
      "Iteration 96/100: Train Loss = 0.1928, Valid Loss = 0.2423\n",
      "Iteration 97/100: Train Loss = 0.1924, Valid Loss = 0.2421\n",
      "Iteration 98/100: Train Loss = 0.1921, Valid Loss = 0.2421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:50:36,418] Trial 21 finished with value: 0.9643625438004493 and parameters: {'max_depth': 7, 'min_samples_leaf': 15, 'n_estimators': 100, 'learning_rate': 0.5035215960203637, 'subsample': 0.7817570489669037}. Best is trial 6 with value: 0.965235693124235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 99/100: Train Loss = 0.1919, Valid Loss = 0.2418\n",
      "Iteration 100/100: Train Loss = 0.1915, Valid Loss = 0.2416\n",
      "Iteration 1/125: Train Loss = 0.6689, Valid Loss = 0.6696\n",
      "Iteration 2/125: Train Loss = 0.6467, Valid Loss = 0.6478\n",
      "Iteration 3/125: Train Loss = 0.6257, Valid Loss = 0.6274\n",
      "Iteration 4/125: Train Loss = 0.6064, Valid Loss = 0.6087\n",
      "Iteration 5/125: Train Loss = 0.5882, Valid Loss = 0.5908\n",
      "Iteration 6/125: Train Loss = 0.5713, Valid Loss = 0.5742\n",
      "Iteration 7/125: Train Loss = 0.5555, Valid Loss = 0.5587\n",
      "Iteration 8/125: Train Loss = 0.5406, Valid Loss = 0.5443\n",
      "Iteration 9/125: Train Loss = 0.5267, Valid Loss = 0.5307\n",
      "Iteration 10/125: Train Loss = 0.5139, Valid Loss = 0.5184\n",
      "Iteration 11/125: Train Loss = 0.5016, Valid Loss = 0.5065\n",
      "Iteration 12/125: Train Loss = 0.4903, Valid Loss = 0.4958\n",
      "Iteration 13/125: Train Loss = 0.4795, Valid Loss = 0.4852\n",
      "Iteration 14/125: Train Loss = 0.4695, Valid Loss = 0.4755\n",
      "Iteration 15/125: Train Loss = 0.4602, Valid Loss = 0.4663\n",
      "Iteration 16/125: Train Loss = 0.4513, Valid Loss = 0.4577\n",
      "Iteration 17/125: Train Loss = 0.4430, Valid Loss = 0.4499\n",
      "Iteration 18/125: Train Loss = 0.4350, Valid Loss = 0.4424\n",
      "Iteration 19/125: Train Loss = 0.4274, Valid Loss = 0.4352\n",
      "Iteration 20/125: Train Loss = 0.4203, Valid Loss = 0.4284\n",
      "Iteration 21/125: Train Loss = 0.4137, Valid Loss = 0.4221\n",
      "Iteration 22/125: Train Loss = 0.4073, Valid Loss = 0.4162\n",
      "Iteration 23/125: Train Loss = 0.4012, Valid Loss = 0.4104\n",
      "Iteration 24/125: Train Loss = 0.3955, Valid Loss = 0.4047\n",
      "Iteration 25/125: Train Loss = 0.3900, Valid Loss = 0.3995\n",
      "Iteration 26/125: Train Loss = 0.3850, Valid Loss = 0.3945\n",
      "Iteration 27/125: Train Loss = 0.3800, Valid Loss = 0.3895\n",
      "Iteration 28/125: Train Loss = 0.3751, Valid Loss = 0.3852\n",
      "Iteration 29/125: Train Loss = 0.3705, Valid Loss = 0.3810\n",
      "Iteration 30/125: Train Loss = 0.3662, Valid Loss = 0.3769\n",
      "Iteration 31/125: Train Loss = 0.3621, Valid Loss = 0.3732\n",
      "Iteration 32/125: Train Loss = 0.3579, Valid Loss = 0.3695\n",
      "Iteration 33/125: Train Loss = 0.3541, Valid Loss = 0.3661\n",
      "Iteration 34/125: Train Loss = 0.3505, Valid Loss = 0.3625\n",
      "Iteration 35/125: Train Loss = 0.3469, Valid Loss = 0.3594\n",
      "Iteration 36/125: Train Loss = 0.3436, Valid Loss = 0.3563\n",
      "Iteration 37/125: Train Loss = 0.3404, Valid Loss = 0.3532\n",
      "Iteration 38/125: Train Loss = 0.3373, Valid Loss = 0.3505\n",
      "Iteration 39/125: Train Loss = 0.3344, Valid Loss = 0.3477\n",
      "Iteration 40/125: Train Loss = 0.3315, Valid Loss = 0.3450\n",
      "Iteration 41/125: Train Loss = 0.3288, Valid Loss = 0.3425\n",
      "Iteration 42/125: Train Loss = 0.3261, Valid Loss = 0.3402\n",
      "Iteration 43/125: Train Loss = 0.3236, Valid Loss = 0.3378\n",
      "Iteration 44/125: Train Loss = 0.3211, Valid Loss = 0.3355\n",
      "Iteration 45/125: Train Loss = 0.3188, Valid Loss = 0.3334\n",
      "Iteration 46/125: Train Loss = 0.3164, Valid Loss = 0.3313\n",
      "Iteration 47/125: Train Loss = 0.3142, Valid Loss = 0.3294\n",
      "Iteration 48/125: Train Loss = 0.3121, Valid Loss = 0.3274\n",
      "Iteration 49/125: Train Loss = 0.3100, Valid Loss = 0.3256\n",
      "Iteration 50/125: Train Loss = 0.3078, Valid Loss = 0.3238\n",
      "Iteration 51/125: Train Loss = 0.3058, Valid Loss = 0.3220\n",
      "Iteration 52/125: Train Loss = 0.3038, Valid Loss = 0.3202\n",
      "Iteration 53/125: Train Loss = 0.3019, Valid Loss = 0.3185\n",
      "Iteration 54/125: Train Loss = 0.3001, Valid Loss = 0.3169\n",
      "Iteration 55/125: Train Loss = 0.2984, Valid Loss = 0.3153\n",
      "Iteration 56/125: Train Loss = 0.2966, Valid Loss = 0.3138\n",
      "Iteration 57/125: Train Loss = 0.2949, Valid Loss = 0.3124\n",
      "Iteration 58/125: Train Loss = 0.2932, Valid Loss = 0.3109\n",
      "Iteration 59/125: Train Loss = 0.2915, Valid Loss = 0.3095\n",
      "Iteration 60/125: Train Loss = 0.2899, Valid Loss = 0.3083\n",
      "Iteration 61/125: Train Loss = 0.2884, Valid Loss = 0.3072\n",
      "Iteration 62/125: Train Loss = 0.2870, Valid Loss = 0.3060\n",
      "Iteration 63/125: Train Loss = 0.2855, Valid Loss = 0.3046\n",
      "Iteration 64/125: Train Loss = 0.2842, Valid Loss = 0.3034\n",
      "Iteration 65/125: Train Loss = 0.2828, Valid Loss = 0.3022\n",
      "Iteration 66/125: Train Loss = 0.2814, Valid Loss = 0.3010\n",
      "Iteration 67/125: Train Loss = 0.2801, Valid Loss = 0.3000\n",
      "Iteration 68/125: Train Loss = 0.2789, Valid Loss = 0.2989\n",
      "Iteration 69/125: Train Loss = 0.2777, Valid Loss = 0.2978\n",
      "Iteration 70/125: Train Loss = 0.2765, Valid Loss = 0.2969\n",
      "Iteration 71/125: Train Loss = 0.2753, Valid Loss = 0.2958\n",
      "Iteration 72/125: Train Loss = 0.2742, Valid Loss = 0.2949\n",
      "Iteration 73/125: Train Loss = 0.2731, Valid Loss = 0.2939\n",
      "Iteration 74/125: Train Loss = 0.2720, Valid Loss = 0.2931\n",
      "Iteration 75/125: Train Loss = 0.2708, Valid Loss = 0.2921\n",
      "Iteration 76/125: Train Loss = 0.2698, Valid Loss = 0.2911\n",
      "Iteration 77/125: Train Loss = 0.2687, Valid Loss = 0.2900\n",
      "Iteration 78/125: Train Loss = 0.2677, Valid Loss = 0.2892\n",
      "Iteration 79/125: Train Loss = 0.2668, Valid Loss = 0.2883\n",
      "Iteration 80/125: Train Loss = 0.2659, Valid Loss = 0.2877\n",
      "Iteration 81/125: Train Loss = 0.2649, Valid Loss = 0.2870\n",
      "Iteration 82/125: Train Loss = 0.2641, Valid Loss = 0.2862\n",
      "Iteration 83/125: Train Loss = 0.2632, Valid Loss = 0.2855\n",
      "Iteration 84/125: Train Loss = 0.2624, Valid Loss = 0.2848\n",
      "Iteration 85/125: Train Loss = 0.2616, Valid Loss = 0.2840\n",
      "Iteration 86/125: Train Loss = 0.2606, Valid Loss = 0.2832\n",
      "Iteration 87/125: Train Loss = 0.2599, Valid Loss = 0.2826\n",
      "Iteration 88/125: Train Loss = 0.2591, Valid Loss = 0.2819\n",
      "Iteration 89/125: Train Loss = 0.2582, Valid Loss = 0.2812\n",
      "Iteration 90/125: Train Loss = 0.2575, Valid Loss = 0.2806\n",
      "Iteration 91/125: Train Loss = 0.2567, Valid Loss = 0.2800\n",
      "Iteration 92/125: Train Loss = 0.2560, Valid Loss = 0.2794\n",
      "Iteration 93/125: Train Loss = 0.2553, Valid Loss = 0.2787\n",
      "Iteration 94/125: Train Loss = 0.2545, Valid Loss = 0.2782\n",
      "Iteration 95/125: Train Loss = 0.2538, Valid Loss = 0.2776\n",
      "Iteration 96/125: Train Loss = 0.2531, Valid Loss = 0.2770\n",
      "Iteration 97/125: Train Loss = 0.2524, Valid Loss = 0.2764\n",
      "Iteration 98/125: Train Loss = 0.2516, Valid Loss = 0.2758\n",
      "Iteration 99/125: Train Loss = 0.2509, Valid Loss = 0.2753\n",
      "Iteration 100/125: Train Loss = 0.2501, Valid Loss = 0.2748\n",
      "Iteration 101/125: Train Loss = 0.2496, Valid Loss = 0.2744\n",
      "Iteration 102/125: Train Loss = 0.2490, Valid Loss = 0.2739\n",
      "Iteration 103/125: Train Loss = 0.2484, Valid Loss = 0.2734\n",
      "Iteration 104/125: Train Loss = 0.2478, Valid Loss = 0.2731\n",
      "Iteration 105/125: Train Loss = 0.2471, Valid Loss = 0.2727\n",
      "Iteration 106/125: Train Loss = 0.2466, Valid Loss = 0.2723\n",
      "Iteration 107/125: Train Loss = 0.2460, Valid Loss = 0.2718\n",
      "Iteration 108/125: Train Loss = 0.2455, Valid Loss = 0.2713\n",
      "Iteration 109/125: Train Loss = 0.2449, Valid Loss = 0.2707\n",
      "Iteration 110/125: Train Loss = 0.2443, Valid Loss = 0.2703\n",
      "Iteration 111/125: Train Loss = 0.2437, Valid Loss = 0.2698\n",
      "Iteration 112/125: Train Loss = 0.2432, Valid Loss = 0.2693\n",
      "Iteration 113/125: Train Loss = 0.2426, Valid Loss = 0.2689\n",
      "Iteration 114/125: Train Loss = 0.2421, Valid Loss = 0.2685\n",
      "Iteration 115/125: Train Loss = 0.2415, Valid Loss = 0.2681\n",
      "Iteration 116/125: Train Loss = 0.2410, Valid Loss = 0.2678\n",
      "Iteration 117/125: Train Loss = 0.2405, Valid Loss = 0.2675\n",
      "Iteration 118/125: Train Loss = 0.2399, Valid Loss = 0.2671\n",
      "Iteration 119/125: Train Loss = 0.2394, Valid Loss = 0.2668\n",
      "Iteration 120/125: Train Loss = 0.2390, Valid Loss = 0.2664\n",
      "Iteration 121/125: Train Loss = 0.2384, Valid Loss = 0.2660\n",
      "Iteration 122/125: Train Loss = 0.2380, Valid Loss = 0.2656\n",
      "Iteration 123/125: Train Loss = 0.2375, Valid Loss = 0.2653\n",
      "Iteration 124/125: Train Loss = 0.2371, Valid Loss = 0.2648\n",
      "Iteration 125/125: Train Loss = 0.2367, Valid Loss = 0.2644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:50:42,695] Trial 22 finished with value: 0.9635035021294929 and parameters: {'max_depth': 7, 'min_samples_leaf': 18, 'n_estimators': 125, 'learning_rate': 0.14488702626990924, 'subsample': 0.6892931138916784}. Best is trial 6 with value: 0.965235693124235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/105: Train Loss = 0.6260, Valid Loss = 0.6282\n",
      "Iteration 2/105: Train Loss = 0.5728, Valid Loss = 0.5763\n",
      "Iteration 3/105: Train Loss = 0.5295, Valid Loss = 0.5342\n",
      "Iteration 4/105: Train Loss = 0.4941, Valid Loss = 0.4997\n",
      "Iteration 5/105: Train Loss = 0.4646, Valid Loss = 0.4709\n",
      "Iteration 6/105: Train Loss = 0.4402, Valid Loss = 0.4470\n",
      "Iteration 7/105: Train Loss = 0.4202, Valid Loss = 0.4275\n",
      "Iteration 8/105: Train Loss = 0.4027, Valid Loss = 0.4106\n",
      "Iteration 9/105: Train Loss = 0.3878, Valid Loss = 0.3962\n",
      "Iteration 10/105: Train Loss = 0.3751, Valid Loss = 0.3840\n",
      "Iteration 11/105: Train Loss = 0.3638, Valid Loss = 0.3736\n",
      "Iteration 12/105: Train Loss = 0.3542, Valid Loss = 0.3644\n",
      "Iteration 13/105: Train Loss = 0.3451, Valid Loss = 0.3558\n",
      "Iteration 14/105: Train Loss = 0.3374, Valid Loss = 0.3491\n",
      "Iteration 15/105: Train Loss = 0.3305, Valid Loss = 0.3425\n",
      "Iteration 16/105: Train Loss = 0.3242, Valid Loss = 0.3365\n",
      "Iteration 17/105: Train Loss = 0.3186, Valid Loss = 0.3314\n",
      "Iteration 18/105: Train Loss = 0.3135, Valid Loss = 0.3263\n",
      "Iteration 19/105: Train Loss = 0.3087, Valid Loss = 0.3221\n",
      "Iteration 20/105: Train Loss = 0.3043, Valid Loss = 0.3180\n",
      "Iteration 21/105: Train Loss = 0.3004, Valid Loss = 0.3145\n",
      "Iteration 22/105: Train Loss = 0.2967, Valid Loss = 0.3112\n",
      "Iteration 23/105: Train Loss = 0.2931, Valid Loss = 0.3080\n",
      "Iteration 24/105: Train Loss = 0.2899, Valid Loss = 0.3051\n",
      "Iteration 25/105: Train Loss = 0.2869, Valid Loss = 0.3025\n",
      "Iteration 26/105: Train Loss = 0.2841, Valid Loss = 0.3001\n",
      "Iteration 27/105: Train Loss = 0.2815, Valid Loss = 0.2977\n",
      "Iteration 28/105: Train Loss = 0.2788, Valid Loss = 0.2954\n",
      "Iteration 29/105: Train Loss = 0.2765, Valid Loss = 0.2929\n",
      "Iteration 30/105: Train Loss = 0.2741, Valid Loss = 0.2907\n",
      "Iteration 31/105: Train Loss = 0.2722, Valid Loss = 0.2891\n",
      "Iteration 32/105: Train Loss = 0.2702, Valid Loss = 0.2874\n",
      "Iteration 33/105: Train Loss = 0.2683, Valid Loss = 0.2859\n",
      "Iteration 34/105: Train Loss = 0.2665, Valid Loss = 0.2842\n",
      "Iteration 35/105: Train Loss = 0.2649, Valid Loss = 0.2827\n",
      "Iteration 36/105: Train Loss = 0.2633, Valid Loss = 0.2814\n",
      "Iteration 37/105: Train Loss = 0.2617, Valid Loss = 0.2799\n",
      "Iteration 38/105: Train Loss = 0.2603, Valid Loss = 0.2787\n",
      "Iteration 39/105: Train Loss = 0.2585, Valid Loss = 0.2772\n",
      "Iteration 40/105: Train Loss = 0.2570, Valid Loss = 0.2760\n",
      "Iteration 41/105: Train Loss = 0.2557, Valid Loss = 0.2750\n",
      "Iteration 42/105: Train Loss = 0.2544, Valid Loss = 0.2740\n",
      "Iteration 43/105: Train Loss = 0.2531, Valid Loss = 0.2729\n",
      "Iteration 44/105: Train Loss = 0.2518, Valid Loss = 0.2715\n",
      "Iteration 45/105: Train Loss = 0.2505, Valid Loss = 0.2705\n",
      "Iteration 46/105: Train Loss = 0.2493, Valid Loss = 0.2698\n",
      "Iteration 47/105: Train Loss = 0.2482, Valid Loss = 0.2690\n",
      "Iteration 48/105: Train Loss = 0.2472, Valid Loss = 0.2681\n",
      "Iteration 49/105: Train Loss = 0.2462, Valid Loss = 0.2672\n",
      "Iteration 50/105: Train Loss = 0.2451, Valid Loss = 0.2663\n",
      "Iteration 51/105: Train Loss = 0.2442, Valid Loss = 0.2656\n",
      "Iteration 52/105: Train Loss = 0.2432, Valid Loss = 0.2646\n",
      "Iteration 53/105: Train Loss = 0.2422, Valid Loss = 0.2637\n",
      "Iteration 54/105: Train Loss = 0.2414, Valid Loss = 0.2633\n",
      "Iteration 55/105: Train Loss = 0.2407, Valid Loss = 0.2628\n",
      "Iteration 56/105: Train Loss = 0.2400, Valid Loss = 0.2623\n",
      "Iteration 57/105: Train Loss = 0.2391, Valid Loss = 0.2613\n",
      "Iteration 58/105: Train Loss = 0.2384, Valid Loss = 0.2609\n",
      "Iteration 59/105: Train Loss = 0.2375, Valid Loss = 0.2604\n",
      "Iteration 60/105: Train Loss = 0.2366, Valid Loss = 0.2600\n",
      "Iteration 61/105: Train Loss = 0.2357, Valid Loss = 0.2593\n",
      "Iteration 62/105: Train Loss = 0.2350, Valid Loss = 0.2588\n",
      "Iteration 63/105: Train Loss = 0.2345, Valid Loss = 0.2584\n",
      "Iteration 64/105: Train Loss = 0.2338, Valid Loss = 0.2580\n",
      "Iteration 65/105: Train Loss = 0.2331, Valid Loss = 0.2578\n",
      "Iteration 66/105: Train Loss = 0.2325, Valid Loss = 0.2574\n",
      "Iteration 67/105: Train Loss = 0.2320, Valid Loss = 0.2571\n",
      "Iteration 68/105: Train Loss = 0.2314, Valid Loss = 0.2569\n",
      "Iteration 69/105: Train Loss = 0.2307, Valid Loss = 0.2565\n",
      "Iteration 70/105: Train Loss = 0.2302, Valid Loss = 0.2559\n",
      "Iteration 71/105: Train Loss = 0.2297, Valid Loss = 0.2555\n",
      "Iteration 72/105: Train Loss = 0.2293, Valid Loss = 0.2552\n",
      "Iteration 73/105: Train Loss = 0.2289, Valid Loss = 0.2549\n",
      "Iteration 74/105: Train Loss = 0.2284, Valid Loss = 0.2548\n",
      "Iteration 75/105: Train Loss = 0.2279, Valid Loss = 0.2546\n",
      "Iteration 76/105: Train Loss = 0.2274, Valid Loss = 0.2543\n",
      "Iteration 77/105: Train Loss = 0.2267, Valid Loss = 0.2537\n",
      "Iteration 78/105: Train Loss = 0.2263, Valid Loss = 0.2533\n",
      "Iteration 79/105: Train Loss = 0.2259, Valid Loss = 0.2532\n",
      "Iteration 80/105: Train Loss = 0.2255, Valid Loss = 0.2527\n",
      "Iteration 81/105: Train Loss = 0.2250, Valid Loss = 0.2524\n",
      "Iteration 82/105: Train Loss = 0.2245, Valid Loss = 0.2520\n",
      "Iteration 83/105: Train Loss = 0.2238, Valid Loss = 0.2516\n",
      "Iteration 84/105: Train Loss = 0.2233, Valid Loss = 0.2513\n",
      "Iteration 85/105: Train Loss = 0.2229, Valid Loss = 0.2510\n",
      "Iteration 86/105: Train Loss = 0.2224, Valid Loss = 0.2508\n",
      "Iteration 87/105: Train Loss = 0.2220, Valid Loss = 0.2505\n",
      "Iteration 88/105: Train Loss = 0.2217, Valid Loss = 0.2503\n",
      "Iteration 89/105: Train Loss = 0.2212, Valid Loss = 0.2500\n",
      "Iteration 90/105: Train Loss = 0.2208, Valid Loss = 0.2497\n",
      "Iteration 91/105: Train Loss = 0.2204, Valid Loss = 0.2493\n",
      "Iteration 92/105: Train Loss = 0.2200, Valid Loss = 0.2491\n",
      "Iteration 93/105: Train Loss = 0.2196, Valid Loss = 0.2489\n",
      "Iteration 94/105: Train Loss = 0.2192, Valid Loss = 0.2488\n",
      "Iteration 95/105: Train Loss = 0.2188, Valid Loss = 0.2487\n",
      "Iteration 96/105: Train Loss = 0.2184, Valid Loss = 0.2485\n",
      "Iteration 97/105: Train Loss = 0.2181, Valid Loss = 0.2482\n",
      "Iteration 98/105: Train Loss = 0.2178, Valid Loss = 0.2481\n",
      "Iteration 99/105: Train Loss = 0.2174, Valid Loss = 0.2479\n",
      "Iteration 100/105: Train Loss = 0.2170, Valid Loss = 0.2478\n",
      "Iteration 101/105: Train Loss = 0.2168, Valid Loss = 0.2476\n",
      "Iteration 102/105: Train Loss = 0.2166, Valid Loss = 0.2472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:50:47,365] Trial 23 finished with value: 0.9639804933103036 and parameters: {'max_depth': 5, 'min_samples_leaf': 17, 'n_estimators': 105, 'learning_rate': 0.44137036864583695, 'subsample': 0.8446220906037643}. Best is trial 6 with value: 0.965235693124235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 103/105: Train Loss = 0.2162, Valid Loss = 0.2471\n",
      "Iteration 104/105: Train Loss = 0.2159, Valid Loss = 0.2471\n",
      "Iteration 105/105: Train Loss = 0.2155, Valid Loss = 0.2469\n",
      "Iteration 1/80: Train Loss = 0.6124, Valid Loss = 0.6150\n",
      "Iteration 2/80: Train Loss = 0.5497, Valid Loss = 0.5540\n",
      "Iteration 3/80: Train Loss = 0.5014, Valid Loss = 0.5078\n",
      "Iteration 4/80: Train Loss = 0.4630, Valid Loss = 0.4710\n",
      "Iteration 5/80: Train Loss = 0.4316, Valid Loss = 0.4411\n",
      "Iteration 6/80: Train Loss = 0.4059, Valid Loss = 0.4171\n",
      "Iteration 7/80: Train Loss = 0.3850, Valid Loss = 0.3981\n",
      "Iteration 8/80: Train Loss = 0.3673, Valid Loss = 0.3813\n",
      "Iteration 9/80: Train Loss = 0.3525, Valid Loss = 0.3675\n",
      "Iteration 10/80: Train Loss = 0.3397, Valid Loss = 0.3551\n",
      "Iteration 11/80: Train Loss = 0.3284, Valid Loss = 0.3450\n",
      "Iteration 12/80: Train Loss = 0.3187, Valid Loss = 0.3370\n",
      "Iteration 13/80: Train Loss = 0.3104, Valid Loss = 0.3294\n",
      "Iteration 14/80: Train Loss = 0.3031, Valid Loss = 0.3231\n",
      "Iteration 15/80: Train Loss = 0.2961, Valid Loss = 0.3169\n",
      "Iteration 16/80: Train Loss = 0.2900, Valid Loss = 0.3114\n",
      "Iteration 17/80: Train Loss = 0.2843, Valid Loss = 0.3064\n",
      "Iteration 18/80: Train Loss = 0.2793, Valid Loss = 0.3019\n",
      "Iteration 19/80: Train Loss = 0.2749, Valid Loss = 0.2985\n",
      "Iteration 20/80: Train Loss = 0.2706, Valid Loss = 0.2953\n",
      "Iteration 21/80: Train Loss = 0.2670, Valid Loss = 0.2924\n",
      "Iteration 22/80: Train Loss = 0.2635, Valid Loss = 0.2898\n",
      "Iteration 23/80: Train Loss = 0.2603, Valid Loss = 0.2872\n",
      "Iteration 24/80: Train Loss = 0.2573, Valid Loss = 0.2839\n",
      "Iteration 25/80: Train Loss = 0.2544, Valid Loss = 0.2812\n",
      "Iteration 26/80: Train Loss = 0.2517, Valid Loss = 0.2793\n",
      "Iteration 27/80: Train Loss = 0.2491, Valid Loss = 0.2776\n",
      "Iteration 28/80: Train Loss = 0.2469, Valid Loss = 0.2759\n",
      "Iteration 29/80: Train Loss = 0.2448, Valid Loss = 0.2740\n",
      "Iteration 30/80: Train Loss = 0.2425, Valid Loss = 0.2725\n",
      "Iteration 31/80: Train Loss = 0.2403, Valid Loss = 0.2710\n",
      "Iteration 32/80: Train Loss = 0.2381, Valid Loss = 0.2690\n",
      "Iteration 33/80: Train Loss = 0.2363, Valid Loss = 0.2675\n",
      "Iteration 34/80: Train Loss = 0.2342, Valid Loss = 0.2663\n",
      "Iteration 35/80: Train Loss = 0.2327, Valid Loss = 0.2653\n",
      "Iteration 36/80: Train Loss = 0.2311, Valid Loss = 0.2642\n",
      "Iteration 37/80: Train Loss = 0.2293, Valid Loss = 0.2631\n",
      "Iteration 38/80: Train Loss = 0.2276, Valid Loss = 0.2615\n",
      "Iteration 39/80: Train Loss = 0.2263, Valid Loss = 0.2603\n",
      "Iteration 40/80: Train Loss = 0.2248, Valid Loss = 0.2592\n",
      "Iteration 41/80: Train Loss = 0.2236, Valid Loss = 0.2583\n",
      "Iteration 42/80: Train Loss = 0.2224, Valid Loss = 0.2576\n",
      "Iteration 43/80: Train Loss = 0.2214, Valid Loss = 0.2568\n",
      "Iteration 44/80: Train Loss = 0.2203, Valid Loss = 0.2561\n",
      "Iteration 45/80: Train Loss = 0.2193, Valid Loss = 0.2553\n",
      "Iteration 46/80: Train Loss = 0.2183, Valid Loss = 0.2548\n",
      "Iteration 47/80: Train Loss = 0.2174, Valid Loss = 0.2543\n",
      "Iteration 48/80: Train Loss = 0.2163, Valid Loss = 0.2535\n",
      "Iteration 49/80: Train Loss = 0.2154, Valid Loss = 0.2527\n",
      "Iteration 50/80: Train Loss = 0.2145, Valid Loss = 0.2525\n",
      "Iteration 51/80: Train Loss = 0.2134, Valid Loss = 0.2517\n",
      "Iteration 52/80: Train Loss = 0.2125, Valid Loss = 0.2515\n",
      "Iteration 53/80: Train Loss = 0.2117, Valid Loss = 0.2513\n",
      "Iteration 54/80: Train Loss = 0.2107, Valid Loss = 0.2506\n",
      "Iteration 55/80: Train Loss = 0.2098, Valid Loss = 0.2497\n",
      "Iteration 56/80: Train Loss = 0.2091, Valid Loss = 0.2492\n",
      "Iteration 57/80: Train Loss = 0.2083, Valid Loss = 0.2489\n",
      "Iteration 58/80: Train Loss = 0.2076, Valid Loss = 0.2484\n",
      "Iteration 59/80: Train Loss = 0.2069, Valid Loss = 0.2480\n",
      "Iteration 60/80: Train Loss = 0.2060, Valid Loss = 0.2476\n",
      "Iteration 61/80: Train Loss = 0.2054, Valid Loss = 0.2472\n",
      "Iteration 62/80: Train Loss = 0.2048, Valid Loss = 0.2470\n",
      "Iteration 63/80: Train Loss = 0.2043, Valid Loss = 0.2467\n",
      "Iteration 64/80: Train Loss = 0.2035, Valid Loss = 0.2462\n",
      "Iteration 65/80: Train Loss = 0.2030, Valid Loss = 0.2460\n",
      "Iteration 66/80: Train Loss = 0.2023, Valid Loss = 0.2456\n",
      "Iteration 67/80: Train Loss = 0.2017, Valid Loss = 0.2452\n",
      "Iteration 68/80: Train Loss = 0.2012, Valid Loss = 0.2449\n",
      "Iteration 69/80: Train Loss = 0.2007, Valid Loss = 0.2449\n",
      "Iteration 70/80: Train Loss = 0.2002, Valid Loss = 0.2446\n",
      "Iteration 71/80: Train Loss = 0.1995, Valid Loss = 0.2441\n",
      "Iteration 72/80: Train Loss = 0.1990, Valid Loss = 0.2440\n",
      "Iteration 73/80: Train Loss = 0.1986, Valid Loss = 0.2439\n",
      "Iteration 74/80: Train Loss = 0.1980, Valid Loss = 0.2438\n",
      "Iteration 75/80: Train Loss = 0.1974, Valid Loss = 0.2436\n",
      "Iteration 76/80: Train Loss = 0.1969, Valid Loss = 0.2434\n",
      "Iteration 77/80: Train Loss = 0.1965, Valid Loss = 0.2433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:50:52,116] Trial 24 finished with value: 0.9648830018034918 and parameters: {'max_depth': 7, 'min_samples_leaf': 12, 'n_estimators': 80, 'learning_rate': 0.5040800946645468, 'subsample': 0.9192881340808673}. Best is trial 6 with value: 0.965235693124235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 78/80: Train Loss = 0.1961, Valid Loss = 0.2432\n",
      "Iteration 79/80: Train Loss = 0.1957, Valid Loss = 0.2430\n",
      "Iteration 80/80: Train Loss = 0.1953, Valid Loss = 0.2430\n",
      "Iteration 1/50: Train Loss = 0.6602, Valid Loss = 0.6608\n",
      "Iteration 2/50: Train Loss = 0.6306, Valid Loss = 0.6321\n",
      "Iteration 3/50: Train Loss = 0.6041, Valid Loss = 0.6061\n",
      "Iteration 4/50: Train Loss = 0.5802, Valid Loss = 0.5825\n",
      "Iteration 5/50: Train Loss = 0.5589, Valid Loss = 0.5621\n",
      "Iteration 6/50: Train Loss = 0.5394, Valid Loss = 0.5430\n",
      "Iteration 7/50: Train Loss = 0.5220, Valid Loss = 0.5261\n",
      "Iteration 8/50: Train Loss = 0.5061, Valid Loss = 0.5106\n",
      "Iteration 9/50: Train Loss = 0.4912, Valid Loss = 0.4961\n",
      "Iteration 10/50: Train Loss = 0.4775, Valid Loss = 0.4824\n",
      "Iteration 11/50: Train Loss = 0.4653, Valid Loss = 0.4706\n",
      "Iteration 12/50: Train Loss = 0.4542, Valid Loss = 0.4600\n",
      "Iteration 13/50: Train Loss = 0.4439, Valid Loss = 0.4499\n",
      "Iteration 14/50: Train Loss = 0.4344, Valid Loss = 0.4406\n",
      "Iteration 15/50: Train Loss = 0.4254, Valid Loss = 0.4319\n",
      "Iteration 16/50: Train Loss = 0.4175, Valid Loss = 0.4241\n",
      "Iteration 17/50: Train Loss = 0.4100, Valid Loss = 0.4170\n",
      "Iteration 18/50: Train Loss = 0.4031, Valid Loss = 0.4101\n",
      "Iteration 19/50: Train Loss = 0.3964, Valid Loss = 0.4034\n",
      "Iteration 20/50: Train Loss = 0.3903, Valid Loss = 0.3976\n",
      "Iteration 21/50: Train Loss = 0.3846, Valid Loss = 0.3922\n",
      "Iteration 22/50: Train Loss = 0.3793, Valid Loss = 0.3872\n",
      "Iteration 23/50: Train Loss = 0.3745, Valid Loss = 0.3825\n",
      "Iteration 24/50: Train Loss = 0.3696, Valid Loss = 0.3778\n",
      "Iteration 25/50: Train Loss = 0.3652, Valid Loss = 0.3737\n",
      "Iteration 26/50: Train Loss = 0.3608, Valid Loss = 0.3693\n",
      "Iteration 27/50: Train Loss = 0.3569, Valid Loss = 0.3656\n",
      "Iteration 28/50: Train Loss = 0.3532, Valid Loss = 0.3620\n",
      "Iteration 29/50: Train Loss = 0.3496, Valid Loss = 0.3585\n",
      "Iteration 30/50: Train Loss = 0.3462, Valid Loss = 0.3551\n",
      "Iteration 31/50: Train Loss = 0.3430, Valid Loss = 0.3521\n",
      "Iteration 32/50: Train Loss = 0.3399, Valid Loss = 0.3491\n",
      "Iteration 33/50: Train Loss = 0.3370, Valid Loss = 0.3462\n",
      "Iteration 34/50: Train Loss = 0.3342, Valid Loss = 0.3435\n",
      "Iteration 35/50: Train Loss = 0.3316, Valid Loss = 0.3411\n",
      "Iteration 36/50: Train Loss = 0.3291, Valid Loss = 0.3388\n",
      "Iteration 37/50: Train Loss = 0.3267, Valid Loss = 0.3364\n",
      "Iteration 38/50: Train Loss = 0.3243, Valid Loss = 0.3342\n",
      "Iteration 39/50: Train Loss = 0.3222, Valid Loss = 0.3323\n",
      "Iteration 40/50: Train Loss = 0.3200, Valid Loss = 0.3303\n",
      "Iteration 41/50: Train Loss = 0.3179, Valid Loss = 0.3284\n",
      "Iteration 42/50: Train Loss = 0.3159, Valid Loss = 0.3264\n",
      "Iteration 43/50: Train Loss = 0.3140, Valid Loss = 0.3244\n",
      "Iteration 44/50: Train Loss = 0.3122, Valid Loss = 0.3229\n",
      "Iteration 45/50: Train Loss = 0.3103, Valid Loss = 0.3213\n",
      "Iteration 46/50: Train Loss = 0.3084, Valid Loss = 0.3195\n",
      "Iteration 47/50: Train Loss = 0.3068, Valid Loss = 0.3180\n",
      "Iteration 48/50: Train Loss = 0.3052, Valid Loss = 0.3167\n",
      "Iteration 49/50: Train Loss = 0.3037, Valid Loss = 0.3152\n",
      "Iteration 50/50: Train Loss = 0.3021, Valid Loss = 0.3136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:50:53,959] Trial 25 finished with value: 0.9540895035250069 and parameters: {'max_depth': 4, 'min_samples_leaf': 12, 'n_estimators': 50, 'learning_rate': 0.22090927257459447, 'subsample': 0.9324820018344014}. Best is trial 6 with value: 0.965235693124235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/80: Train Loss = 0.6783, Valid Loss = 0.6789\n",
      "Iteration 2/80: Train Loss = 0.6641, Valid Loss = 0.6652\n",
      "Iteration 3/80: Train Loss = 0.6503, Valid Loss = 0.6521\n",
      "Iteration 4/80: Train Loss = 0.6373, Valid Loss = 0.6398\n",
      "Iteration 5/80: Train Loss = 0.6247, Valid Loss = 0.6277\n",
      "Iteration 6/80: Train Loss = 0.6126, Valid Loss = 0.6161\n",
      "Iteration 7/80: Train Loss = 0.6009, Valid Loss = 0.6052\n",
      "Iteration 8/80: Train Loss = 0.5896, Valid Loss = 0.5944\n",
      "Iteration 9/80: Train Loss = 0.5788, Valid Loss = 0.5841\n",
      "Iteration 10/80: Train Loss = 0.5684, Valid Loss = 0.5743\n",
      "Iteration 11/80: Train Loss = 0.5585, Valid Loss = 0.5648\n",
      "Iteration 12/80: Train Loss = 0.5489, Valid Loss = 0.5559\n",
      "Iteration 13/80: Train Loss = 0.5398, Valid Loss = 0.5472\n",
      "Iteration 14/80: Train Loss = 0.5309, Valid Loss = 0.5390\n",
      "Iteration 15/80: Train Loss = 0.5224, Valid Loss = 0.5308\n",
      "Iteration 16/80: Train Loss = 0.5144, Valid Loss = 0.5232\n",
      "Iteration 17/80: Train Loss = 0.5065, Valid Loss = 0.5157\n",
      "Iteration 18/80: Train Loss = 0.4987, Valid Loss = 0.5085\n",
      "Iteration 19/80: Train Loss = 0.4914, Valid Loss = 0.5016\n",
      "Iteration 20/80: Train Loss = 0.4844, Valid Loss = 0.4948\n",
      "Iteration 21/80: Train Loss = 0.4775, Valid Loss = 0.4882\n",
      "Iteration 22/80: Train Loss = 0.4709, Valid Loss = 0.4820\n",
      "Iteration 23/80: Train Loss = 0.4645, Valid Loss = 0.4761\n",
      "Iteration 24/80: Train Loss = 0.4584, Valid Loss = 0.4704\n",
      "Iteration 25/80: Train Loss = 0.4524, Valid Loss = 0.4649\n",
      "Iteration 26/80: Train Loss = 0.4467, Valid Loss = 0.4596\n",
      "Iteration 27/80: Train Loss = 0.4411, Valid Loss = 0.4545\n",
      "Iteration 28/80: Train Loss = 0.4358, Valid Loss = 0.4495\n",
      "Iteration 29/80: Train Loss = 0.4305, Valid Loss = 0.4446\n",
      "Iteration 30/80: Train Loss = 0.4255, Valid Loss = 0.4401\n",
      "Iteration 31/80: Train Loss = 0.4208, Valid Loss = 0.4356\n",
      "Iteration 32/80: Train Loss = 0.4161, Valid Loss = 0.4313\n",
      "Iteration 33/80: Train Loss = 0.4114, Valid Loss = 0.4271\n",
      "Iteration 34/80: Train Loss = 0.4071, Valid Loss = 0.4231\n",
      "Iteration 35/80: Train Loss = 0.4029, Valid Loss = 0.4193\n",
      "Iteration 36/80: Train Loss = 0.3988, Valid Loss = 0.4156\n",
      "Iteration 37/80: Train Loss = 0.3947, Valid Loss = 0.4118\n",
      "Iteration 38/80: Train Loss = 0.3908, Valid Loss = 0.4082\n",
      "Iteration 39/80: Train Loss = 0.3869, Valid Loss = 0.4047\n",
      "Iteration 40/80: Train Loss = 0.3832, Valid Loss = 0.4014\n",
      "Iteration 41/80: Train Loss = 0.3796, Valid Loss = 0.3982\n",
      "Iteration 42/80: Train Loss = 0.3762, Valid Loss = 0.3951\n",
      "Iteration 43/80: Train Loss = 0.3728, Valid Loss = 0.3921\n",
      "Iteration 44/80: Train Loss = 0.3696, Valid Loss = 0.3891\n",
      "Iteration 45/80: Train Loss = 0.3663, Valid Loss = 0.3862\n",
      "Iteration 46/80: Train Loss = 0.3631, Valid Loss = 0.3833\n",
      "Iteration 47/80: Train Loss = 0.3600, Valid Loss = 0.3805\n",
      "Iteration 48/80: Train Loss = 0.3570, Valid Loss = 0.3779\n",
      "Iteration 49/80: Train Loss = 0.3541, Valid Loss = 0.3754\n",
      "Iteration 50/80: Train Loss = 0.3512, Valid Loss = 0.3727\n",
      "Iteration 51/80: Train Loss = 0.3485, Valid Loss = 0.3702\n",
      "Iteration 52/80: Train Loss = 0.3458, Valid Loss = 0.3679\n",
      "Iteration 53/80: Train Loss = 0.3432, Valid Loss = 0.3656\n",
      "Iteration 54/80: Train Loss = 0.3406, Valid Loss = 0.3634\n",
      "Iteration 55/80: Train Loss = 0.3380, Valid Loss = 0.3612\n",
      "Iteration 56/80: Train Loss = 0.3357, Valid Loss = 0.3590\n",
      "Iteration 57/80: Train Loss = 0.3333, Valid Loss = 0.3569\n",
      "Iteration 58/80: Train Loss = 0.3309, Valid Loss = 0.3548\n",
      "Iteration 59/80: Train Loss = 0.3287, Valid Loss = 0.3528\n",
      "Iteration 60/80: Train Loss = 0.3266, Valid Loss = 0.3508\n",
      "Iteration 61/80: Train Loss = 0.3244, Valid Loss = 0.3490\n",
      "Iteration 62/80: Train Loss = 0.3224, Valid Loss = 0.3472\n",
      "Iteration 63/80: Train Loss = 0.3203, Valid Loss = 0.3454\n",
      "Iteration 64/80: Train Loss = 0.3183, Valid Loss = 0.3437\n",
      "Iteration 65/80: Train Loss = 0.3164, Valid Loss = 0.3420\n",
      "Iteration 66/80: Train Loss = 0.3144, Valid Loss = 0.3403\n",
      "Iteration 67/80: Train Loss = 0.3125, Valid Loss = 0.3387\n",
      "Iteration 68/80: Train Loss = 0.3105, Valid Loss = 0.3372\n",
      "Iteration 69/80: Train Loss = 0.3087, Valid Loss = 0.3357\n",
      "Iteration 70/80: Train Loss = 0.3069, Valid Loss = 0.3341\n",
      "Iteration 71/80: Train Loss = 0.3052, Valid Loss = 0.3326\n",
      "Iteration 72/80: Train Loss = 0.3036, Valid Loss = 0.3312\n",
      "Iteration 73/80: Train Loss = 0.3018, Valid Loss = 0.3300\n",
      "Iteration 74/80: Train Loss = 0.3002, Valid Loss = 0.3286\n",
      "Iteration 75/80: Train Loss = 0.2987, Valid Loss = 0.3273\n",
      "Iteration 76/80: Train Loss = 0.2972, Valid Loss = 0.3261\n",
      "Iteration 77/80: Train Loss = 0.2956, Valid Loss = 0.3248\n",
      "Iteration 78/80: Train Loss = 0.2941, Valid Loss = 0.3235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:51:00,989] Trial 26 finished with value: 0.962779436380205 and parameters: {'max_depth': 9, 'min_samples_leaf': 7, 'n_estimators': 80, 'learning_rate': 0.08388246627370188, 'subsample': 0.9121250729123646}. Best is trial 6 with value: 0.965235693124235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 79/80: Train Loss = 0.2926, Valid Loss = 0.3224\n",
      "Iteration 80/80: Train Loss = 0.2911, Valid Loss = 0.3212\n",
      "Iteration 1/140: Train Loss = 0.6404, Valid Loss = 0.6423\n",
      "Iteration 2/140: Train Loss = 0.5957, Valid Loss = 0.5992\n",
      "Iteration 3/140: Train Loss = 0.5576, Valid Loss = 0.5622\n",
      "Iteration 4/140: Train Loss = 0.5248, Valid Loss = 0.5299\n",
      "Iteration 5/140: Train Loss = 0.4967, Valid Loss = 0.5024\n",
      "Iteration 6/140: Train Loss = 0.4720, Valid Loss = 0.4786\n",
      "Iteration 7/140: Train Loss = 0.4505, Valid Loss = 0.4585\n",
      "Iteration 8/140: Train Loss = 0.4316, Valid Loss = 0.4409\n",
      "Iteration 9/140: Train Loss = 0.4152, Valid Loss = 0.4255\n",
      "Iteration 10/140: Train Loss = 0.4005, Valid Loss = 0.4113\n",
      "Iteration 11/140: Train Loss = 0.3875, Valid Loss = 0.3996\n",
      "Iteration 12/140: Train Loss = 0.3755, Valid Loss = 0.3887\n",
      "Iteration 13/140: Train Loss = 0.3653, Valid Loss = 0.3789\n",
      "Iteration 14/140: Train Loss = 0.3557, Valid Loss = 0.3700\n",
      "Iteration 15/140: Train Loss = 0.3469, Valid Loss = 0.3619\n",
      "Iteration 16/140: Train Loss = 0.3393, Valid Loss = 0.3547\n",
      "Iteration 17/140: Train Loss = 0.3321, Valid Loss = 0.3482\n",
      "Iteration 18/140: Train Loss = 0.3255, Valid Loss = 0.3424\n",
      "Iteration 19/140: Train Loss = 0.3196, Valid Loss = 0.3373\n",
      "Iteration 20/140: Train Loss = 0.3142, Valid Loss = 0.3324\n",
      "Iteration 21/140: Train Loss = 0.3088, Valid Loss = 0.3276\n",
      "Iteration 22/140: Train Loss = 0.3041, Valid Loss = 0.3231\n",
      "Iteration 23/140: Train Loss = 0.2998, Valid Loss = 0.3194\n",
      "Iteration 24/140: Train Loss = 0.2958, Valid Loss = 0.3157\n",
      "Iteration 25/140: Train Loss = 0.2921, Valid Loss = 0.3124\n",
      "Iteration 26/140: Train Loss = 0.2885, Valid Loss = 0.3089\n",
      "Iteration 27/140: Train Loss = 0.2850, Valid Loss = 0.3059\n",
      "Iteration 28/140: Train Loss = 0.2818, Valid Loss = 0.3031\n",
      "Iteration 29/140: Train Loss = 0.2788, Valid Loss = 0.3007\n",
      "Iteration 30/140: Train Loss = 0.2758, Valid Loss = 0.2982\n",
      "Iteration 31/140: Train Loss = 0.2731, Valid Loss = 0.2960\n",
      "Iteration 32/140: Train Loss = 0.2705, Valid Loss = 0.2939\n",
      "Iteration 33/140: Train Loss = 0.2679, Valid Loss = 0.2914\n",
      "Iteration 34/140: Train Loss = 0.2653, Valid Loss = 0.2890\n",
      "Iteration 35/140: Train Loss = 0.2629, Valid Loss = 0.2872\n",
      "Iteration 36/140: Train Loss = 0.2603, Valid Loss = 0.2852\n",
      "Iteration 37/140: Train Loss = 0.2583, Valid Loss = 0.2835\n",
      "Iteration 38/140: Train Loss = 0.2564, Valid Loss = 0.2821\n",
      "Iteration 39/140: Train Loss = 0.2543, Valid Loss = 0.2805\n",
      "Iteration 40/140: Train Loss = 0.2526, Valid Loss = 0.2789\n",
      "Iteration 41/140: Train Loss = 0.2509, Valid Loss = 0.2776\n",
      "Iteration 42/140: Train Loss = 0.2493, Valid Loss = 0.2761\n",
      "Iteration 43/140: Train Loss = 0.2478, Valid Loss = 0.2751\n",
      "Iteration 44/140: Train Loss = 0.2463, Valid Loss = 0.2737\n",
      "Iteration 45/140: Train Loss = 0.2447, Valid Loss = 0.2726\n",
      "Iteration 46/140: Train Loss = 0.2432, Valid Loss = 0.2713\n",
      "Iteration 47/140: Train Loss = 0.2420, Valid Loss = 0.2705\n",
      "Iteration 48/140: Train Loss = 0.2407, Valid Loss = 0.2695\n",
      "Iteration 49/140: Train Loss = 0.2394, Valid Loss = 0.2685\n",
      "Iteration 50/140: Train Loss = 0.2382, Valid Loss = 0.2674\n",
      "Iteration 51/140: Train Loss = 0.2369, Valid Loss = 0.2666\n",
      "Iteration 52/140: Train Loss = 0.2358, Valid Loss = 0.2658\n",
      "Iteration 53/140: Train Loss = 0.2347, Valid Loss = 0.2651\n",
      "Iteration 54/140: Train Loss = 0.2337, Valid Loss = 0.2643\n",
      "Iteration 55/140: Train Loss = 0.2326, Valid Loss = 0.2634\n",
      "Iteration 56/140: Train Loss = 0.2316, Valid Loss = 0.2629\n",
      "Iteration 57/140: Train Loss = 0.2306, Valid Loss = 0.2623\n",
      "Iteration 58/140: Train Loss = 0.2295, Valid Loss = 0.2612\n",
      "Iteration 59/140: Train Loss = 0.2285, Valid Loss = 0.2606\n",
      "Iteration 60/140: Train Loss = 0.2276, Valid Loss = 0.2597\n",
      "Iteration 61/140: Train Loss = 0.2267, Valid Loss = 0.2591\n",
      "Iteration 62/140: Train Loss = 0.2259, Valid Loss = 0.2584\n",
      "Iteration 63/140: Train Loss = 0.2250, Valid Loss = 0.2578\n",
      "Iteration 64/140: Train Loss = 0.2241, Valid Loss = 0.2573\n",
      "Iteration 65/140: Train Loss = 0.2233, Valid Loss = 0.2569\n",
      "Iteration 66/140: Train Loss = 0.2224, Valid Loss = 0.2566\n",
      "Iteration 67/140: Train Loss = 0.2217, Valid Loss = 0.2560\n",
      "Iteration 68/140: Train Loss = 0.2208, Valid Loss = 0.2553\n",
      "Iteration 69/140: Train Loss = 0.2199, Valid Loss = 0.2549\n",
      "Iteration 70/140: Train Loss = 0.2192, Valid Loss = 0.2544\n",
      "Iteration 71/140: Train Loss = 0.2185, Valid Loss = 0.2542\n",
      "Iteration 72/140: Train Loss = 0.2179, Valid Loss = 0.2539\n",
      "Iteration 73/140: Train Loss = 0.2172, Valid Loss = 0.2534\n",
      "Iteration 74/140: Train Loss = 0.2166, Valid Loss = 0.2528\n",
      "Iteration 75/140: Train Loss = 0.2161, Valid Loss = 0.2525\n",
      "Iteration 76/140: Train Loss = 0.2155, Valid Loss = 0.2521\n",
      "Iteration 77/140: Train Loss = 0.2148, Valid Loss = 0.2518\n",
      "Iteration 78/140: Train Loss = 0.2142, Valid Loss = 0.2515\n",
      "Iteration 79/140: Train Loss = 0.2137, Valid Loss = 0.2511\n",
      "Iteration 80/140: Train Loss = 0.2129, Valid Loss = 0.2507\n",
      "Iteration 81/140: Train Loss = 0.2124, Valid Loss = 0.2503\n",
      "Iteration 82/140: Train Loss = 0.2117, Valid Loss = 0.2500\n",
      "Iteration 83/140: Train Loss = 0.2113, Valid Loss = 0.2495\n",
      "Iteration 84/140: Train Loss = 0.2108, Valid Loss = 0.2494\n",
      "Iteration 85/140: Train Loss = 0.2103, Valid Loss = 0.2492\n",
      "Iteration 86/140: Train Loss = 0.2098, Valid Loss = 0.2489\n",
      "Iteration 87/140: Train Loss = 0.2094, Valid Loss = 0.2489\n",
      "Iteration 88/140: Train Loss = 0.2087, Valid Loss = 0.2485\n",
      "Iteration 89/140: Train Loss = 0.2083, Valid Loss = 0.2481\n",
      "Iteration 90/140: Train Loss = 0.2078, Valid Loss = 0.2478\n",
      "Iteration 91/140: Train Loss = 0.2074, Valid Loss = 0.2476\n",
      "Iteration 92/140: Train Loss = 0.2068, Valid Loss = 0.2472\n",
      "Iteration 93/140: Train Loss = 0.2064, Valid Loss = 0.2471\n",
      "Iteration 94/140: Train Loss = 0.2060, Valid Loss = 0.2470\n",
      "Iteration 95/140: Train Loss = 0.2056, Valid Loss = 0.2469\n",
      "Iteration 96/140: Train Loss = 0.2052, Valid Loss = 0.2468\n",
      "Iteration 97/140: Train Loss = 0.2047, Valid Loss = 0.2465\n",
      "Iteration 98/140: Train Loss = 0.2043, Valid Loss = 0.2463\n",
      "Iteration 99/140: Train Loss = 0.2039, Valid Loss = 0.2461\n",
      "Iteration 100/140: Train Loss = 0.2035, Valid Loss = 0.2458\n",
      "Iteration 101/140: Train Loss = 0.2031, Valid Loss = 0.2456\n",
      "Iteration 102/140: Train Loss = 0.2027, Valid Loss = 0.2455\n",
      "Iteration 103/140: Train Loss = 0.2024, Valid Loss = 0.2452\n",
      "Iteration 104/140: Train Loss = 0.2021, Valid Loss = 0.2450\n",
      "Iteration 105/140: Train Loss = 0.2018, Valid Loss = 0.2448\n",
      "Iteration 106/140: Train Loss = 0.2015, Valid Loss = 0.2448\n",
      "Iteration 107/140: Train Loss = 0.2012, Valid Loss = 0.2447\n",
      "Iteration 108/140: Train Loss = 0.2007, Valid Loss = 0.2445\n",
      "Iteration 109/140: Train Loss = 0.2004, Valid Loss = 0.2443\n",
      "Iteration 110/140: Train Loss = 0.2000, Valid Loss = 0.2441\n",
      "Iteration 111/140: Train Loss = 0.1997, Valid Loss = 0.2440\n",
      "Iteration 112/140: Train Loss = 0.1993, Valid Loss = 0.2439\n",
      "Iteration 113/140: Train Loss = 0.1990, Valid Loss = 0.2437\n",
      "Iteration 114/140: Train Loss = 0.1987, Valid Loss = 0.2436\n",
      "Iteration 115/140: Train Loss = 0.1985, Valid Loss = 0.2435\n",
      "Iteration 116/140: Train Loss = 0.1982, Valid Loss = 0.2433\n",
      "Iteration 117/140: Train Loss = 0.1979, Valid Loss = 0.2433\n",
      "Iteration 118/140: Train Loss = 0.1977, Valid Loss = 0.2434\n",
      "Iteration 119/140: Train Loss = 0.1973, Valid Loss = 0.2432\n",
      "Iteration 120/140: Train Loss = 0.1971, Valid Loss = 0.2431\n",
      "Iteration 121/140: Train Loss = 0.1968, Valid Loss = 0.2430\n",
      "Iteration 122/140: Train Loss = 0.1965, Valid Loss = 0.2428\n",
      "Iteration 123/140: Train Loss = 0.1961, Valid Loss = 0.2427\n",
      "Iteration 124/140: Train Loss = 0.1958, Valid Loss = 0.2427\n",
      "Iteration 125/140: Train Loss = 0.1955, Valid Loss = 0.2425\n",
      "Iteration 126/140: Train Loss = 0.1953, Valid Loss = 0.2424\n",
      "Iteration 127/140: Train Loss = 0.1950, Valid Loss = 0.2423\n",
      "Iteration 128/140: Train Loss = 0.1946, Valid Loss = 0.2421\n",
      "Iteration 129/140: Train Loss = 0.1944, Valid Loss = 0.2419\n",
      "Iteration 130/140: Train Loss = 0.1941, Valid Loss = 0.2418\n",
      "Iteration 131/140: Train Loss = 0.1938, Valid Loss = 0.2416\n",
      "Iteration 132/140: Train Loss = 0.1935, Valid Loss = 0.2415\n",
      "Iteration 133/140: Train Loss = 0.1933, Valid Loss = 0.2414\n",
      "Iteration 134/140: Train Loss = 0.1930, Valid Loss = 0.2412\n",
      "Iteration 135/140: Train Loss = 0.1927, Valid Loss = 0.2412\n",
      "Iteration 136/140: Train Loss = 0.1925, Valid Loss = 0.2411\n",
      "Iteration 137/140: Train Loss = 0.1923, Valid Loss = 0.2411\n",
      "Iteration 138/140: Train Loss = 0.1921, Valid Loss = 0.2410\n",
      "Iteration 139/140: Train Loss = 0.1918, Valid Loss = 0.2408\n",
      "Iteration 140/140: Train Loss = 0.1915, Valid Loss = 0.2407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:51:08,520] Trial 27 finished with value: 0.9657550072635349 and parameters: {'max_depth': 7, 'min_samples_leaf': 11, 'n_estimators': 140, 'learning_rate': 0.3219266375247475, 'subsample': 0.7623886541997188}. Best is trial 27 with value: 0.9657550072635349.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/140: Train Loss = 0.6643, Valid Loss = 0.6656\n",
      "Iteration 2/140: Train Loss = 0.6373, Valid Loss = 0.6396\n",
      "Iteration 3/140: Train Loss = 0.6126, Valid Loss = 0.6162\n",
      "Iteration 4/140: Train Loss = 0.5901, Valid Loss = 0.5945\n",
      "Iteration 5/140: Train Loss = 0.5692, Valid Loss = 0.5747\n",
      "Iteration 6/140: Train Loss = 0.5499, Valid Loss = 0.5564\n",
      "Iteration 7/140: Train Loss = 0.5316, Valid Loss = 0.5390\n",
      "Iteration 8/140: Train Loss = 0.5147, Valid Loss = 0.5232\n",
      "Iteration 9/140: Train Loss = 0.4995, Valid Loss = 0.5089\n",
      "Iteration 10/140: Train Loss = 0.4849, Valid Loss = 0.4952\n",
      "Iteration 11/140: Train Loss = 0.4716, Valid Loss = 0.4826\n",
      "Iteration 12/140: Train Loss = 0.4589, Valid Loss = 0.4708\n",
      "Iteration 13/140: Train Loss = 0.4470, Valid Loss = 0.4598\n",
      "Iteration 14/140: Train Loss = 0.4361, Valid Loss = 0.4493\n",
      "Iteration 15/140: Train Loss = 0.4258, Valid Loss = 0.4398\n",
      "Iteration 16/140: Train Loss = 0.4163, Valid Loss = 0.4313\n",
      "Iteration 17/140: Train Loss = 0.4072, Valid Loss = 0.4232\n",
      "Iteration 18/140: Train Loss = 0.3987, Valid Loss = 0.4154\n",
      "Iteration 19/140: Train Loss = 0.3906, Valid Loss = 0.4079\n",
      "Iteration 20/140: Train Loss = 0.3830, Valid Loss = 0.4012\n",
      "Iteration 21/140: Train Loss = 0.3760, Valid Loss = 0.3950\n",
      "Iteration 22/140: Train Loss = 0.3693, Valid Loss = 0.3891\n",
      "Iteration 23/140: Train Loss = 0.3630, Valid Loss = 0.3835\n",
      "Iteration 24/140: Train Loss = 0.3569, Valid Loss = 0.3782\n",
      "Iteration 25/140: Train Loss = 0.3511, Valid Loss = 0.3731\n",
      "Iteration 26/140: Train Loss = 0.3455, Valid Loss = 0.3681\n",
      "Iteration 27/140: Train Loss = 0.3403, Valid Loss = 0.3635\n",
      "Iteration 28/140: Train Loss = 0.3354, Valid Loss = 0.3595\n",
      "Iteration 29/140: Train Loss = 0.3307, Valid Loss = 0.3554\n",
      "Iteration 30/140: Train Loss = 0.3263, Valid Loss = 0.3516\n",
      "Iteration 31/140: Train Loss = 0.3221, Valid Loss = 0.3480\n",
      "Iteration 32/140: Train Loss = 0.3180, Valid Loss = 0.3446\n",
      "Iteration 33/140: Train Loss = 0.3141, Valid Loss = 0.3412\n",
      "Iteration 34/140: Train Loss = 0.3103, Valid Loss = 0.3380\n",
      "Iteration 35/140: Train Loss = 0.3067, Valid Loss = 0.3349\n",
      "Iteration 36/140: Train Loss = 0.3034, Valid Loss = 0.3318\n",
      "Iteration 37/140: Train Loss = 0.3001, Valid Loss = 0.3292\n",
      "Iteration 38/140: Train Loss = 0.2970, Valid Loss = 0.3267\n",
      "Iteration 39/140: Train Loss = 0.2939, Valid Loss = 0.3243\n",
      "Iteration 40/140: Train Loss = 0.2910, Valid Loss = 0.3218\n",
      "Iteration 41/140: Train Loss = 0.2883, Valid Loss = 0.3195\n",
      "Iteration 42/140: Train Loss = 0.2857, Valid Loss = 0.3174\n",
      "Iteration 43/140: Train Loss = 0.2831, Valid Loss = 0.3154\n",
      "Iteration 44/140: Train Loss = 0.2805, Valid Loss = 0.3132\n",
      "Iteration 45/140: Train Loss = 0.2780, Valid Loss = 0.3113\n",
      "Iteration 46/140: Train Loss = 0.2757, Valid Loss = 0.3095\n",
      "Iteration 47/140: Train Loss = 0.2734, Valid Loss = 0.3075\n",
      "Iteration 48/140: Train Loss = 0.2712, Valid Loss = 0.3056\n",
      "Iteration 49/140: Train Loss = 0.2691, Valid Loss = 0.3040\n",
      "Iteration 50/140: Train Loss = 0.2671, Valid Loss = 0.3025\n",
      "Iteration 51/140: Train Loss = 0.2651, Valid Loss = 0.3008\n",
      "Iteration 52/140: Train Loss = 0.2632, Valid Loss = 0.2993\n",
      "Iteration 53/140: Train Loss = 0.2614, Valid Loss = 0.2980\n",
      "Iteration 54/140: Train Loss = 0.2596, Valid Loss = 0.2965\n",
      "Iteration 55/140: Train Loss = 0.2579, Valid Loss = 0.2952\n",
      "Iteration 56/140: Train Loss = 0.2561, Valid Loss = 0.2938\n",
      "Iteration 57/140: Train Loss = 0.2544, Valid Loss = 0.2924\n",
      "Iteration 58/140: Train Loss = 0.2527, Valid Loss = 0.2912\n",
      "Iteration 59/140: Train Loss = 0.2512, Valid Loss = 0.2902\n",
      "Iteration 60/140: Train Loss = 0.2496, Valid Loss = 0.2891\n",
      "Iteration 61/140: Train Loss = 0.2481, Valid Loss = 0.2881\n",
      "Iteration 62/140: Train Loss = 0.2466, Valid Loss = 0.2869\n",
      "Iteration 63/140: Train Loss = 0.2452, Valid Loss = 0.2859\n",
      "Iteration 64/140: Train Loss = 0.2440, Valid Loss = 0.2849\n",
      "Iteration 65/140: Train Loss = 0.2427, Valid Loss = 0.2839\n",
      "Iteration 66/140: Train Loss = 0.2413, Valid Loss = 0.2829\n",
      "Iteration 67/140: Train Loss = 0.2399, Valid Loss = 0.2821\n",
      "Iteration 68/140: Train Loss = 0.2387, Valid Loss = 0.2813\n",
      "Iteration 69/140: Train Loss = 0.2376, Valid Loss = 0.2805\n",
      "Iteration 70/140: Train Loss = 0.2364, Valid Loss = 0.2797\n",
      "Iteration 71/140: Train Loss = 0.2352, Valid Loss = 0.2788\n",
      "Iteration 72/140: Train Loss = 0.2341, Valid Loss = 0.2781\n",
      "Iteration 73/140: Train Loss = 0.2331, Valid Loss = 0.2773\n",
      "Iteration 74/140: Train Loss = 0.2320, Valid Loss = 0.2767\n",
      "Iteration 75/140: Train Loss = 0.2309, Valid Loss = 0.2760\n",
      "Iteration 76/140: Train Loss = 0.2299, Valid Loss = 0.2752\n",
      "Iteration 77/140: Train Loss = 0.2288, Valid Loss = 0.2747\n",
      "Iteration 78/140: Train Loss = 0.2278, Valid Loss = 0.2739\n",
      "Iteration 79/140: Train Loss = 0.2269, Valid Loss = 0.2734\n",
      "Iteration 80/140: Train Loss = 0.2260, Valid Loss = 0.2728\n",
      "Iteration 81/140: Train Loss = 0.2250, Valid Loss = 0.2721\n",
      "Iteration 82/140: Train Loss = 0.2241, Valid Loss = 0.2714\n",
      "Iteration 83/140: Train Loss = 0.2231, Valid Loss = 0.2707\n",
      "Iteration 84/140: Train Loss = 0.2223, Valid Loss = 0.2702\n",
      "Iteration 85/140: Train Loss = 0.2214, Valid Loss = 0.2698\n",
      "Iteration 86/140: Train Loss = 0.2205, Valid Loss = 0.2693\n",
      "Iteration 87/140: Train Loss = 0.2197, Valid Loss = 0.2688\n",
      "Iteration 88/140: Train Loss = 0.2188, Valid Loss = 0.2681\n",
      "Iteration 89/140: Train Loss = 0.2179, Valid Loss = 0.2675\n",
      "Iteration 90/140: Train Loss = 0.2171, Valid Loss = 0.2669\n",
      "Iteration 91/140: Train Loss = 0.2164, Valid Loss = 0.2665\n",
      "Iteration 92/140: Train Loss = 0.2156, Valid Loss = 0.2659\n",
      "Iteration 93/140: Train Loss = 0.2149, Valid Loss = 0.2653\n",
      "Iteration 94/140: Train Loss = 0.2142, Valid Loss = 0.2648\n",
      "Iteration 95/140: Train Loss = 0.2136, Valid Loss = 0.2643\n",
      "Iteration 96/140: Train Loss = 0.2129, Valid Loss = 0.2640\n",
      "Iteration 97/140: Train Loss = 0.2122, Valid Loss = 0.2634\n",
      "Iteration 98/140: Train Loss = 0.2115, Valid Loss = 0.2629\n",
      "Iteration 99/140: Train Loss = 0.2109, Valid Loss = 0.2624\n",
      "Iteration 100/140: Train Loss = 0.2103, Valid Loss = 0.2620\n",
      "Iteration 101/140: Train Loss = 0.2095, Valid Loss = 0.2615\n",
      "Iteration 102/140: Train Loss = 0.2089, Valid Loss = 0.2613\n",
      "Iteration 103/140: Train Loss = 0.2083, Valid Loss = 0.2609\n",
      "Iteration 104/140: Train Loss = 0.2078, Valid Loss = 0.2605\n",
      "Iteration 105/140: Train Loss = 0.2071, Valid Loss = 0.2604\n",
      "Iteration 106/140: Train Loss = 0.2065, Valid Loss = 0.2598\n",
      "Iteration 107/140: Train Loss = 0.2059, Valid Loss = 0.2595\n",
      "Iteration 108/140: Train Loss = 0.2053, Valid Loss = 0.2591\n",
      "Iteration 109/140: Train Loss = 0.2047, Valid Loss = 0.2589\n",
      "Iteration 110/140: Train Loss = 0.2042, Valid Loss = 0.2586\n",
      "Iteration 111/140: Train Loss = 0.2037, Valid Loss = 0.2582\n",
      "Iteration 112/140: Train Loss = 0.2032, Valid Loss = 0.2578\n",
      "Iteration 113/140: Train Loss = 0.2026, Valid Loss = 0.2575\n",
      "Iteration 114/140: Train Loss = 0.2021, Valid Loss = 0.2574\n",
      "Iteration 115/140: Train Loss = 0.2015, Valid Loss = 0.2574\n",
      "Iteration 116/140: Train Loss = 0.2010, Valid Loss = 0.2570\n",
      "Iteration 117/140: Train Loss = 0.2004, Valid Loss = 0.2565\n",
      "Iteration 118/140: Train Loss = 0.1999, Valid Loss = 0.2563\n",
      "Iteration 119/140: Train Loss = 0.1994, Valid Loss = 0.2561\n",
      "Iteration 120/140: Train Loss = 0.1989, Valid Loss = 0.2559\n",
      "Iteration 121/140: Train Loss = 0.1984, Valid Loss = 0.2557\n",
      "Iteration 122/140: Train Loss = 0.1979, Valid Loss = 0.2553\n",
      "Iteration 123/140: Train Loss = 0.1975, Valid Loss = 0.2549\n",
      "Iteration 124/140: Train Loss = 0.1970, Valid Loss = 0.2547\n",
      "Iteration 125/140: Train Loss = 0.1965, Valid Loss = 0.2545\n",
      "Iteration 126/140: Train Loss = 0.1962, Valid Loss = 0.2543\n",
      "Iteration 127/140: Train Loss = 0.1957, Valid Loss = 0.2539\n",
      "Iteration 128/140: Train Loss = 0.1953, Valid Loss = 0.2538\n",
      "Iteration 129/140: Train Loss = 0.1949, Valid Loss = 0.2536\n",
      "Iteration 130/140: Train Loss = 0.1943, Valid Loss = 0.2534\n",
      "Iteration 131/140: Train Loss = 0.1939, Valid Loss = 0.2534\n",
      "Iteration 132/140: Train Loss = 0.1935, Valid Loss = 0.2531\n",
      "Iteration 133/140: Train Loss = 0.1931, Valid Loss = 0.2530\n",
      "Iteration 134/140: Train Loss = 0.1927, Valid Loss = 0.2528\n",
      "Iteration 135/140: Train Loss = 0.1923, Valid Loss = 0.2524\n",
      "Iteration 136/140: Train Loss = 0.1917, Valid Loss = 0.2521\n",
      "Iteration 137/140: Train Loss = 0.1913, Valid Loss = 0.2519\n",
      "Iteration 138/140: Train Loss = 0.1909, Valid Loss = 0.2519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:51:17,617] Trial 28 finished with value: 0.9648803327880916 and parameters: {'max_depth': 10, 'min_samples_leaf': 7, 'n_estimators': 140, 'learning_rate': 0.16524303464831572, 'subsample': 0.5812322631414446}. Best is trial 27 with value: 0.9657550072635349.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 139/140: Train Loss = 0.1904, Valid Loss = 0.2519\n",
      "Iteration 140/140: Train Loss = 0.1901, Valid Loss = 0.2518\n",
      "Iteration 1/145: Train Loss = 0.6487, Valid Loss = 0.6495\n",
      "Iteration 2/145: Train Loss = 0.6105, Valid Loss = 0.6120\n",
      "Iteration 3/145: Train Loss = 0.5780, Valid Loss = 0.5801\n",
      "Iteration 4/145: Train Loss = 0.5488, Valid Loss = 0.5516\n",
      "Iteration 5/145: Train Loss = 0.5238, Valid Loss = 0.5270\n",
      "Iteration 6/145: Train Loss = 0.5014, Valid Loss = 0.5055\n",
      "Iteration 7/145: Train Loss = 0.4814, Valid Loss = 0.4865\n",
      "Iteration 8/145: Train Loss = 0.4637, Valid Loss = 0.4690\n",
      "Iteration 9/145: Train Loss = 0.4482, Valid Loss = 0.4538\n",
      "Iteration 10/145: Train Loss = 0.4343, Valid Loss = 0.4408\n",
      "Iteration 11/145: Train Loss = 0.4218, Valid Loss = 0.4285\n",
      "Iteration 12/145: Train Loss = 0.4106, Valid Loss = 0.4181\n",
      "Iteration 13/145: Train Loss = 0.4005, Valid Loss = 0.4084\n",
      "Iteration 14/145: Train Loss = 0.3909, Valid Loss = 0.3993\n",
      "Iteration 15/145: Train Loss = 0.3823, Valid Loss = 0.3910\n",
      "Iteration 16/145: Train Loss = 0.3744, Valid Loss = 0.3835\n",
      "Iteration 17/145: Train Loss = 0.3676, Valid Loss = 0.3768\n",
      "Iteration 18/145: Train Loss = 0.3609, Valid Loss = 0.3704\n",
      "Iteration 19/145: Train Loss = 0.3545, Valid Loss = 0.3645\n",
      "Iteration 20/145: Train Loss = 0.3490, Valid Loss = 0.3592\n",
      "Iteration 21/145: Train Loss = 0.3439, Valid Loss = 0.3541\n",
      "Iteration 22/145: Train Loss = 0.3389, Valid Loss = 0.3493\n",
      "Iteration 23/145: Train Loss = 0.3344, Valid Loss = 0.3447\n",
      "Iteration 24/145: Train Loss = 0.3301, Valid Loss = 0.3405\n",
      "Iteration 25/145: Train Loss = 0.3258, Valid Loss = 0.3366\n",
      "Iteration 26/145: Train Loss = 0.3221, Valid Loss = 0.3328\n",
      "Iteration 27/145: Train Loss = 0.3185, Valid Loss = 0.3296\n",
      "Iteration 28/145: Train Loss = 0.3153, Valid Loss = 0.3263\n",
      "Iteration 29/145: Train Loss = 0.3121, Valid Loss = 0.3236\n",
      "Iteration 30/145: Train Loss = 0.3089, Valid Loss = 0.3205\n",
      "Iteration 31/145: Train Loss = 0.3060, Valid Loss = 0.3176\n",
      "Iteration 32/145: Train Loss = 0.3033, Valid Loss = 0.3150\n",
      "Iteration 33/145: Train Loss = 0.3006, Valid Loss = 0.3125\n",
      "Iteration 34/145: Train Loss = 0.2984, Valid Loss = 0.3108\n",
      "Iteration 35/145: Train Loss = 0.2960, Valid Loss = 0.3085\n",
      "Iteration 36/145: Train Loss = 0.2939, Valid Loss = 0.3068\n",
      "Iteration 37/145: Train Loss = 0.2919, Valid Loss = 0.3049\n",
      "Iteration 38/145: Train Loss = 0.2900, Valid Loss = 0.3030\n",
      "Iteration 39/145: Train Loss = 0.2882, Valid Loss = 0.3013\n",
      "Iteration 40/145: Train Loss = 0.2862, Valid Loss = 0.2993\n",
      "Iteration 41/145: Train Loss = 0.2844, Valid Loss = 0.2975\n",
      "Iteration 42/145: Train Loss = 0.2827, Valid Loss = 0.2959\n",
      "Iteration 43/145: Train Loss = 0.2812, Valid Loss = 0.2948\n",
      "Iteration 44/145: Train Loss = 0.2796, Valid Loss = 0.2933\n",
      "Iteration 45/145: Train Loss = 0.2780, Valid Loss = 0.2921\n",
      "Iteration 46/145: Train Loss = 0.2765, Valid Loss = 0.2905\n",
      "Iteration 47/145: Train Loss = 0.2748, Valid Loss = 0.2888\n",
      "Iteration 48/145: Train Loss = 0.2733, Valid Loss = 0.2875\n",
      "Iteration 49/145: Train Loss = 0.2721, Valid Loss = 0.2863\n",
      "Iteration 50/145: Train Loss = 0.2708, Valid Loss = 0.2852\n",
      "Iteration 51/145: Train Loss = 0.2696, Valid Loss = 0.2842\n",
      "Iteration 52/145: Train Loss = 0.2684, Valid Loss = 0.2833\n",
      "Iteration 53/145: Train Loss = 0.2673, Valid Loss = 0.2822\n",
      "Iteration 54/145: Train Loss = 0.2663, Valid Loss = 0.2813\n",
      "Iteration 55/145: Train Loss = 0.2650, Valid Loss = 0.2805\n",
      "Iteration 56/145: Train Loss = 0.2639, Valid Loss = 0.2797\n",
      "Iteration 57/145: Train Loss = 0.2628, Valid Loss = 0.2785\n",
      "Iteration 58/145: Train Loss = 0.2617, Valid Loss = 0.2775\n",
      "Iteration 59/145: Train Loss = 0.2607, Valid Loss = 0.2765\n",
      "Iteration 60/145: Train Loss = 0.2598, Valid Loss = 0.2757\n",
      "Iteration 61/145: Train Loss = 0.2588, Valid Loss = 0.2749\n",
      "Iteration 62/145: Train Loss = 0.2580, Valid Loss = 0.2743\n",
      "Iteration 63/145: Train Loss = 0.2571, Valid Loss = 0.2735\n",
      "Iteration 64/145: Train Loss = 0.2562, Valid Loss = 0.2729\n",
      "Iteration 65/145: Train Loss = 0.2554, Valid Loss = 0.2723\n",
      "Iteration 66/145: Train Loss = 0.2544, Valid Loss = 0.2713\n",
      "Iteration 67/145: Train Loss = 0.2536, Valid Loss = 0.2706\n",
      "Iteration 68/145: Train Loss = 0.2528, Valid Loss = 0.2699\n",
      "Iteration 69/145: Train Loss = 0.2518, Valid Loss = 0.2690\n",
      "Iteration 70/145: Train Loss = 0.2510, Valid Loss = 0.2686\n",
      "Iteration 71/145: Train Loss = 0.2504, Valid Loss = 0.2681\n",
      "Iteration 72/145: Train Loss = 0.2497, Valid Loss = 0.2675\n",
      "Iteration 73/145: Train Loss = 0.2490, Valid Loss = 0.2668\n",
      "Iteration 74/145: Train Loss = 0.2483, Valid Loss = 0.2662\n",
      "Iteration 75/145: Train Loss = 0.2477, Valid Loss = 0.2659\n",
      "Iteration 76/145: Train Loss = 0.2471, Valid Loss = 0.2654\n",
      "Iteration 77/145: Train Loss = 0.2465, Valid Loss = 0.2650\n",
      "Iteration 78/145: Train Loss = 0.2459, Valid Loss = 0.2646\n",
      "Iteration 79/145: Train Loss = 0.2452, Valid Loss = 0.2641\n",
      "Iteration 80/145: Train Loss = 0.2446, Valid Loss = 0.2637\n",
      "Iteration 81/145: Train Loss = 0.2441, Valid Loss = 0.2633\n",
      "Iteration 82/145: Train Loss = 0.2436, Valid Loss = 0.2630\n",
      "Iteration 83/145: Train Loss = 0.2431, Valid Loss = 0.2626\n",
      "Iteration 84/145: Train Loss = 0.2425, Valid Loss = 0.2622\n",
      "Iteration 85/145: Train Loss = 0.2419, Valid Loss = 0.2617\n",
      "Iteration 86/145: Train Loss = 0.2415, Valid Loss = 0.2613\n",
      "Iteration 87/145: Train Loss = 0.2409, Valid Loss = 0.2609\n",
      "Iteration 88/145: Train Loss = 0.2404, Valid Loss = 0.2603\n",
      "Iteration 89/145: Train Loss = 0.2401, Valid Loss = 0.2601\n",
      "Iteration 90/145: Train Loss = 0.2396, Valid Loss = 0.2595\n",
      "Iteration 91/145: Train Loss = 0.2392, Valid Loss = 0.2591\n",
      "Iteration 92/145: Train Loss = 0.2387, Valid Loss = 0.2585\n",
      "Iteration 93/145: Train Loss = 0.2381, Valid Loss = 0.2582\n",
      "Iteration 94/145: Train Loss = 0.2376, Valid Loss = 0.2579\n",
      "Iteration 95/145: Train Loss = 0.2371, Valid Loss = 0.2575\n",
      "Iteration 96/145: Train Loss = 0.2367, Valid Loss = 0.2572\n",
      "Iteration 97/145: Train Loss = 0.2362, Valid Loss = 0.2568\n",
      "Iteration 98/145: Train Loss = 0.2359, Valid Loss = 0.2566\n",
      "Iteration 99/145: Train Loss = 0.2355, Valid Loss = 0.2564\n",
      "Iteration 100/145: Train Loss = 0.2352, Valid Loss = 0.2563\n",
      "Iteration 101/145: Train Loss = 0.2348, Valid Loss = 0.2560\n",
      "Iteration 102/145: Train Loss = 0.2344, Valid Loss = 0.2558\n",
      "Iteration 103/145: Train Loss = 0.2339, Valid Loss = 0.2555\n",
      "Iteration 104/145: Train Loss = 0.2336, Valid Loss = 0.2553\n",
      "Iteration 105/145: Train Loss = 0.2332, Valid Loss = 0.2549\n",
      "Iteration 106/145: Train Loss = 0.2328, Valid Loss = 0.2545\n",
      "Iteration 107/145: Train Loss = 0.2324, Valid Loss = 0.2542\n",
      "Iteration 108/145: Train Loss = 0.2320, Valid Loss = 0.2538\n",
      "Iteration 109/145: Train Loss = 0.2316, Valid Loss = 0.2536\n",
      "Iteration 110/145: Train Loss = 0.2313, Valid Loss = 0.2532\n",
      "Iteration 111/145: Train Loss = 0.2310, Valid Loss = 0.2532\n",
      "Iteration 112/145: Train Loss = 0.2307, Valid Loss = 0.2532\n",
      "Iteration 113/145: Train Loss = 0.2305, Valid Loss = 0.2530\n",
      "Iteration 114/145: Train Loss = 0.2301, Valid Loss = 0.2527\n",
      "Iteration 115/145: Train Loss = 0.2297, Valid Loss = 0.2524\n",
      "Iteration 116/145: Train Loss = 0.2293, Valid Loss = 0.2522\n",
      "Iteration 117/145: Train Loss = 0.2290, Valid Loss = 0.2519\n",
      "Iteration 118/145: Train Loss = 0.2285, Valid Loss = 0.2516\n",
      "Iteration 119/145: Train Loss = 0.2283, Valid Loss = 0.2513\n",
      "Iteration 120/145: Train Loss = 0.2280, Valid Loss = 0.2509\n",
      "Iteration 121/145: Train Loss = 0.2276, Valid Loss = 0.2507\n",
      "Iteration 122/145: Train Loss = 0.2272, Valid Loss = 0.2504\n",
      "Iteration 123/145: Train Loss = 0.2269, Valid Loss = 0.2501\n",
      "Iteration 124/145: Train Loss = 0.2266, Valid Loss = 0.2501\n",
      "Iteration 125/145: Train Loss = 0.2263, Valid Loss = 0.2498\n",
      "Iteration 126/145: Train Loss = 0.2261, Valid Loss = 0.2498\n",
      "Iteration 127/145: Train Loss = 0.2258, Valid Loss = 0.2497\n",
      "Iteration 128/145: Train Loss = 0.2256, Valid Loss = 0.2495\n",
      "Iteration 129/145: Train Loss = 0.2253, Valid Loss = 0.2495\n",
      "Iteration 130/145: Train Loss = 0.2250, Valid Loss = 0.2493\n",
      "Iteration 131/145: Train Loss = 0.2247, Valid Loss = 0.2492\n",
      "Iteration 132/145: Train Loss = 0.2245, Valid Loss = 0.2489\n",
      "Iteration 133/145: Train Loss = 0.2242, Valid Loss = 0.2488\n",
      "Iteration 134/145: Train Loss = 0.2239, Valid Loss = 0.2487\n",
      "Iteration 135/145: Train Loss = 0.2237, Valid Loss = 0.2486\n",
      "Iteration 136/145: Train Loss = 0.2234, Valid Loss = 0.2485\n",
      "Iteration 137/145: Train Loss = 0.2231, Valid Loss = 0.2483\n",
      "Iteration 138/145: Train Loss = 0.2227, Valid Loss = 0.2479\n",
      "Iteration 139/145: Train Loss = 0.2225, Valid Loss = 0.2477\n",
      "Iteration 140/145: Train Loss = 0.2223, Valid Loss = 0.2476\n",
      "Iteration 141/145: Train Loss = 0.2220, Valid Loss = 0.2473\n",
      "Iteration 142/145: Train Loss = 0.2218, Valid Loss = 0.2471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:51:23,378] Trial 29 finished with value: 0.9643770327411934 and parameters: {'max_depth': 5, 'min_samples_leaf': 19, 'n_estimators': 145, 'learning_rate': 0.2846085649977731, 'subsample': 0.6686720105160319}. Best is trial 27 with value: 0.9657550072635349.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 143/145: Train Loss = 0.2216, Valid Loss = 0.2470\n",
      "Iteration 144/145: Train Loss = 0.2213, Valid Loss = 0.2467\n",
      "Iteration 145/145: Train Loss = 0.2211, Valid Loss = 0.2466\n",
      "Iteration 1/125: Train Loss = 0.6885, Valid Loss = 0.6886\n",
      "Iteration 2/125: Train Loss = 0.6840, Valid Loss = 0.6841\n",
      "Iteration 3/125: Train Loss = 0.6795, Valid Loss = 0.6797\n",
      "Iteration 4/125: Train Loss = 0.6750, Valid Loss = 0.6754\n",
      "Iteration 5/125: Train Loss = 0.6707, Valid Loss = 0.6712\n",
      "Iteration 6/125: Train Loss = 0.6664, Valid Loss = 0.6670\n",
      "Iteration 7/125: Train Loss = 0.6622, Valid Loss = 0.6629\n",
      "Iteration 8/125: Train Loss = 0.6580, Valid Loss = 0.6588\n",
      "Iteration 9/125: Train Loss = 0.6540, Valid Loss = 0.6548\n",
      "Iteration 10/125: Train Loss = 0.6499, Valid Loss = 0.6509\n",
      "Iteration 11/125: Train Loss = 0.6459, Valid Loss = 0.6470\n",
      "Iteration 12/125: Train Loss = 0.6420, Valid Loss = 0.6432\n",
      "Iteration 13/125: Train Loss = 0.6382, Valid Loss = 0.6395\n",
      "Iteration 14/125: Train Loss = 0.6344, Valid Loss = 0.6357\n",
      "Iteration 15/125: Train Loss = 0.6306, Valid Loss = 0.6321\n",
      "Iteration 16/125: Train Loss = 0.6269, Valid Loss = 0.6285\n",
      "Iteration 17/125: Train Loss = 0.6233, Valid Loss = 0.6249\n",
      "Iteration 18/125: Train Loss = 0.6197, Valid Loss = 0.6214\n",
      "Iteration 19/125: Train Loss = 0.6162, Valid Loss = 0.6181\n",
      "Iteration 20/125: Train Loss = 0.6128, Valid Loss = 0.6146\n",
      "Iteration 21/125: Train Loss = 0.6093, Valid Loss = 0.6113\n",
      "Iteration 22/125: Train Loss = 0.6060, Valid Loss = 0.6081\n",
      "Iteration 23/125: Train Loss = 0.6027, Valid Loss = 0.6048\n",
      "Iteration 24/125: Train Loss = 0.5994, Valid Loss = 0.6017\n",
      "Iteration 25/125: Train Loss = 0.5962, Valid Loss = 0.5985\n",
      "Iteration 26/125: Train Loss = 0.5930, Valid Loss = 0.5954\n",
      "Iteration 27/125: Train Loss = 0.5898, Valid Loss = 0.5923\n",
      "Iteration 28/125: Train Loss = 0.5867, Valid Loss = 0.5893\n",
      "Iteration 29/125: Train Loss = 0.5837, Valid Loss = 0.5864\n",
      "Iteration 30/125: Train Loss = 0.5807, Valid Loss = 0.5835\n",
      "Iteration 31/125: Train Loss = 0.5777, Valid Loss = 0.5806\n",
      "Iteration 32/125: Train Loss = 0.5747, Valid Loss = 0.5778\n",
      "Iteration 33/125: Train Loss = 0.5718, Valid Loss = 0.5749\n",
      "Iteration 34/125: Train Loss = 0.5690, Valid Loss = 0.5722\n",
      "Iteration 35/125: Train Loss = 0.5663, Valid Loss = 0.5695\n",
      "Iteration 36/125: Train Loss = 0.5635, Valid Loss = 0.5668\n",
      "Iteration 37/125: Train Loss = 0.5608, Valid Loss = 0.5642\n",
      "Iteration 38/125: Train Loss = 0.5582, Valid Loss = 0.5616\n",
      "Iteration 39/125: Train Loss = 0.5556, Valid Loss = 0.5590\n",
      "Iteration 40/125: Train Loss = 0.5529, Valid Loss = 0.5565\n",
      "Iteration 41/125: Train Loss = 0.5504, Valid Loss = 0.5540\n",
      "Iteration 42/125: Train Loss = 0.5478, Valid Loss = 0.5515\n",
      "Iteration 43/125: Train Loss = 0.5453, Valid Loss = 0.5491\n",
      "Iteration 44/125: Train Loss = 0.5429, Valid Loss = 0.5467\n",
      "Iteration 45/125: Train Loss = 0.5404, Valid Loss = 0.5442\n",
      "Iteration 46/125: Train Loss = 0.5380, Valid Loss = 0.5419\n",
      "Iteration 47/125: Train Loss = 0.5357, Valid Loss = 0.5396\n",
      "Iteration 48/125: Train Loss = 0.5333, Valid Loss = 0.5373\n",
      "Iteration 49/125: Train Loss = 0.5311, Valid Loss = 0.5351\n",
      "Iteration 50/125: Train Loss = 0.5288, Valid Loss = 0.5329\n",
      "Iteration 51/125: Train Loss = 0.5266, Valid Loss = 0.5307\n",
      "Iteration 52/125: Train Loss = 0.5244, Valid Loss = 0.5286\n",
      "Iteration 53/125: Train Loss = 0.5222, Valid Loss = 0.5264\n",
      "Iteration 54/125: Train Loss = 0.5201, Valid Loss = 0.5244\n",
      "Iteration 55/125: Train Loss = 0.5180, Valid Loss = 0.5223\n",
      "Iteration 56/125: Train Loss = 0.5159, Valid Loss = 0.5203\n",
      "Iteration 57/125: Train Loss = 0.5139, Valid Loss = 0.5182\n",
      "Iteration 58/125: Train Loss = 0.5118, Valid Loss = 0.5162\n",
      "Iteration 59/125: Train Loss = 0.5099, Valid Loss = 0.5143\n",
      "Iteration 60/125: Train Loss = 0.5079, Valid Loss = 0.5123\n",
      "Iteration 61/125: Train Loss = 0.5059, Valid Loss = 0.5104\n",
      "Iteration 62/125: Train Loss = 0.5040, Valid Loss = 0.5086\n",
      "Iteration 63/125: Train Loss = 0.5021, Valid Loss = 0.5067\n",
      "Iteration 64/125: Train Loss = 0.5002, Valid Loss = 0.5049\n",
      "Iteration 65/125: Train Loss = 0.4984, Valid Loss = 0.5031\n",
      "Iteration 66/125: Train Loss = 0.4965, Valid Loss = 0.5013\n",
      "Iteration 67/125: Train Loss = 0.4947, Valid Loss = 0.4996\n",
      "Iteration 68/125: Train Loss = 0.4929, Valid Loss = 0.4978\n",
      "Iteration 69/125: Train Loss = 0.4911, Valid Loss = 0.4961\n",
      "Iteration 70/125: Train Loss = 0.4893, Valid Loss = 0.4943\n",
      "Iteration 71/125: Train Loss = 0.4876, Valid Loss = 0.4926\n",
      "Iteration 72/125: Train Loss = 0.4859, Valid Loss = 0.4910\n",
      "Iteration 73/125: Train Loss = 0.4842, Valid Loss = 0.4894\n",
      "Iteration 74/125: Train Loss = 0.4825, Valid Loss = 0.4877\n",
      "Iteration 75/125: Train Loss = 0.4809, Valid Loss = 0.4861\n",
      "Iteration 76/125: Train Loss = 0.4792, Valid Loss = 0.4845\n",
      "Iteration 77/125: Train Loss = 0.4776, Valid Loss = 0.4829\n",
      "Iteration 78/125: Train Loss = 0.4760, Valid Loss = 0.4814\n",
      "Iteration 79/125: Train Loss = 0.4744, Valid Loss = 0.4798\n",
      "Iteration 80/125: Train Loss = 0.4729, Valid Loss = 0.4783\n",
      "Iteration 81/125: Train Loss = 0.4714, Valid Loss = 0.4769\n",
      "Iteration 82/125: Train Loss = 0.4700, Valid Loss = 0.4755\n",
      "Iteration 83/125: Train Loss = 0.4684, Valid Loss = 0.4739\n",
      "Iteration 84/125: Train Loss = 0.4669, Valid Loss = 0.4724\n",
      "Iteration 85/125: Train Loss = 0.4654, Valid Loss = 0.4710\n",
      "Iteration 86/125: Train Loss = 0.4640, Valid Loss = 0.4696\n",
      "Iteration 87/125: Train Loss = 0.4626, Valid Loss = 0.4682\n",
      "Iteration 88/125: Train Loss = 0.4612, Valid Loss = 0.4669\n",
      "Iteration 89/125: Train Loss = 0.4598, Valid Loss = 0.4655\n",
      "Iteration 90/125: Train Loss = 0.4584, Valid Loss = 0.4642\n",
      "Iteration 91/125: Train Loss = 0.4571, Valid Loss = 0.4628\n",
      "Iteration 92/125: Train Loss = 0.4557, Valid Loss = 0.4615\n",
      "Iteration 93/125: Train Loss = 0.4544, Valid Loss = 0.4601\n",
      "Iteration 94/125: Train Loss = 0.4530, Valid Loss = 0.4589\n",
      "Iteration 95/125: Train Loss = 0.4517, Valid Loss = 0.4575\n",
      "Iteration 96/125: Train Loss = 0.4504, Valid Loss = 0.4562\n",
      "Iteration 97/125: Train Loss = 0.4491, Valid Loss = 0.4549\n",
      "Iteration 98/125: Train Loss = 0.4479, Valid Loss = 0.4537\n",
      "Iteration 99/125: Train Loss = 0.4466, Valid Loss = 0.4525\n",
      "Iteration 100/125: Train Loss = 0.4454, Valid Loss = 0.4513\n",
      "Iteration 101/125: Train Loss = 0.4442, Valid Loss = 0.4501\n",
      "Iteration 102/125: Train Loss = 0.4430, Valid Loss = 0.4490\n",
      "Iteration 103/125: Train Loss = 0.4418, Valid Loss = 0.4478\n",
      "Iteration 104/125: Train Loss = 0.4406, Valid Loss = 0.4467\n",
      "Iteration 105/125: Train Loss = 0.4394, Valid Loss = 0.4455\n",
      "Iteration 106/125: Train Loss = 0.4382, Valid Loss = 0.4443\n",
      "Iteration 107/125: Train Loss = 0.4371, Valid Loss = 0.4432\n",
      "Iteration 108/125: Train Loss = 0.4360, Valid Loss = 0.4421\n",
      "Iteration 109/125: Train Loss = 0.4348, Valid Loss = 0.4410\n",
      "Iteration 110/125: Train Loss = 0.4337, Valid Loss = 0.4399\n",
      "Iteration 111/125: Train Loss = 0.4326, Valid Loss = 0.4388\n",
      "Iteration 112/125: Train Loss = 0.4315, Valid Loss = 0.4378\n",
      "Iteration 113/125: Train Loss = 0.4305, Valid Loss = 0.4367\n",
      "Iteration 114/125: Train Loss = 0.4294, Valid Loss = 0.4357\n",
      "Iteration 115/125: Train Loss = 0.4284, Valid Loss = 0.4347\n",
      "Iteration 116/125: Train Loss = 0.4273, Valid Loss = 0.4336\n",
      "Iteration 117/125: Train Loss = 0.4263, Valid Loss = 0.4326\n",
      "Iteration 118/125: Train Loss = 0.4252, Valid Loss = 0.4316\n",
      "Iteration 119/125: Train Loss = 0.4242, Valid Loss = 0.4306\n",
      "Iteration 120/125: Train Loss = 0.4232, Valid Loss = 0.4296\n",
      "Iteration 121/125: Train Loss = 0.4222, Valid Loss = 0.4287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:51:27,283] Trial 30 finished with value: 0.9410799598885116 and parameters: {'max_depth': 3, 'min_samples_leaf': 16, 'n_estimators': 125, 'learning_rate': 0.03210410711854999, 'subsample': 0.748576283378865}. Best is trial 27 with value: 0.9657550072635349.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 122/125: Train Loss = 0.4213, Valid Loss = 0.4278\n",
      "Iteration 123/125: Train Loss = 0.4204, Valid Loss = 0.4269\n",
      "Iteration 124/125: Train Loss = 0.4194, Valid Loss = 0.4259\n",
      "Iteration 125/125: Train Loss = 0.4184, Valid Loss = 0.4250\n",
      "Iteration 1/70: Train Loss = 0.5942, Valid Loss = 0.5973\n",
      "Iteration 2/70: Train Loss = 0.5227, Valid Loss = 0.5282\n",
      "Iteration 3/70: Train Loss = 0.4690, Valid Loss = 0.4763\n",
      "Iteration 4/70: Train Loss = 0.4292, Valid Loss = 0.4387\n",
      "Iteration 5/70: Train Loss = 0.3978, Valid Loss = 0.4097\n",
      "Iteration 6/70: Train Loss = 0.3731, Valid Loss = 0.3872\n",
      "Iteration 7/70: Train Loss = 0.3542, Valid Loss = 0.3693\n",
      "Iteration 8/70: Train Loss = 0.3378, Valid Loss = 0.3550\n",
      "Iteration 9/70: Train Loss = 0.3241, Valid Loss = 0.3428\n",
      "Iteration 10/70: Train Loss = 0.3128, Valid Loss = 0.3319\n",
      "Iteration 11/70: Train Loss = 0.3021, Valid Loss = 0.3229\n",
      "Iteration 12/70: Train Loss = 0.2930, Valid Loss = 0.3152\n",
      "Iteration 13/70: Train Loss = 0.2852, Valid Loss = 0.3085\n",
      "Iteration 14/70: Train Loss = 0.2786, Valid Loss = 0.3027\n",
      "Iteration 15/70: Train Loss = 0.2727, Valid Loss = 0.2981\n",
      "Iteration 16/70: Train Loss = 0.2675, Valid Loss = 0.2936\n",
      "Iteration 17/70: Train Loss = 0.2631, Valid Loss = 0.2901\n",
      "Iteration 18/70: Train Loss = 0.2592, Valid Loss = 0.2866\n",
      "Iteration 19/70: Train Loss = 0.2555, Valid Loss = 0.2842\n",
      "Iteration 20/70: Train Loss = 0.2514, Valid Loss = 0.2807\n",
      "Iteration 21/70: Train Loss = 0.2478, Valid Loss = 0.2775\n",
      "Iteration 22/70: Train Loss = 0.2449, Valid Loss = 0.2750\n",
      "Iteration 23/70: Train Loss = 0.2418, Valid Loss = 0.2732\n",
      "Iteration 24/70: Train Loss = 0.2395, Valid Loss = 0.2717\n",
      "Iteration 25/70: Train Loss = 0.2372, Valid Loss = 0.2697\n",
      "Iteration 26/70: Train Loss = 0.2349, Valid Loss = 0.2680\n",
      "Iteration 27/70: Train Loss = 0.2328, Valid Loss = 0.2660\n",
      "Iteration 28/70: Train Loss = 0.2307, Valid Loss = 0.2647\n",
      "Iteration 29/70: Train Loss = 0.2285, Valid Loss = 0.2632\n",
      "Iteration 30/70: Train Loss = 0.2264, Valid Loss = 0.2618\n",
      "Iteration 31/70: Train Loss = 0.2251, Valid Loss = 0.2605\n",
      "Iteration 32/70: Train Loss = 0.2235, Valid Loss = 0.2595\n",
      "Iteration 33/70: Train Loss = 0.2219, Valid Loss = 0.2581\n",
      "Iteration 34/70: Train Loss = 0.2207, Valid Loss = 0.2575\n",
      "Iteration 35/70: Train Loss = 0.2192, Valid Loss = 0.2564\n",
      "Iteration 36/70: Train Loss = 0.2182, Valid Loss = 0.2561\n",
      "Iteration 37/70: Train Loss = 0.2169, Valid Loss = 0.2552\n",
      "Iteration 38/70: Train Loss = 0.2155, Valid Loss = 0.2549\n",
      "Iteration 39/70: Train Loss = 0.2144, Valid Loss = 0.2546\n",
      "Iteration 40/70: Train Loss = 0.2134, Valid Loss = 0.2537\n",
      "Iteration 41/70: Train Loss = 0.2122, Valid Loss = 0.2532\n",
      "Iteration 42/70: Train Loss = 0.2113, Valid Loss = 0.2527\n",
      "Iteration 43/70: Train Loss = 0.2102, Valid Loss = 0.2518\n",
      "Iteration 44/70: Train Loss = 0.2093, Valid Loss = 0.2513\n",
      "Iteration 45/70: Train Loss = 0.2080, Valid Loss = 0.2513\n",
      "Iteration 46/70: Train Loss = 0.2072, Valid Loss = 0.2509\n",
      "Iteration 47/70: Train Loss = 0.2065, Valid Loss = 0.2504\n",
      "Iteration 48/70: Train Loss = 0.2056, Valid Loss = 0.2498\n",
      "Iteration 49/70: Train Loss = 0.2049, Valid Loss = 0.2494\n",
      "Iteration 50/70: Train Loss = 0.2039, Valid Loss = 0.2488\n",
      "Iteration 51/70: Train Loss = 0.2032, Valid Loss = 0.2482\n",
      "Iteration 52/70: Train Loss = 0.2026, Valid Loss = 0.2482\n",
      "Iteration 53/70: Train Loss = 0.2017, Valid Loss = 0.2476\n",
      "Iteration 54/70: Train Loss = 0.2011, Valid Loss = 0.2476\n",
      "Iteration 55/70: Train Loss = 0.2004, Valid Loss = 0.2471\n",
      "Iteration 56/70: Train Loss = 0.1994, Valid Loss = 0.2466\n",
      "Iteration 57/70: Train Loss = 0.1990, Valid Loss = 0.2461\n",
      "Iteration 58/70: Train Loss = 0.1984, Valid Loss = 0.2457\n",
      "Iteration 59/70: Train Loss = 0.1979, Valid Loss = 0.2456\n",
      "Iteration 60/70: Train Loss = 0.1973, Valid Loss = 0.2457\n",
      "Iteration 61/70: Train Loss = 0.1965, Valid Loss = 0.2453\n",
      "Iteration 62/70: Train Loss = 0.1958, Valid Loss = 0.2450\n",
      "Iteration 63/70: Train Loss = 0.1950, Valid Loss = 0.2446\n",
      "Iteration 64/70: Train Loss = 0.1946, Valid Loss = 0.2445\n",
      "Iteration 65/70: Train Loss = 0.1942, Valid Loss = 0.2444\n",
      "Iteration 66/70: Train Loss = 0.1939, Valid Loss = 0.2445\n",
      "Iteration 67/70: Train Loss = 0.1935, Valid Loss = 0.2443\n",
      "Iteration 68/70: Train Loss = 0.1930, Valid Loss = 0.2442\n",
      "Iteration 69/70: Train Loss = 0.1922, Valid Loss = 0.2442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:51:31,246] Trial 31 finished with value: 0.9649947191623867 and parameters: {'max_depth': 7, 'min_samples_leaf': 11, 'n_estimators': 70, 'learning_rate': 0.6314369375439696, 'subsample': 0.8414901069343628}. Best is trial 27 with value: 0.9657550072635349.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70/70: Train Loss = 0.1918, Valid Loss = 0.2437\n",
      "Iteration 1/70: Train Loss = 0.6445, Valid Loss = 0.6462\n",
      "Iteration 2/70: Train Loss = 0.6027, Valid Loss = 0.6055\n",
      "Iteration 3/70: Train Loss = 0.5666, Valid Loss = 0.5708\n",
      "Iteration 4/70: Train Loss = 0.5350, Valid Loss = 0.5405\n",
      "Iteration 5/70: Train Loss = 0.5079, Valid Loss = 0.5145\n",
      "Iteration 6/70: Train Loss = 0.4842, Valid Loss = 0.4915\n",
      "Iteration 7/70: Train Loss = 0.4633, Valid Loss = 0.4714\n",
      "Iteration 8/70: Train Loss = 0.4446, Valid Loss = 0.4540\n",
      "Iteration 9/70: Train Loss = 0.4278, Valid Loss = 0.4380\n",
      "Iteration 10/70: Train Loss = 0.4128, Valid Loss = 0.4240\n",
      "Iteration 11/70: Train Loss = 0.3994, Valid Loss = 0.4113\n",
      "Iteration 12/70: Train Loss = 0.3875, Valid Loss = 0.4002\n",
      "Iteration 13/70: Train Loss = 0.3768, Valid Loss = 0.3902\n",
      "Iteration 14/70: Train Loss = 0.3670, Valid Loss = 0.3814\n",
      "Iteration 15/70: Train Loss = 0.3579, Valid Loss = 0.3738\n",
      "Iteration 16/70: Train Loss = 0.3497, Valid Loss = 0.3662\n",
      "Iteration 17/70: Train Loss = 0.3424, Valid Loss = 0.3591\n",
      "Iteration 18/70: Train Loss = 0.3353, Valid Loss = 0.3525\n",
      "Iteration 19/70: Train Loss = 0.3289, Valid Loss = 0.3468\n",
      "Iteration 20/70: Train Loss = 0.3231, Valid Loss = 0.3414\n",
      "Iteration 21/70: Train Loss = 0.3176, Valid Loss = 0.3361\n",
      "Iteration 22/70: Train Loss = 0.3127, Valid Loss = 0.3318\n",
      "Iteration 23/70: Train Loss = 0.3079, Valid Loss = 0.3275\n",
      "Iteration 24/70: Train Loss = 0.3033, Valid Loss = 0.3234\n",
      "Iteration 25/70: Train Loss = 0.2991, Valid Loss = 0.3196\n",
      "Iteration 26/70: Train Loss = 0.2953, Valid Loss = 0.3160\n",
      "Iteration 27/70: Train Loss = 0.2916, Valid Loss = 0.3126\n",
      "Iteration 28/70: Train Loss = 0.2882, Valid Loss = 0.3096\n",
      "Iteration 29/70: Train Loss = 0.2848, Valid Loss = 0.3065\n",
      "Iteration 30/70: Train Loss = 0.2818, Valid Loss = 0.3041\n",
      "Iteration 31/70: Train Loss = 0.2785, Valid Loss = 0.3017\n",
      "Iteration 32/70: Train Loss = 0.2756, Valid Loss = 0.2994\n",
      "Iteration 33/70: Train Loss = 0.2731, Valid Loss = 0.2972\n",
      "Iteration 34/70: Train Loss = 0.2704, Valid Loss = 0.2950\n",
      "Iteration 35/70: Train Loss = 0.2680, Valid Loss = 0.2931\n",
      "Iteration 36/70: Train Loss = 0.2656, Valid Loss = 0.2911\n",
      "Iteration 37/70: Train Loss = 0.2634, Valid Loss = 0.2892\n",
      "Iteration 38/70: Train Loss = 0.2613, Valid Loss = 0.2873\n",
      "Iteration 39/70: Train Loss = 0.2593, Valid Loss = 0.2856\n",
      "Iteration 40/70: Train Loss = 0.2574, Valid Loss = 0.2842\n",
      "Iteration 41/70: Train Loss = 0.2557, Valid Loss = 0.2827\n",
      "Iteration 42/70: Train Loss = 0.2538, Valid Loss = 0.2814\n",
      "Iteration 43/70: Train Loss = 0.2521, Valid Loss = 0.2799\n",
      "Iteration 44/70: Train Loss = 0.2504, Valid Loss = 0.2789\n",
      "Iteration 45/70: Train Loss = 0.2487, Valid Loss = 0.2776\n",
      "Iteration 46/70: Train Loss = 0.2470, Valid Loss = 0.2764\n",
      "Iteration 47/70: Train Loss = 0.2455, Valid Loss = 0.2752\n",
      "Iteration 48/70: Train Loss = 0.2442, Valid Loss = 0.2742\n",
      "Iteration 49/70: Train Loss = 0.2428, Valid Loss = 0.2732\n",
      "Iteration 50/70: Train Loss = 0.2415, Valid Loss = 0.2722\n",
      "Iteration 51/70: Train Loss = 0.2402, Valid Loss = 0.2712\n",
      "Iteration 52/70: Train Loss = 0.2391, Valid Loss = 0.2698\n",
      "Iteration 53/70: Train Loss = 0.2380, Valid Loss = 0.2689\n",
      "Iteration 54/70: Train Loss = 0.2368, Valid Loss = 0.2680\n",
      "Iteration 55/70: Train Loss = 0.2357, Valid Loss = 0.2674\n",
      "Iteration 56/70: Train Loss = 0.2346, Valid Loss = 0.2665\n",
      "Iteration 57/70: Train Loss = 0.2334, Valid Loss = 0.2658\n",
      "Iteration 58/70: Train Loss = 0.2323, Valid Loss = 0.2650\n",
      "Iteration 59/70: Train Loss = 0.2313, Valid Loss = 0.2643\n",
      "Iteration 60/70: Train Loss = 0.2304, Valid Loss = 0.2635\n",
      "Iteration 61/70: Train Loss = 0.2294, Valid Loss = 0.2628\n",
      "Iteration 62/70: Train Loss = 0.2285, Valid Loss = 0.2622\n",
      "Iteration 63/70: Train Loss = 0.2276, Valid Loss = 0.2617\n",
      "Iteration 64/70: Train Loss = 0.2267, Valid Loss = 0.2615\n",
      "Iteration 65/70: Train Loss = 0.2259, Valid Loss = 0.2609\n",
      "Iteration 66/70: Train Loss = 0.2250, Valid Loss = 0.2602\n",
      "Iteration 67/70: Train Loss = 0.2241, Valid Loss = 0.2597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:51:35,238] Trial 32 finished with value: 0.9641646553729186 and parameters: {'max_depth': 7, 'min_samples_leaf': 9, 'n_estimators': 70, 'learning_rate': 0.29264101851603114, 'subsample': 0.838570752994886}. Best is trial 27 with value: 0.9657550072635349.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 68/70: Train Loss = 0.2233, Valid Loss = 0.2590\n",
      "Iteration 69/70: Train Loss = 0.2226, Valid Loss = 0.2586\n",
      "Iteration 70/70: Train Loss = 0.2218, Valid Loss = 0.2580\n",
      "Iteration 1/65: Train Loss = 0.5883, Valid Loss = 0.5924\n",
      "Iteration 2/65: Train Loss = 0.5130, Valid Loss = 0.5194\n",
      "Iteration 3/65: Train Loss = 0.4583, Valid Loss = 0.4669\n",
      "Iteration 4/65: Train Loss = 0.4166, Valid Loss = 0.4280\n",
      "Iteration 5/65: Train Loss = 0.3854, Valid Loss = 0.3994\n",
      "Iteration 6/65: Train Loss = 0.3600, Valid Loss = 0.3761\n",
      "Iteration 7/65: Train Loss = 0.3399, Valid Loss = 0.3584\n",
      "Iteration 8/65: Train Loss = 0.3227, Valid Loss = 0.3438\n",
      "Iteration 9/65: Train Loss = 0.3087, Valid Loss = 0.3317\n",
      "Iteration 10/65: Train Loss = 0.2974, Valid Loss = 0.3221\n",
      "Iteration 11/65: Train Loss = 0.2880, Valid Loss = 0.3149\n",
      "Iteration 12/65: Train Loss = 0.2796, Valid Loss = 0.3074\n",
      "Iteration 13/65: Train Loss = 0.2718, Valid Loss = 0.3003\n",
      "Iteration 14/65: Train Loss = 0.2648, Valid Loss = 0.2946\n",
      "Iteration 15/65: Train Loss = 0.2587, Valid Loss = 0.2898\n",
      "Iteration 16/65: Train Loss = 0.2532, Valid Loss = 0.2854\n",
      "Iteration 17/65: Train Loss = 0.2486, Valid Loss = 0.2817\n",
      "Iteration 18/65: Train Loss = 0.2438, Valid Loss = 0.2786\n",
      "Iteration 19/65: Train Loss = 0.2394, Valid Loss = 0.2759\n",
      "Iteration 20/65: Train Loss = 0.2354, Valid Loss = 0.2740\n",
      "Iteration 21/65: Train Loss = 0.2321, Valid Loss = 0.2715\n",
      "Iteration 22/65: Train Loss = 0.2289, Valid Loss = 0.2687\n",
      "Iteration 23/65: Train Loss = 0.2259, Valid Loss = 0.2676\n",
      "Iteration 24/65: Train Loss = 0.2234, Valid Loss = 0.2664\n",
      "Iteration 25/65: Train Loss = 0.2205, Valid Loss = 0.2652\n",
      "Iteration 26/65: Train Loss = 0.2177, Valid Loss = 0.2636\n",
      "Iteration 27/65: Train Loss = 0.2157, Valid Loss = 0.2625\n",
      "Iteration 28/65: Train Loss = 0.2133, Valid Loss = 0.2605\n",
      "Iteration 29/65: Train Loss = 0.2114, Valid Loss = 0.2589\n",
      "Iteration 30/65: Train Loss = 0.2095, Valid Loss = 0.2580\n",
      "Iteration 31/65: Train Loss = 0.2078, Valid Loss = 0.2563\n",
      "Iteration 32/65: Train Loss = 0.2059, Valid Loss = 0.2553\n",
      "Iteration 33/65: Train Loss = 0.2040, Valid Loss = 0.2542\n",
      "Iteration 34/65: Train Loss = 0.2028, Valid Loss = 0.2534\n",
      "Iteration 35/65: Train Loss = 0.2013, Valid Loss = 0.2525\n",
      "Iteration 36/65: Train Loss = 0.1999, Valid Loss = 0.2518\n",
      "Iteration 37/65: Train Loss = 0.1983, Valid Loss = 0.2509\n",
      "Iteration 38/65: Train Loss = 0.1972, Valid Loss = 0.2503\n",
      "Iteration 39/65: Train Loss = 0.1960, Valid Loss = 0.2500\n",
      "Iteration 40/65: Train Loss = 0.1949, Valid Loss = 0.2498\n",
      "Iteration 41/65: Train Loss = 0.1936, Valid Loss = 0.2494\n",
      "Iteration 42/65: Train Loss = 0.1923, Valid Loss = 0.2492\n",
      "Iteration 43/65: Train Loss = 0.1913, Valid Loss = 0.2482\n",
      "Iteration 44/65: Train Loss = 0.1903, Valid Loss = 0.2476\n",
      "Iteration 45/65: Train Loss = 0.1894, Valid Loss = 0.2477\n",
      "Iteration 46/65: Train Loss = 0.1884, Valid Loss = 0.2475\n",
      "Iteration 47/65: Train Loss = 0.1874, Valid Loss = 0.2470\n",
      "Iteration 48/65: Train Loss = 0.1864, Valid Loss = 0.2465\n",
      "Iteration 49/65: Train Loss = 0.1856, Valid Loss = 0.2464\n",
      "Iteration 50/65: Train Loss = 0.1847, Valid Loss = 0.2457\n",
      "Iteration 51/65: Train Loss = 0.1839, Valid Loss = 0.2452\n",
      "Iteration 52/65: Train Loss = 0.1831, Valid Loss = 0.2452\n",
      "Iteration 53/65: Train Loss = 0.1824, Valid Loss = 0.2450\n",
      "Iteration 54/65: Train Loss = 0.1816, Valid Loss = 0.2451\n",
      "Iteration 55/65: Train Loss = 0.1810, Valid Loss = 0.2448\n",
      "Iteration 56/65: Train Loss = 0.1804, Valid Loss = 0.2444\n",
      "Iteration 57/65: Train Loss = 0.1797, Valid Loss = 0.2441\n",
      "Iteration 58/65: Train Loss = 0.1789, Valid Loss = 0.2439\n",
      "Iteration 59/65: Train Loss = 0.1784, Valid Loss = 0.2440\n",
      "Iteration 60/65: Train Loss = 0.1778, Valid Loss = 0.2439\n",
      "Iteration 61/65: Train Loss = 0.1770, Valid Loss = 0.2439\n",
      "Iteration 62/65: Train Loss = 0.1764, Valid Loss = 0.2436\n",
      "Iteration 63/65: Train Loss = 0.1756, Valid Loss = 0.2437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:51:39,566] Trial 33 finished with value: 0.9648158951305721 and parameters: {'max_depth': 9, 'min_samples_leaf': 11, 'n_estimators': 65, 'learning_rate': 0.6500436265890529, 'subsample': 0.7636354545560328}. Best is trial 27 with value: 0.9657550072635349.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 64/65: Train Loss = 0.1750, Valid Loss = 0.2436\n",
      "Iteration 65/65: Train Loss = 0.1745, Valid Loss = 0.2431\n",
      "Iteration 1/55: Train Loss = 0.6613, Valid Loss = 0.6622\n",
      "Iteration 2/55: Train Loss = 0.6322, Valid Loss = 0.6337\n",
      "Iteration 3/55: Train Loss = 0.6058, Valid Loss = 0.6079\n",
      "Iteration 4/55: Train Loss = 0.5814, Valid Loss = 0.5844\n",
      "Iteration 5/55: Train Loss = 0.5595, Valid Loss = 0.5633\n",
      "Iteration 6/55: Train Loss = 0.5395, Valid Loss = 0.5440\n",
      "Iteration 7/55: Train Loss = 0.5212, Valid Loss = 0.5264\n",
      "Iteration 8/55: Train Loss = 0.5046, Valid Loss = 0.5108\n",
      "Iteration 9/55: Train Loss = 0.4894, Valid Loss = 0.4963\n",
      "Iteration 10/55: Train Loss = 0.4755, Valid Loss = 0.4829\n",
      "Iteration 11/55: Train Loss = 0.4624, Valid Loss = 0.4705\n",
      "Iteration 12/55: Train Loss = 0.4505, Valid Loss = 0.4592\n",
      "Iteration 13/55: Train Loss = 0.4394, Valid Loss = 0.4486\n",
      "Iteration 14/55: Train Loss = 0.4291, Valid Loss = 0.4388\n",
      "Iteration 15/55: Train Loss = 0.4195, Valid Loss = 0.4297\n",
      "Iteration 16/55: Train Loss = 0.4106, Valid Loss = 0.4212\n",
      "Iteration 17/55: Train Loss = 0.4023, Valid Loss = 0.4135\n",
      "Iteration 18/55: Train Loss = 0.3947, Valid Loss = 0.4064\n",
      "Iteration 19/55: Train Loss = 0.3877, Valid Loss = 0.3994\n",
      "Iteration 20/55: Train Loss = 0.3809, Valid Loss = 0.3931\n",
      "Iteration 21/55: Train Loss = 0.3747, Valid Loss = 0.3871\n",
      "Iteration 22/55: Train Loss = 0.3687, Valid Loss = 0.3816\n",
      "Iteration 23/55: Train Loss = 0.3632, Valid Loss = 0.3764\n",
      "Iteration 24/55: Train Loss = 0.3579, Valid Loss = 0.3716\n",
      "Iteration 25/55: Train Loss = 0.3530, Valid Loss = 0.3671\n",
      "Iteration 26/55: Train Loss = 0.3482, Valid Loss = 0.3628\n",
      "Iteration 27/55: Train Loss = 0.3437, Valid Loss = 0.3586\n",
      "Iteration 28/55: Train Loss = 0.3393, Valid Loss = 0.3543\n",
      "Iteration 29/55: Train Loss = 0.3353, Valid Loss = 0.3508\n",
      "Iteration 30/55: Train Loss = 0.3316, Valid Loss = 0.3473\n",
      "Iteration 31/55: Train Loss = 0.3279, Valid Loss = 0.3438\n",
      "Iteration 32/55: Train Loss = 0.3244, Valid Loss = 0.3407\n",
      "Iteration 33/55: Train Loss = 0.3211, Valid Loss = 0.3375\n",
      "Iteration 34/55: Train Loss = 0.3179, Valid Loss = 0.3346\n",
      "Iteration 35/55: Train Loss = 0.3149, Valid Loss = 0.3317\n",
      "Iteration 36/55: Train Loss = 0.3118, Valid Loss = 0.3289\n",
      "Iteration 37/55: Train Loss = 0.3091, Valid Loss = 0.3264\n",
      "Iteration 38/55: Train Loss = 0.3064, Valid Loss = 0.3239\n",
      "Iteration 39/55: Train Loss = 0.3038, Valid Loss = 0.3216\n",
      "Iteration 40/55: Train Loss = 0.3012, Valid Loss = 0.3194\n",
      "Iteration 41/55: Train Loss = 0.2988, Valid Loss = 0.3173\n",
      "Iteration 42/55: Train Loss = 0.2966, Valid Loss = 0.3151\n",
      "Iteration 43/55: Train Loss = 0.2944, Valid Loss = 0.3131\n",
      "Iteration 44/55: Train Loss = 0.2923, Valid Loss = 0.3112\n",
      "Iteration 45/55: Train Loss = 0.2902, Valid Loss = 0.3093\n",
      "Iteration 46/55: Train Loss = 0.2881, Valid Loss = 0.3075\n",
      "Iteration 47/55: Train Loss = 0.2863, Valid Loss = 0.3059\n",
      "Iteration 48/55: Train Loss = 0.2844, Valid Loss = 0.3041\n",
      "Iteration 49/55: Train Loss = 0.2827, Valid Loss = 0.3026\n",
      "Iteration 50/55: Train Loss = 0.2810, Valid Loss = 0.3012\n",
      "Iteration 51/55: Train Loss = 0.2793, Valid Loss = 0.2996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:51:42,297] Trial 34 finished with value: 0.9614113753436357 and parameters: {'max_depth': 6, 'min_samples_leaf': 5, 'n_estimators': 55, 'learning_rate': 0.19677515591519582, 'subsample': 0.8279177303013826}. Best is trial 27 with value: 0.9657550072635349.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52/55: Train Loss = 0.2776, Valid Loss = 0.2981\n",
      "Iteration 53/55: Train Loss = 0.2760, Valid Loss = 0.2965\n",
      "Iteration 54/55: Train Loss = 0.2745, Valid Loss = 0.2953\n",
      "Iteration 55/55: Train Loss = 0.2730, Valid Loss = 0.2940\n",
      "Iteration 1/90: Train Loss = 0.6470, Valid Loss = 0.6478\n",
      "Iteration 2/90: Train Loss = 0.6075, Valid Loss = 0.6094\n",
      "Iteration 3/90: Train Loss = 0.5729, Valid Loss = 0.5758\n",
      "Iteration 4/90: Train Loss = 0.5433, Valid Loss = 0.5469\n",
      "Iteration 5/90: Train Loss = 0.5177, Valid Loss = 0.5217\n",
      "Iteration 6/90: Train Loss = 0.4957, Valid Loss = 0.5001\n",
      "Iteration 7/90: Train Loss = 0.4761, Valid Loss = 0.4812\n",
      "Iteration 8/90: Train Loss = 0.4592, Valid Loss = 0.4649\n",
      "Iteration 9/90: Train Loss = 0.4443, Valid Loss = 0.4505\n",
      "Iteration 10/90: Train Loss = 0.4307, Valid Loss = 0.4368\n",
      "Iteration 11/90: Train Loss = 0.4184, Valid Loss = 0.4251\n",
      "Iteration 12/90: Train Loss = 0.4073, Valid Loss = 0.4143\n",
      "Iteration 13/90: Train Loss = 0.3974, Valid Loss = 0.4042\n",
      "Iteration 14/90: Train Loss = 0.3884, Valid Loss = 0.3956\n",
      "Iteration 15/90: Train Loss = 0.3805, Valid Loss = 0.3884\n",
      "Iteration 16/90: Train Loss = 0.3732, Valid Loss = 0.3814\n",
      "Iteration 17/90: Train Loss = 0.3664, Valid Loss = 0.3745\n",
      "Iteration 18/90: Train Loss = 0.3600, Valid Loss = 0.3684\n",
      "Iteration 19/90: Train Loss = 0.3544, Valid Loss = 0.3629\n",
      "Iteration 20/90: Train Loss = 0.3493, Valid Loss = 0.3583\n",
      "Iteration 21/90: Train Loss = 0.3443, Valid Loss = 0.3534\n",
      "Iteration 22/90: Train Loss = 0.3397, Valid Loss = 0.3490\n",
      "Iteration 23/90: Train Loss = 0.3354, Valid Loss = 0.3447\n",
      "Iteration 24/90: Train Loss = 0.3314, Valid Loss = 0.3409\n",
      "Iteration 25/90: Train Loss = 0.3280, Valid Loss = 0.3375\n",
      "Iteration 26/90: Train Loss = 0.3243, Valid Loss = 0.3340\n",
      "Iteration 27/90: Train Loss = 0.3213, Valid Loss = 0.3311\n",
      "Iteration 28/90: Train Loss = 0.3183, Valid Loss = 0.3281\n",
      "Iteration 29/90: Train Loss = 0.3154, Valid Loss = 0.3254\n",
      "Iteration 30/90: Train Loss = 0.3125, Valid Loss = 0.3228\n",
      "Iteration 31/90: Train Loss = 0.3097, Valid Loss = 0.3201\n",
      "Iteration 32/90: Train Loss = 0.3071, Valid Loss = 0.3177\n",
      "Iteration 33/90: Train Loss = 0.3049, Valid Loss = 0.3153\n",
      "Iteration 34/90: Train Loss = 0.3026, Valid Loss = 0.3131\n",
      "Iteration 35/90: Train Loss = 0.3007, Valid Loss = 0.3115\n",
      "Iteration 36/90: Train Loss = 0.2986, Valid Loss = 0.3093\n",
      "Iteration 37/90: Train Loss = 0.2967, Valid Loss = 0.3074\n",
      "Iteration 38/90: Train Loss = 0.2946, Valid Loss = 0.3053\n",
      "Iteration 39/90: Train Loss = 0.2929, Valid Loss = 0.3038\n",
      "Iteration 40/90: Train Loss = 0.2912, Valid Loss = 0.3021\n",
      "Iteration 41/90: Train Loss = 0.2896, Valid Loss = 0.3005\n",
      "Iteration 42/90: Train Loss = 0.2882, Valid Loss = 0.2994\n",
      "Iteration 43/90: Train Loss = 0.2863, Valid Loss = 0.2979\n",
      "Iteration 44/90: Train Loss = 0.2849, Valid Loss = 0.2966\n",
      "Iteration 45/90: Train Loss = 0.2835, Valid Loss = 0.2952\n",
      "Iteration 46/90: Train Loss = 0.2821, Valid Loss = 0.2941\n",
      "Iteration 47/90: Train Loss = 0.2808, Valid Loss = 0.2929\n",
      "Iteration 48/90: Train Loss = 0.2795, Valid Loss = 0.2919\n",
      "Iteration 49/90: Train Loss = 0.2782, Valid Loss = 0.2909\n",
      "Iteration 50/90: Train Loss = 0.2769, Valid Loss = 0.2899\n",
      "Iteration 51/90: Train Loss = 0.2757, Valid Loss = 0.2886\n",
      "Iteration 52/90: Train Loss = 0.2745, Valid Loss = 0.2877\n",
      "Iteration 53/90: Train Loss = 0.2735, Valid Loss = 0.2867\n",
      "Iteration 54/90: Train Loss = 0.2725, Valid Loss = 0.2858\n",
      "Iteration 55/90: Train Loss = 0.2715, Valid Loss = 0.2848\n",
      "Iteration 56/90: Train Loss = 0.2704, Valid Loss = 0.2838\n",
      "Iteration 57/90: Train Loss = 0.2693, Valid Loss = 0.2828\n",
      "Iteration 58/90: Train Loss = 0.2684, Valid Loss = 0.2822\n",
      "Iteration 59/90: Train Loss = 0.2675, Valid Loss = 0.2815\n",
      "Iteration 60/90: Train Loss = 0.2666, Valid Loss = 0.2807\n",
      "Iteration 61/90: Train Loss = 0.2656, Valid Loss = 0.2800\n",
      "Iteration 62/90: Train Loss = 0.2648, Valid Loss = 0.2792\n",
      "Iteration 63/90: Train Loss = 0.2638, Valid Loss = 0.2783\n",
      "Iteration 64/90: Train Loss = 0.2630, Valid Loss = 0.2774\n",
      "Iteration 65/90: Train Loss = 0.2620, Valid Loss = 0.2765\n",
      "Iteration 66/90: Train Loss = 0.2612, Valid Loss = 0.2757\n",
      "Iteration 67/90: Train Loss = 0.2605, Valid Loss = 0.2753\n",
      "Iteration 68/90: Train Loss = 0.2597, Valid Loss = 0.2745\n",
      "Iteration 69/90: Train Loss = 0.2590, Valid Loss = 0.2738\n",
      "Iteration 70/90: Train Loss = 0.2584, Valid Loss = 0.2734\n",
      "Iteration 71/90: Train Loss = 0.2577, Valid Loss = 0.2728\n",
      "Iteration 72/90: Train Loss = 0.2572, Valid Loss = 0.2722\n",
      "Iteration 73/90: Train Loss = 0.2565, Valid Loss = 0.2717\n",
      "Iteration 74/90: Train Loss = 0.2557, Valid Loss = 0.2710\n",
      "Iteration 75/90: Train Loss = 0.2552, Valid Loss = 0.2704\n",
      "Iteration 76/90: Train Loss = 0.2546, Valid Loss = 0.2699\n",
      "Iteration 77/90: Train Loss = 0.2540, Valid Loss = 0.2693\n",
      "Iteration 78/90: Train Loss = 0.2535, Valid Loss = 0.2688\n",
      "Iteration 79/90: Train Loss = 0.2529, Valid Loss = 0.2684\n",
      "Iteration 80/90: Train Loss = 0.2523, Valid Loss = 0.2679\n",
      "Iteration 81/90: Train Loss = 0.2518, Valid Loss = 0.2676\n",
      "Iteration 82/90: Train Loss = 0.2512, Valid Loss = 0.2673\n",
      "Iteration 83/90: Train Loss = 0.2507, Valid Loss = 0.2666\n",
      "Iteration 84/90: Train Loss = 0.2502, Valid Loss = 0.2662\n",
      "Iteration 85/90: Train Loss = 0.2497, Valid Loss = 0.2657\n",
      "Iteration 86/90: Train Loss = 0.2493, Valid Loss = 0.2653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:51:45,509] Trial 35 finished with value: 0.9611010069813817 and parameters: {'max_depth': 4, 'min_samples_leaf': 9, 'n_estimators': 90, 'learning_rate': 0.31605918454875487, 'subsample': 0.7880828896973697}. Best is trial 27 with value: 0.9657550072635349.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 87/90: Train Loss = 0.2488, Valid Loss = 0.2646\n",
      "Iteration 88/90: Train Loss = 0.2483, Valid Loss = 0.2643\n",
      "Iteration 89/90: Train Loss = 0.2477, Valid Loss = 0.2640\n",
      "Iteration 90/90: Train Loss = 0.2472, Valid Loss = 0.2637\n",
      "Iteration 1/140: Train Loss = 0.5839, Valid Loss = 0.5869\n",
      "Iteration 2/140: Train Loss = 0.5086, Valid Loss = 0.5135\n",
      "Iteration 3/140: Train Loss = 0.4561, Valid Loss = 0.4624\n",
      "Iteration 4/140: Train Loss = 0.4169, Valid Loss = 0.4245\n",
      "Iteration 5/140: Train Loss = 0.3875, Valid Loss = 0.3965\n",
      "Iteration 6/140: Train Loss = 0.3648, Valid Loss = 0.3743\n",
      "Iteration 7/140: Train Loss = 0.3464, Valid Loss = 0.3572\n",
      "Iteration 8/140: Train Loss = 0.3319, Valid Loss = 0.3439\n",
      "Iteration 9/140: Train Loss = 0.3201, Valid Loss = 0.3326\n",
      "Iteration 10/140: Train Loss = 0.3095, Valid Loss = 0.3238\n",
      "Iteration 11/140: Train Loss = 0.3006, Valid Loss = 0.3159\n",
      "Iteration 12/140: Train Loss = 0.2932, Valid Loss = 0.3090\n",
      "Iteration 13/140: Train Loss = 0.2861, Valid Loss = 0.3032\n",
      "Iteration 14/140: Train Loss = 0.2803, Valid Loss = 0.2981\n",
      "Iteration 15/140: Train Loss = 0.2750, Valid Loss = 0.2933\n",
      "Iteration 16/140: Train Loss = 0.2705, Valid Loss = 0.2892\n",
      "Iteration 17/140: Train Loss = 0.2665, Valid Loss = 0.2853\n",
      "Iteration 18/140: Train Loss = 0.2622, Valid Loss = 0.2813\n",
      "Iteration 19/140: Train Loss = 0.2583, Valid Loss = 0.2783\n",
      "Iteration 20/140: Train Loss = 0.2553, Valid Loss = 0.2758\n",
      "Iteration 21/140: Train Loss = 0.2524, Valid Loss = 0.2736\n",
      "Iteration 22/140: Train Loss = 0.2497, Valid Loss = 0.2717\n",
      "Iteration 23/140: Train Loss = 0.2467, Valid Loss = 0.2688\n",
      "Iteration 24/140: Train Loss = 0.2446, Valid Loss = 0.2674\n",
      "Iteration 25/140: Train Loss = 0.2423, Valid Loss = 0.2656\n",
      "Iteration 26/140: Train Loss = 0.2403, Valid Loss = 0.2636\n",
      "Iteration 27/140: Train Loss = 0.2381, Valid Loss = 0.2622\n",
      "Iteration 28/140: Train Loss = 0.2363, Valid Loss = 0.2606\n",
      "Iteration 29/140: Train Loss = 0.2347, Valid Loss = 0.2595\n",
      "Iteration 30/140: Train Loss = 0.2333, Valid Loss = 0.2586\n",
      "Iteration 31/140: Train Loss = 0.2318, Valid Loss = 0.2575\n",
      "Iteration 32/140: Train Loss = 0.2302, Valid Loss = 0.2569\n",
      "Iteration 33/140: Train Loss = 0.2287, Valid Loss = 0.2565\n",
      "Iteration 34/140: Train Loss = 0.2270, Valid Loss = 0.2559\n",
      "Iteration 35/140: Train Loss = 0.2258, Valid Loss = 0.2547\n",
      "Iteration 36/140: Train Loss = 0.2247, Valid Loss = 0.2539\n",
      "Iteration 37/140: Train Loss = 0.2233, Valid Loss = 0.2530\n",
      "Iteration 38/140: Train Loss = 0.2222, Valid Loss = 0.2526\n",
      "Iteration 39/140: Train Loss = 0.2211, Valid Loss = 0.2522\n",
      "Iteration 40/140: Train Loss = 0.2202, Valid Loss = 0.2518\n",
      "Iteration 41/140: Train Loss = 0.2192, Valid Loss = 0.2512\n",
      "Iteration 42/140: Train Loss = 0.2183, Valid Loss = 0.2502\n",
      "Iteration 43/140: Train Loss = 0.2174, Valid Loss = 0.2493\n",
      "Iteration 44/140: Train Loss = 0.2165, Valid Loss = 0.2488\n",
      "Iteration 45/140: Train Loss = 0.2158, Valid Loss = 0.2485\n",
      "Iteration 46/140: Train Loss = 0.2150, Valid Loss = 0.2479\n",
      "Iteration 47/140: Train Loss = 0.2144, Valid Loss = 0.2475\n",
      "Iteration 48/140: Train Loss = 0.2134, Valid Loss = 0.2475\n",
      "Iteration 49/140: Train Loss = 0.2126, Valid Loss = 0.2472\n",
      "Iteration 50/140: Train Loss = 0.2118, Valid Loss = 0.2466\n",
      "Iteration 51/140: Train Loss = 0.2113, Valid Loss = 0.2462\n",
      "Iteration 52/140: Train Loss = 0.2105, Valid Loss = 0.2457\n",
      "Iteration 53/140: Train Loss = 0.2097, Valid Loss = 0.2454\n",
      "Iteration 54/140: Train Loss = 0.2090, Valid Loss = 0.2452\n",
      "Iteration 55/140: Train Loss = 0.2083, Valid Loss = 0.2447\n",
      "Iteration 56/140: Train Loss = 0.2076, Valid Loss = 0.2445\n",
      "Iteration 57/140: Train Loss = 0.2072, Valid Loss = 0.2447\n",
      "Iteration 58/140: Train Loss = 0.2065, Valid Loss = 0.2444\n",
      "Iteration 59/140: Train Loss = 0.2061, Valid Loss = 0.2441\n",
      "Iteration 60/140: Train Loss = 0.2053, Valid Loss = 0.2436\n",
      "Iteration 61/140: Train Loss = 0.2049, Valid Loss = 0.2434\n",
      "Iteration 62/140: Train Loss = 0.2043, Valid Loss = 0.2431\n",
      "Iteration 63/140: Train Loss = 0.2038, Valid Loss = 0.2432\n",
      "Iteration 64/140: Train Loss = 0.2033, Valid Loss = 0.2431\n",
      "Iteration 65/140: Train Loss = 0.2028, Valid Loss = 0.2429\n",
      "Iteration 66/140: Train Loss = 0.2021, Valid Loss = 0.2425\n",
      "Iteration 67/140: Train Loss = 0.2016, Valid Loss = 0.2422\n",
      "Iteration 68/140: Train Loss = 0.2011, Valid Loss = 0.2421\n",
      "Iteration 69/140: Train Loss = 0.2007, Valid Loss = 0.2418\n",
      "Iteration 70/140: Train Loss = 0.2003, Valid Loss = 0.2415\n",
      "Iteration 71/140: Train Loss = 0.1999, Valid Loss = 0.2413\n",
      "Iteration 72/140: Train Loss = 0.1996, Valid Loss = 0.2412\n",
      "Iteration 73/140: Train Loss = 0.1993, Valid Loss = 0.2414\n",
      "Iteration 74/140: Train Loss = 0.1989, Valid Loss = 0.2413\n",
      "Iteration 75/140: Train Loss = 0.1985, Valid Loss = 0.2411\n",
      "Iteration 76/140: Train Loss = 0.1982, Valid Loss = 0.2412\n",
      "Iteration 77/140: Train Loss = 0.1976, Valid Loss = 0.2413\n",
      "Iteration 78/140: Train Loss = 0.1972, Valid Loss = 0.2414\n",
      "Iteration 79/140: Train Loss = 0.1968, Valid Loss = 0.2414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:51:49,518] Trial 36 finished with value: 0.9650458117429052 and parameters: {'max_depth': 6, 'min_samples_leaf': 14, 'n_estimators': 140, 'learning_rate': 0.7234446962053851, 'subsample': 0.8652769824890513}. Best is trial 27 with value: 0.9657550072635349.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 80/140: Train Loss = 0.1966, Valid Loss = 0.2413\n",
      "Iteration 81/140: Train Loss = 0.1963, Valid Loss = 0.2415\n",
      "Early stopping triggered.\n",
      "Iteration 1/140: Train Loss = 0.6316, Valid Loss = 0.6330\n",
      "Iteration 2/140: Train Loss = 0.5810, Valid Loss = 0.5840\n",
      "Iteration 3/140: Train Loss = 0.5396, Valid Loss = 0.5441\n",
      "Iteration 4/140: Train Loss = 0.5052, Valid Loss = 0.5108\n",
      "Iteration 5/140: Train Loss = 0.4763, Valid Loss = 0.4826\n",
      "Iteration 6/140: Train Loss = 0.4520, Valid Loss = 0.4595\n",
      "Iteration 7/140: Train Loss = 0.4309, Valid Loss = 0.4391\n",
      "Iteration 8/140: Train Loss = 0.4128, Valid Loss = 0.4220\n",
      "Iteration 9/140: Train Loss = 0.3971, Valid Loss = 0.4065\n",
      "Iteration 10/140: Train Loss = 0.3835, Valid Loss = 0.3939\n",
      "Iteration 11/140: Train Loss = 0.3713, Valid Loss = 0.3826\n",
      "Iteration 12/140: Train Loss = 0.3605, Valid Loss = 0.3724\n",
      "Iteration 13/140: Train Loss = 0.3510, Valid Loss = 0.3637\n",
      "Iteration 14/140: Train Loss = 0.3423, Valid Loss = 0.3555\n",
      "Iteration 15/140: Train Loss = 0.3340, Valid Loss = 0.3482\n",
      "Iteration 16/140: Train Loss = 0.3269, Valid Loss = 0.3416\n",
      "Iteration 17/140: Train Loss = 0.3207, Valid Loss = 0.3353\n",
      "Iteration 18/140: Train Loss = 0.3146, Valid Loss = 0.3301\n",
      "Iteration 19/140: Train Loss = 0.3092, Valid Loss = 0.3250\n",
      "Iteration 20/140: Train Loss = 0.3044, Valid Loss = 0.3204\n",
      "Iteration 21/140: Train Loss = 0.3001, Valid Loss = 0.3164\n",
      "Iteration 22/140: Train Loss = 0.2957, Valid Loss = 0.3127\n",
      "Iteration 23/140: Train Loss = 0.2918, Valid Loss = 0.3092\n",
      "Iteration 24/140: Train Loss = 0.2884, Valid Loss = 0.3060\n",
      "Iteration 25/140: Train Loss = 0.2851, Valid Loss = 0.3031\n",
      "Iteration 26/140: Train Loss = 0.2820, Valid Loss = 0.3003\n",
      "Iteration 27/140: Train Loss = 0.2790, Valid Loss = 0.2976\n",
      "Iteration 28/140: Train Loss = 0.2764, Valid Loss = 0.2954\n",
      "Iteration 29/140: Train Loss = 0.2738, Valid Loss = 0.2929\n",
      "Iteration 30/140: Train Loss = 0.2711, Valid Loss = 0.2907\n",
      "Iteration 31/140: Train Loss = 0.2687, Valid Loss = 0.2885\n",
      "Iteration 32/140: Train Loss = 0.2665, Valid Loss = 0.2870\n",
      "Iteration 33/140: Train Loss = 0.2640, Valid Loss = 0.2848\n",
      "Iteration 34/140: Train Loss = 0.2621, Valid Loss = 0.2831\n",
      "Iteration 35/140: Train Loss = 0.2602, Valid Loss = 0.2818\n",
      "Iteration 36/140: Train Loss = 0.2585, Valid Loss = 0.2804\n",
      "Iteration 37/140: Train Loss = 0.2563, Valid Loss = 0.2789\n",
      "Iteration 38/140: Train Loss = 0.2546, Valid Loss = 0.2773\n",
      "Iteration 39/140: Train Loss = 0.2528, Valid Loss = 0.2758\n",
      "Iteration 40/140: Train Loss = 0.2511, Valid Loss = 0.2745\n",
      "Iteration 41/140: Train Loss = 0.2498, Valid Loss = 0.2735\n",
      "Iteration 42/140: Train Loss = 0.2483, Valid Loss = 0.2721\n",
      "Iteration 43/140: Train Loss = 0.2469, Valid Loss = 0.2711\n",
      "Iteration 44/140: Train Loss = 0.2455, Valid Loss = 0.2700\n",
      "Iteration 45/140: Train Loss = 0.2442, Valid Loss = 0.2686\n",
      "Iteration 46/140: Train Loss = 0.2429, Valid Loss = 0.2675\n",
      "Iteration 47/140: Train Loss = 0.2416, Valid Loss = 0.2664\n",
      "Iteration 48/140: Train Loss = 0.2405, Valid Loss = 0.2657\n",
      "Iteration 49/140: Train Loss = 0.2395, Valid Loss = 0.2650\n",
      "Iteration 50/140: Train Loss = 0.2383, Valid Loss = 0.2642\n",
      "Iteration 51/140: Train Loss = 0.2373, Valid Loss = 0.2630\n",
      "Iteration 52/140: Train Loss = 0.2364, Valid Loss = 0.2620\n",
      "Iteration 53/140: Train Loss = 0.2353, Valid Loss = 0.2612\n",
      "Iteration 54/140: Train Loss = 0.2344, Valid Loss = 0.2607\n",
      "Iteration 55/140: Train Loss = 0.2337, Valid Loss = 0.2604\n",
      "Iteration 56/140: Train Loss = 0.2329, Valid Loss = 0.2596\n",
      "Iteration 57/140: Train Loss = 0.2321, Valid Loss = 0.2589\n",
      "Iteration 58/140: Train Loss = 0.2313, Valid Loss = 0.2585\n",
      "Iteration 59/140: Train Loss = 0.2304, Valid Loss = 0.2578\n",
      "Iteration 60/140: Train Loss = 0.2297, Valid Loss = 0.2575\n",
      "Iteration 61/140: Train Loss = 0.2289, Valid Loss = 0.2569\n",
      "Iteration 62/140: Train Loss = 0.2281, Valid Loss = 0.2564\n",
      "Iteration 63/140: Train Loss = 0.2276, Valid Loss = 0.2558\n",
      "Iteration 64/140: Train Loss = 0.2269, Valid Loss = 0.2551\n",
      "Iteration 65/140: Train Loss = 0.2261, Valid Loss = 0.2547\n",
      "Iteration 66/140: Train Loss = 0.2255, Valid Loss = 0.2545\n",
      "Iteration 67/140: Train Loss = 0.2249, Valid Loss = 0.2541\n",
      "Iteration 68/140: Train Loss = 0.2244, Valid Loss = 0.2535\n",
      "Iteration 69/140: Train Loss = 0.2238, Valid Loss = 0.2531\n",
      "Iteration 70/140: Train Loss = 0.2232, Valid Loss = 0.2527\n",
      "Iteration 71/140: Train Loss = 0.2226, Valid Loss = 0.2524\n",
      "Iteration 72/140: Train Loss = 0.2219, Valid Loss = 0.2520\n",
      "Iteration 73/140: Train Loss = 0.2214, Valid Loss = 0.2516\n",
      "Iteration 74/140: Train Loss = 0.2208, Valid Loss = 0.2511\n",
      "Iteration 75/140: Train Loss = 0.2203, Valid Loss = 0.2508\n",
      "Iteration 76/140: Train Loss = 0.2197, Valid Loss = 0.2503\n",
      "Iteration 77/140: Train Loss = 0.2191, Valid Loss = 0.2497\n",
      "Iteration 78/140: Train Loss = 0.2185, Valid Loss = 0.2496\n",
      "Iteration 79/140: Train Loss = 0.2180, Valid Loss = 0.2493\n",
      "Iteration 80/140: Train Loss = 0.2176, Valid Loss = 0.2491\n",
      "Iteration 81/140: Train Loss = 0.2171, Valid Loss = 0.2489\n",
      "Iteration 82/140: Train Loss = 0.2166, Valid Loss = 0.2486\n",
      "Iteration 83/140: Train Loss = 0.2161, Valid Loss = 0.2486\n",
      "Iteration 84/140: Train Loss = 0.2157, Valid Loss = 0.2482\n",
      "Iteration 85/140: Train Loss = 0.2152, Valid Loss = 0.2480\n",
      "Iteration 86/140: Train Loss = 0.2148, Valid Loss = 0.2477\n",
      "Iteration 87/140: Train Loss = 0.2143, Valid Loss = 0.2473\n",
      "Iteration 88/140: Train Loss = 0.2139, Valid Loss = 0.2470\n",
      "Iteration 89/140: Train Loss = 0.2135, Valid Loss = 0.2468\n",
      "Iteration 90/140: Train Loss = 0.2130, Valid Loss = 0.2468\n",
      "Iteration 91/140: Train Loss = 0.2125, Valid Loss = 0.2466\n",
      "Iteration 92/140: Train Loss = 0.2121, Valid Loss = 0.2464\n",
      "Iteration 93/140: Train Loss = 0.2117, Valid Loss = 0.2462\n",
      "Iteration 94/140: Train Loss = 0.2113, Valid Loss = 0.2461\n",
      "Iteration 95/140: Train Loss = 0.2111, Valid Loss = 0.2461\n",
      "Iteration 96/140: Train Loss = 0.2107, Valid Loss = 0.2459\n",
      "Iteration 97/140: Train Loss = 0.2105, Valid Loss = 0.2456\n",
      "Iteration 98/140: Train Loss = 0.2100, Valid Loss = 0.2452\n",
      "Iteration 99/140: Train Loss = 0.2096, Valid Loss = 0.2454\n",
      "Iteration 100/140: Train Loss = 0.2092, Valid Loss = 0.2450\n",
      "Iteration 101/140: Train Loss = 0.2090, Valid Loss = 0.2449\n",
      "Iteration 102/140: Train Loss = 0.2087, Valid Loss = 0.2448\n",
      "Iteration 103/140: Train Loss = 0.2084, Valid Loss = 0.2446\n",
      "Iteration 104/140: Train Loss = 0.2081, Valid Loss = 0.2444\n",
      "Iteration 105/140: Train Loss = 0.2076, Valid Loss = 0.2443\n",
      "Iteration 106/140: Train Loss = 0.2071, Valid Loss = 0.2443\n",
      "Iteration 107/140: Train Loss = 0.2068, Valid Loss = 0.2442\n",
      "Iteration 108/140: Train Loss = 0.2065, Valid Loss = 0.2442\n",
      "Iteration 109/140: Train Loss = 0.2061, Valid Loss = 0.2437\n",
      "Iteration 110/140: Train Loss = 0.2058, Valid Loss = 0.2436\n",
      "Iteration 111/140: Train Loss = 0.2055, Valid Loss = 0.2433\n",
      "Iteration 112/140: Train Loss = 0.2051, Valid Loss = 0.2432\n",
      "Iteration 113/140: Train Loss = 0.2048, Valid Loss = 0.2431\n",
      "Iteration 114/140: Train Loss = 0.2045, Valid Loss = 0.2430\n",
      "Iteration 115/140: Train Loss = 0.2043, Valid Loss = 0.2428\n",
      "Iteration 116/140: Train Loss = 0.2040, Valid Loss = 0.2427\n",
      "Iteration 117/140: Train Loss = 0.2037, Valid Loss = 0.2428\n",
      "Iteration 118/140: Train Loss = 0.2033, Valid Loss = 0.2426\n",
      "Iteration 119/140: Train Loss = 0.2031, Valid Loss = 0.2425\n",
      "Iteration 120/140: Train Loss = 0.2027, Valid Loss = 0.2426\n",
      "Iteration 121/140: Train Loss = 0.2024, Valid Loss = 0.2425\n",
      "Iteration 122/140: Train Loss = 0.2021, Valid Loss = 0.2424\n",
      "Iteration 123/140: Train Loss = 0.2018, Valid Loss = 0.2423\n",
      "Iteration 124/140: Train Loss = 0.2016, Valid Loss = 0.2422\n",
      "Iteration 125/140: Train Loss = 0.2012, Valid Loss = 0.2420\n",
      "Iteration 126/140: Train Loss = 0.2009, Valid Loss = 0.2419\n",
      "Iteration 127/140: Train Loss = 0.2006, Valid Loss = 0.2417\n",
      "Iteration 128/140: Train Loss = 0.2004, Valid Loss = 0.2415\n",
      "Iteration 129/140: Train Loss = 0.2002, Valid Loss = 0.2416\n",
      "Iteration 130/140: Train Loss = 0.1999, Valid Loss = 0.2414\n",
      "Iteration 131/140: Train Loss = 0.1998, Valid Loss = 0.2415\n",
      "Iteration 132/140: Train Loss = 0.1996, Valid Loss = 0.2414\n",
      "Iteration 133/140: Train Loss = 0.1993, Valid Loss = 0.2412\n",
      "Iteration 134/140: Train Loss = 0.1989, Valid Loss = 0.2409\n",
      "Iteration 135/140: Train Loss = 0.1987, Valid Loss = 0.2410\n",
      "Iteration 136/140: Train Loss = 0.1985, Valid Loss = 0.2409\n",
      "Iteration 137/140: Train Loss = 0.1982, Valid Loss = 0.2407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:51:56,707] Trial 37 finished with value: 0.9652017584998609 and parameters: {'max_depth': 6, 'min_samples_leaf': 14, 'n_estimators': 140, 'learning_rate': 0.38820432920167947, 'subsample': 0.886972000981122}. Best is trial 27 with value: 0.9657550072635349.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 138/140: Train Loss = 0.1979, Valid Loss = 0.2409\n",
      "Iteration 139/140: Train Loss = 0.1976, Valid Loss = 0.2409\n",
      "Iteration 140/140: Train Loss = 0.1974, Valid Loss = 0.2410\n",
      "Iteration 1/135: Train Loss = 0.6323, Valid Loss = 0.6337\n",
      "Iteration 2/135: Train Loss = 0.5834, Valid Loss = 0.5860\n",
      "Iteration 3/135: Train Loss = 0.5424, Valid Loss = 0.5457\n",
      "Iteration 4/135: Train Loss = 0.5087, Valid Loss = 0.5128\n",
      "Iteration 5/135: Train Loss = 0.4807, Valid Loss = 0.4854\n",
      "Iteration 6/135: Train Loss = 0.4572, Valid Loss = 0.4627\n",
      "Iteration 7/135: Train Loss = 0.4368, Valid Loss = 0.4430\n",
      "Iteration 8/135: Train Loss = 0.4192, Valid Loss = 0.4264\n",
      "Iteration 9/135: Train Loss = 0.4035, Valid Loss = 0.4112\n",
      "Iteration 10/135: Train Loss = 0.3902, Valid Loss = 0.3988\n",
      "Iteration 11/135: Train Loss = 0.3788, Valid Loss = 0.3882\n",
      "Iteration 12/135: Train Loss = 0.3685, Valid Loss = 0.3784\n",
      "Iteration 13/135: Train Loss = 0.3594, Valid Loss = 0.3698\n",
      "Iteration 14/135: Train Loss = 0.3510, Valid Loss = 0.3618\n",
      "Iteration 15/135: Train Loss = 0.3438, Valid Loss = 0.3551\n",
      "Iteration 16/135: Train Loss = 0.3369, Valid Loss = 0.3483\n",
      "Iteration 17/135: Train Loss = 0.3307, Valid Loss = 0.3426\n",
      "Iteration 18/135: Train Loss = 0.3250, Valid Loss = 0.3370\n",
      "Iteration 19/135: Train Loss = 0.3200, Valid Loss = 0.3322\n",
      "Iteration 20/135: Train Loss = 0.3156, Valid Loss = 0.3280\n",
      "Iteration 21/135: Train Loss = 0.3114, Valid Loss = 0.3240\n",
      "Iteration 22/135: Train Loss = 0.3076, Valid Loss = 0.3204\n",
      "Iteration 23/135: Train Loss = 0.3038, Valid Loss = 0.3167\n",
      "Iteration 24/135: Train Loss = 0.3001, Valid Loss = 0.3137\n",
      "Iteration 25/135: Train Loss = 0.2970, Valid Loss = 0.3108\n",
      "Iteration 26/135: Train Loss = 0.2941, Valid Loss = 0.3080\n",
      "Iteration 27/135: Train Loss = 0.2909, Valid Loss = 0.3049\n",
      "Iteration 28/135: Train Loss = 0.2883, Valid Loss = 0.3026\n",
      "Iteration 29/135: Train Loss = 0.2858, Valid Loss = 0.3004\n",
      "Iteration 30/135: Train Loss = 0.2833, Valid Loss = 0.2984\n",
      "Iteration 31/135: Train Loss = 0.2811, Valid Loss = 0.2966\n",
      "Iteration 32/135: Train Loss = 0.2789, Valid Loss = 0.2947\n",
      "Iteration 33/135: Train Loss = 0.2768, Valid Loss = 0.2928\n",
      "Iteration 34/135: Train Loss = 0.2749, Valid Loss = 0.2908\n",
      "Iteration 35/135: Train Loss = 0.2732, Valid Loss = 0.2893\n",
      "Iteration 36/135: Train Loss = 0.2715, Valid Loss = 0.2880\n",
      "Iteration 37/135: Train Loss = 0.2698, Valid Loss = 0.2863\n",
      "Iteration 38/135: Train Loss = 0.2681, Valid Loss = 0.2848\n",
      "Iteration 39/135: Train Loss = 0.2665, Valid Loss = 0.2837\n",
      "Iteration 40/135: Train Loss = 0.2649, Valid Loss = 0.2822\n",
      "Iteration 41/135: Train Loss = 0.2634, Valid Loss = 0.2808\n",
      "Iteration 42/135: Train Loss = 0.2620, Valid Loss = 0.2797\n",
      "Iteration 43/135: Train Loss = 0.2607, Valid Loss = 0.2785\n",
      "Iteration 44/135: Train Loss = 0.2594, Valid Loss = 0.2770\n",
      "Iteration 45/135: Train Loss = 0.2581, Valid Loss = 0.2757\n",
      "Iteration 46/135: Train Loss = 0.2567, Valid Loss = 0.2747\n",
      "Iteration 47/135: Train Loss = 0.2554, Valid Loss = 0.2738\n",
      "Iteration 48/135: Train Loss = 0.2540, Valid Loss = 0.2722\n",
      "Iteration 49/135: Train Loss = 0.2530, Valid Loss = 0.2710\n",
      "Iteration 50/135: Train Loss = 0.2521, Valid Loss = 0.2701\n",
      "Iteration 51/135: Train Loss = 0.2511, Valid Loss = 0.2688\n",
      "Iteration 52/135: Train Loss = 0.2502, Valid Loss = 0.2677\n",
      "Iteration 53/135: Train Loss = 0.2492, Valid Loss = 0.2670\n",
      "Iteration 54/135: Train Loss = 0.2484, Valid Loss = 0.2664\n",
      "Iteration 55/135: Train Loss = 0.2475, Valid Loss = 0.2658\n",
      "Iteration 56/135: Train Loss = 0.2467, Valid Loss = 0.2655\n",
      "Iteration 57/135: Train Loss = 0.2458, Valid Loss = 0.2649\n",
      "Iteration 58/135: Train Loss = 0.2449, Valid Loss = 0.2643\n",
      "Iteration 59/135: Train Loss = 0.2440, Valid Loss = 0.2637\n",
      "Iteration 60/135: Train Loss = 0.2433, Valid Loss = 0.2633\n",
      "Iteration 61/135: Train Loss = 0.2426, Valid Loss = 0.2628\n",
      "Iteration 62/135: Train Loss = 0.2419, Valid Loss = 0.2624\n",
      "Iteration 63/135: Train Loss = 0.2411, Valid Loss = 0.2618\n",
      "Iteration 64/135: Train Loss = 0.2404, Valid Loss = 0.2613\n",
      "Iteration 65/135: Train Loss = 0.2397, Valid Loss = 0.2609\n",
      "Iteration 66/135: Train Loss = 0.2391, Valid Loss = 0.2605\n",
      "Iteration 67/135: Train Loss = 0.2383, Valid Loss = 0.2597\n",
      "Iteration 68/135: Train Loss = 0.2377, Valid Loss = 0.2590\n",
      "Iteration 69/135: Train Loss = 0.2371, Valid Loss = 0.2586\n",
      "Iteration 70/135: Train Loss = 0.2365, Valid Loss = 0.2581\n",
      "Iteration 71/135: Train Loss = 0.2360, Valid Loss = 0.2579\n",
      "Iteration 72/135: Train Loss = 0.2353, Valid Loss = 0.2577\n",
      "Iteration 73/135: Train Loss = 0.2348, Valid Loss = 0.2575\n",
      "Iteration 74/135: Train Loss = 0.2341, Valid Loss = 0.2570\n",
      "Iteration 75/135: Train Loss = 0.2337, Valid Loss = 0.2566\n",
      "Iteration 76/135: Train Loss = 0.2332, Valid Loss = 0.2561\n",
      "Iteration 77/135: Train Loss = 0.2327, Valid Loss = 0.2557\n",
      "Iteration 78/135: Train Loss = 0.2322, Valid Loss = 0.2553\n",
      "Iteration 79/135: Train Loss = 0.2317, Valid Loss = 0.2548\n",
      "Iteration 80/135: Train Loss = 0.2310, Valid Loss = 0.2544\n",
      "Iteration 81/135: Train Loss = 0.2306, Valid Loss = 0.2540\n",
      "Iteration 82/135: Train Loss = 0.2299, Valid Loss = 0.2533\n",
      "Iteration 83/135: Train Loss = 0.2295, Valid Loss = 0.2530\n",
      "Iteration 84/135: Train Loss = 0.2290, Valid Loss = 0.2525\n",
      "Iteration 85/135: Train Loss = 0.2285, Valid Loss = 0.2523\n",
      "Iteration 86/135: Train Loss = 0.2281, Valid Loss = 0.2520\n",
      "Iteration 87/135: Train Loss = 0.2276, Valid Loss = 0.2516\n",
      "Iteration 88/135: Train Loss = 0.2272, Valid Loss = 0.2512\n",
      "Iteration 89/135: Train Loss = 0.2268, Valid Loss = 0.2509\n",
      "Iteration 90/135: Train Loss = 0.2263, Valid Loss = 0.2505\n",
      "Iteration 91/135: Train Loss = 0.2259, Valid Loss = 0.2501\n",
      "Iteration 92/135: Train Loss = 0.2255, Valid Loss = 0.2500\n",
      "Iteration 93/135: Train Loss = 0.2251, Valid Loss = 0.2496\n",
      "Iteration 94/135: Train Loss = 0.2246, Valid Loss = 0.2495\n",
      "Iteration 95/135: Train Loss = 0.2243, Valid Loss = 0.2494\n",
      "Iteration 96/135: Train Loss = 0.2239, Valid Loss = 0.2491\n",
      "Iteration 97/135: Train Loss = 0.2235, Valid Loss = 0.2489\n",
      "Iteration 98/135: Train Loss = 0.2231, Valid Loss = 0.2484\n",
      "Iteration 99/135: Train Loss = 0.2228, Valid Loss = 0.2483\n",
      "Iteration 100/135: Train Loss = 0.2224, Valid Loss = 0.2478\n",
      "Iteration 101/135: Train Loss = 0.2221, Valid Loss = 0.2477\n",
      "Iteration 102/135: Train Loss = 0.2218, Valid Loss = 0.2474\n",
      "Iteration 103/135: Train Loss = 0.2214, Valid Loss = 0.2473\n",
      "Iteration 104/135: Train Loss = 0.2210, Valid Loss = 0.2472\n",
      "Iteration 105/135: Train Loss = 0.2208, Valid Loss = 0.2470\n",
      "Iteration 106/135: Train Loss = 0.2203, Valid Loss = 0.2470\n",
      "Iteration 107/135: Train Loss = 0.2200, Valid Loss = 0.2468\n",
      "Iteration 108/135: Train Loss = 0.2197, Valid Loss = 0.2465\n",
      "Iteration 109/135: Train Loss = 0.2195, Valid Loss = 0.2464\n",
      "Iteration 110/135: Train Loss = 0.2192, Valid Loss = 0.2461\n",
      "Iteration 111/135: Train Loss = 0.2187, Valid Loss = 0.2457\n",
      "Iteration 112/135: Train Loss = 0.2184, Valid Loss = 0.2454\n",
      "Iteration 113/135: Train Loss = 0.2181, Valid Loss = 0.2454\n",
      "Iteration 114/135: Train Loss = 0.2178, Valid Loss = 0.2454\n",
      "Iteration 115/135: Train Loss = 0.2176, Valid Loss = 0.2451\n",
      "Iteration 116/135: Train Loss = 0.2174, Valid Loss = 0.2451\n",
      "Iteration 117/135: Train Loss = 0.2171, Valid Loss = 0.2449\n",
      "Iteration 118/135: Train Loss = 0.2168, Valid Loss = 0.2449\n",
      "Iteration 119/135: Train Loss = 0.2165, Valid Loss = 0.2446\n",
      "Iteration 120/135: Train Loss = 0.2163, Valid Loss = 0.2444\n",
      "Iteration 121/135: Train Loss = 0.2161, Valid Loss = 0.2443\n",
      "Iteration 122/135: Train Loss = 0.2158, Valid Loss = 0.2444\n",
      "Iteration 123/135: Train Loss = 0.2156, Valid Loss = 0.2441\n",
      "Iteration 124/135: Train Loss = 0.2152, Valid Loss = 0.2439\n",
      "Iteration 125/135: Train Loss = 0.2150, Valid Loss = 0.2439\n",
      "Iteration 126/135: Train Loss = 0.2147, Valid Loss = 0.2436\n",
      "Iteration 127/135: Train Loss = 0.2144, Valid Loss = 0.2436\n",
      "Iteration 128/135: Train Loss = 0.2143, Valid Loss = 0.2434\n",
      "Iteration 129/135: Train Loss = 0.2140, Valid Loss = 0.2433\n",
      "Iteration 130/135: Train Loss = 0.2138, Valid Loss = 0.2431\n",
      "Iteration 131/135: Train Loss = 0.2136, Valid Loss = 0.2428\n",
      "Iteration 132/135: Train Loss = 0.2132, Valid Loss = 0.2427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:52:01,991] Trial 38 finished with value: 0.9651956578932318 and parameters: {'max_depth': 5, 'min_samples_leaf': 16, 'n_estimators': 135, 'learning_rate': 0.39515783736370624, 'subsample': 0.6034108461987672}. Best is trial 27 with value: 0.9657550072635349.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 133/135: Train Loss = 0.2130, Valid Loss = 0.2426\n",
      "Iteration 134/135: Train Loss = 0.2128, Valid Loss = 0.2425\n",
      "Iteration 135/135: Train Loss = 0.2125, Valid Loss = 0.2423\n",
      "Iteration 1/135: Train Loss = 0.6810, Valid Loss = 0.6811\n",
      "Iteration 2/135: Train Loss = 0.6694, Valid Loss = 0.6698\n",
      "Iteration 3/135: Train Loss = 0.6583, Valid Loss = 0.6589\n",
      "Iteration 4/135: Train Loss = 0.6478, Valid Loss = 0.6486\n",
      "Iteration 5/135: Train Loss = 0.6374, Valid Loss = 0.6385\n",
      "Iteration 6/135: Train Loss = 0.6277, Valid Loss = 0.6291\n",
      "Iteration 7/135: Train Loss = 0.6182, Valid Loss = 0.6198\n",
      "Iteration 8/135: Train Loss = 0.6091, Valid Loss = 0.6111\n",
      "Iteration 9/135: Train Loss = 0.6003, Valid Loss = 0.6024\n",
      "Iteration 10/135: Train Loss = 0.5920, Valid Loss = 0.5942\n",
      "Iteration 11/135: Train Loss = 0.5839, Valid Loss = 0.5864\n",
      "Iteration 12/135: Train Loss = 0.5760, Valid Loss = 0.5787\n",
      "Iteration 13/135: Train Loss = 0.5686, Valid Loss = 0.5713\n",
      "Iteration 14/135: Train Loss = 0.5613, Valid Loss = 0.5643\n",
      "Iteration 15/135: Train Loss = 0.5544, Valid Loss = 0.5574\n",
      "Iteration 16/135: Train Loss = 0.5478, Valid Loss = 0.5509\n",
      "Iteration 17/135: Train Loss = 0.5413, Valid Loss = 0.5446\n",
      "Iteration 18/135: Train Loss = 0.5351, Valid Loss = 0.5385\n",
      "Iteration 19/135: Train Loss = 0.5289, Valid Loss = 0.5326\n",
      "Iteration 20/135: Train Loss = 0.5231, Valid Loss = 0.5269\n",
      "Iteration 21/135: Train Loss = 0.5175, Valid Loss = 0.5215\n",
      "Iteration 22/135: Train Loss = 0.5122, Valid Loss = 0.5164\n",
      "Iteration 23/135: Train Loss = 0.5070, Valid Loss = 0.5112\n",
      "Iteration 24/135: Train Loss = 0.5019, Valid Loss = 0.5063\n",
      "Iteration 25/135: Train Loss = 0.4968, Valid Loss = 0.5014\n",
      "Iteration 26/135: Train Loss = 0.4922, Valid Loss = 0.4970\n",
      "Iteration 27/135: Train Loss = 0.4877, Valid Loss = 0.4926\n",
      "Iteration 28/135: Train Loss = 0.4833, Valid Loss = 0.4883\n",
      "Iteration 29/135: Train Loss = 0.4789, Valid Loss = 0.4840\n",
      "Iteration 30/135: Train Loss = 0.4749, Valid Loss = 0.4801\n",
      "Iteration 31/135: Train Loss = 0.4709, Valid Loss = 0.4761\n",
      "Iteration 32/135: Train Loss = 0.4669, Valid Loss = 0.4723\n",
      "Iteration 33/135: Train Loss = 0.4630, Valid Loss = 0.4685\n",
      "Iteration 34/135: Train Loss = 0.4594, Valid Loss = 0.4649\n",
      "Iteration 35/135: Train Loss = 0.4559, Valid Loss = 0.4614\n",
      "Iteration 36/135: Train Loss = 0.4523, Valid Loss = 0.4579\n",
      "Iteration 37/135: Train Loss = 0.4490, Valid Loss = 0.4547\n",
      "Iteration 38/135: Train Loss = 0.4458, Valid Loss = 0.4515\n",
      "Iteration 39/135: Train Loss = 0.4426, Valid Loss = 0.4483\n",
      "Iteration 40/135: Train Loss = 0.4394, Valid Loss = 0.4453\n",
      "Iteration 41/135: Train Loss = 0.4363, Valid Loss = 0.4422\n",
      "Iteration 42/135: Train Loss = 0.4335, Valid Loss = 0.4393\n",
      "Iteration 43/135: Train Loss = 0.4307, Valid Loss = 0.4367\n",
      "Iteration 44/135: Train Loss = 0.4280, Valid Loss = 0.4340\n",
      "Iteration 45/135: Train Loss = 0.4253, Valid Loss = 0.4314\n",
      "Iteration 46/135: Train Loss = 0.4226, Valid Loss = 0.4288\n",
      "Iteration 47/135: Train Loss = 0.4200, Valid Loss = 0.4264\n",
      "Iteration 48/135: Train Loss = 0.4176, Valid Loss = 0.4239\n",
      "Iteration 49/135: Train Loss = 0.4151, Valid Loss = 0.4214\n",
      "Iteration 50/135: Train Loss = 0.4127, Valid Loss = 0.4191\n",
      "Iteration 51/135: Train Loss = 0.4105, Valid Loss = 0.4169\n",
      "Iteration 52/135: Train Loss = 0.4083, Valid Loss = 0.4148\n",
      "Iteration 53/135: Train Loss = 0.4061, Valid Loss = 0.4127\n",
      "Iteration 54/135: Train Loss = 0.4040, Valid Loss = 0.4107\n",
      "Iteration 55/135: Train Loss = 0.4020, Valid Loss = 0.4086\n",
      "Iteration 56/135: Train Loss = 0.4000, Valid Loss = 0.4067\n",
      "Iteration 57/135: Train Loss = 0.3980, Valid Loss = 0.4048\n",
      "Iteration 58/135: Train Loss = 0.3961, Valid Loss = 0.4029\n",
      "Iteration 59/135: Train Loss = 0.3942, Valid Loss = 0.4010\n",
      "Iteration 60/135: Train Loss = 0.3923, Valid Loss = 0.3992\n",
      "Iteration 61/135: Train Loss = 0.3906, Valid Loss = 0.3975\n",
      "Iteration 62/135: Train Loss = 0.3888, Valid Loss = 0.3958\n",
      "Iteration 63/135: Train Loss = 0.3871, Valid Loss = 0.3941\n",
      "Iteration 64/135: Train Loss = 0.3854, Valid Loss = 0.3923\n",
      "Iteration 65/135: Train Loss = 0.3837, Valid Loss = 0.3906\n",
      "Iteration 66/135: Train Loss = 0.3821, Valid Loss = 0.3890\n",
      "Iteration 67/135: Train Loss = 0.3804, Valid Loss = 0.3873\n",
      "Iteration 68/135: Train Loss = 0.3789, Valid Loss = 0.3859\n",
      "Iteration 69/135: Train Loss = 0.3774, Valid Loss = 0.3844\n",
      "Iteration 70/135: Train Loss = 0.3760, Valid Loss = 0.3831\n",
      "Iteration 71/135: Train Loss = 0.3746, Valid Loss = 0.3817\n",
      "Iteration 72/135: Train Loss = 0.3732, Valid Loss = 0.3803\n",
      "Iteration 73/135: Train Loss = 0.3717, Valid Loss = 0.3789\n",
      "Iteration 74/135: Train Loss = 0.3704, Valid Loss = 0.3776\n",
      "Iteration 75/135: Train Loss = 0.3690, Valid Loss = 0.3762\n",
      "Iteration 76/135: Train Loss = 0.3677, Valid Loss = 0.3750\n",
      "Iteration 77/135: Train Loss = 0.3664, Valid Loss = 0.3738\n",
      "Iteration 78/135: Train Loss = 0.3651, Valid Loss = 0.3725\n",
      "Iteration 79/135: Train Loss = 0.3639, Valid Loss = 0.3713\n",
      "Iteration 80/135: Train Loss = 0.3626, Valid Loss = 0.3700\n",
      "Iteration 81/135: Train Loss = 0.3614, Valid Loss = 0.3689\n",
      "Iteration 82/135: Train Loss = 0.3603, Valid Loss = 0.3677\n",
      "Iteration 83/135: Train Loss = 0.3592, Valid Loss = 0.3667\n",
      "Iteration 84/135: Train Loss = 0.3580, Valid Loss = 0.3655\n",
      "Iteration 85/135: Train Loss = 0.3569, Valid Loss = 0.3643\n",
      "Iteration 86/135: Train Loss = 0.3557, Valid Loss = 0.3633\n",
      "Iteration 87/135: Train Loss = 0.3546, Valid Loss = 0.3621\n",
      "Iteration 88/135: Train Loss = 0.3535, Valid Loss = 0.3610\n",
      "Iteration 89/135: Train Loss = 0.3525, Valid Loss = 0.3600\n",
      "Iteration 90/135: Train Loss = 0.3515, Valid Loss = 0.3590\n",
      "Iteration 91/135: Train Loss = 0.3505, Valid Loss = 0.3579\n",
      "Iteration 92/135: Train Loss = 0.3495, Valid Loss = 0.3569\n",
      "Iteration 93/135: Train Loss = 0.3486, Valid Loss = 0.3560\n",
      "Iteration 94/135: Train Loss = 0.3476, Valid Loss = 0.3550\n",
      "Iteration 95/135: Train Loss = 0.3467, Valid Loss = 0.3542\n",
      "Iteration 96/135: Train Loss = 0.3457, Valid Loss = 0.3533\n",
      "Iteration 97/135: Train Loss = 0.3448, Valid Loss = 0.3524\n",
      "Iteration 98/135: Train Loss = 0.3438, Valid Loss = 0.3515\n",
      "Iteration 99/135: Train Loss = 0.3429, Valid Loss = 0.3506\n",
      "Iteration 100/135: Train Loss = 0.3421, Valid Loss = 0.3497\n",
      "Iteration 101/135: Train Loss = 0.3412, Valid Loss = 0.3489\n",
      "Iteration 102/135: Train Loss = 0.3404, Valid Loss = 0.3481\n",
      "Iteration 103/135: Train Loss = 0.3395, Valid Loss = 0.3473\n",
      "Iteration 104/135: Train Loss = 0.3387, Valid Loss = 0.3465\n",
      "Iteration 105/135: Train Loss = 0.3378, Valid Loss = 0.3457\n",
      "Iteration 106/135: Train Loss = 0.3371, Valid Loss = 0.3451\n",
      "Iteration 107/135: Train Loss = 0.3364, Valid Loss = 0.3443\n",
      "Iteration 108/135: Train Loss = 0.3357, Valid Loss = 0.3437\n",
      "Iteration 109/135: Train Loss = 0.3349, Valid Loss = 0.3430\n",
      "Iteration 110/135: Train Loss = 0.3342, Valid Loss = 0.3423\n",
      "Iteration 111/135: Train Loss = 0.3334, Valid Loss = 0.3417\n",
      "Iteration 112/135: Train Loss = 0.3327, Valid Loss = 0.3410\n",
      "Iteration 113/135: Train Loss = 0.3320, Valid Loss = 0.3403\n",
      "Iteration 114/135: Train Loss = 0.3312, Valid Loss = 0.3395\n",
      "Iteration 115/135: Train Loss = 0.3306, Valid Loss = 0.3388\n",
      "Iteration 116/135: Train Loss = 0.3298, Valid Loss = 0.3382\n",
      "Iteration 117/135: Train Loss = 0.3291, Valid Loss = 0.3375\n",
      "Iteration 118/135: Train Loss = 0.3285, Valid Loss = 0.3369\n",
      "Iteration 119/135: Train Loss = 0.3278, Valid Loss = 0.3363\n",
      "Iteration 120/135: Train Loss = 0.3272, Valid Loss = 0.3357\n",
      "Iteration 121/135: Train Loss = 0.3265, Valid Loss = 0.3350\n",
      "Iteration 122/135: Train Loss = 0.3259, Valid Loss = 0.3343\n",
      "Iteration 123/135: Train Loss = 0.3253, Valid Loss = 0.3337\n",
      "Iteration 124/135: Train Loss = 0.3247, Valid Loss = 0.3331\n",
      "Iteration 125/135: Train Loss = 0.3241, Valid Loss = 0.3325\n",
      "Iteration 126/135: Train Loss = 0.3235, Valid Loss = 0.3320\n",
      "Iteration 127/135: Train Loss = 0.3229, Valid Loss = 0.3313\n",
      "Iteration 128/135: Train Loss = 0.3223, Valid Loss = 0.3307\n",
      "Iteration 129/135: Train Loss = 0.3217, Valid Loss = 0.3301\n",
      "Iteration 130/135: Train Loss = 0.3211, Valid Loss = 0.3296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:52:05,829] Trial 39 finished with value: 0.949228845193294 and parameters: {'max_depth': 3, 'min_samples_leaf': 18, 'n_estimators': 135, 'learning_rate': 0.08390500031772391, 'subsample': 0.6088693565173435}. Best is trial 27 with value: 0.9657550072635349.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 131/135: Train Loss = 0.3205, Valid Loss = 0.3290\n",
      "Iteration 132/135: Train Loss = 0.3199, Valid Loss = 0.3284\n",
      "Iteration 133/135: Train Loss = 0.3194, Valid Loss = 0.3278\n",
      "Iteration 134/135: Train Loss = 0.3189, Valid Loss = 0.3273\n",
      "Iteration 135/135: Train Loss = 0.3183, Valid Loss = 0.3268\n",
      "Iteration 1/145: Train Loss = 0.6853, Valid Loss = 0.6855\n",
      "Iteration 2/145: Train Loss = 0.6777, Valid Loss = 0.6781\n",
      "Iteration 3/145: Train Loss = 0.6701, Valid Loss = 0.6707\n",
      "Iteration 4/145: Train Loss = 0.6628, Valid Loss = 0.6636\n",
      "Iteration 5/145: Train Loss = 0.6557, Valid Loss = 0.6566\n",
      "Iteration 6/145: Train Loss = 0.6488, Valid Loss = 0.6498\n",
      "Iteration 7/145: Train Loss = 0.6420, Valid Loss = 0.6432\n",
      "Iteration 8/145: Train Loss = 0.6353, Valid Loss = 0.6367\n",
      "Iteration 9/145: Train Loss = 0.6288, Valid Loss = 0.6302\n",
      "Iteration 10/145: Train Loss = 0.6225, Valid Loss = 0.6241\n",
      "Iteration 11/145: Train Loss = 0.6163, Valid Loss = 0.6179\n",
      "Iteration 12/145: Train Loss = 0.6103, Valid Loss = 0.6120\n",
      "Iteration 13/145: Train Loss = 0.6045, Valid Loss = 0.6063\n",
      "Iteration 14/145: Train Loss = 0.5987, Valid Loss = 0.6008\n",
      "Iteration 15/145: Train Loss = 0.5931, Valid Loss = 0.5952\n",
      "Iteration 16/145: Train Loss = 0.5877, Valid Loss = 0.5900\n",
      "Iteration 17/145: Train Loss = 0.5822, Valid Loss = 0.5848\n",
      "Iteration 18/145: Train Loss = 0.5769, Valid Loss = 0.5796\n",
      "Iteration 19/145: Train Loss = 0.5717, Valid Loss = 0.5744\n",
      "Iteration 20/145: Train Loss = 0.5668, Valid Loss = 0.5696\n",
      "Iteration 21/145: Train Loss = 0.5619, Valid Loss = 0.5648\n",
      "Iteration 22/145: Train Loss = 0.5570, Valid Loss = 0.5601\n",
      "Iteration 23/145: Train Loss = 0.5524, Valid Loss = 0.5554\n",
      "Iteration 24/145: Train Loss = 0.5478, Valid Loss = 0.5510\n",
      "Iteration 25/145: Train Loss = 0.5433, Valid Loss = 0.5466\n",
      "Iteration 26/145: Train Loss = 0.5389, Valid Loss = 0.5423\n",
      "Iteration 27/145: Train Loss = 0.5346, Valid Loss = 0.5381\n",
      "Iteration 28/145: Train Loss = 0.5304, Valid Loss = 0.5340\n",
      "Iteration 29/145: Train Loss = 0.5263, Valid Loss = 0.5300\n",
      "Iteration 30/145: Train Loss = 0.5222, Valid Loss = 0.5260\n",
      "Iteration 31/145: Train Loss = 0.5182, Valid Loss = 0.5221\n",
      "Iteration 32/145: Train Loss = 0.5144, Valid Loss = 0.5185\n",
      "Iteration 33/145: Train Loss = 0.5106, Valid Loss = 0.5147\n",
      "Iteration 34/145: Train Loss = 0.5068, Valid Loss = 0.5110\n",
      "Iteration 35/145: Train Loss = 0.5033, Valid Loss = 0.5075\n",
      "Iteration 36/145: Train Loss = 0.4997, Valid Loss = 0.5041\n",
      "Iteration 37/145: Train Loss = 0.4962, Valid Loss = 0.5008\n",
      "Iteration 38/145: Train Loss = 0.4928, Valid Loss = 0.4975\n",
      "Iteration 39/145: Train Loss = 0.4894, Valid Loss = 0.4942\n",
      "Iteration 40/145: Train Loss = 0.4860, Valid Loss = 0.4910\n",
      "Iteration 41/145: Train Loss = 0.4828, Valid Loss = 0.4878\n",
      "Iteration 42/145: Train Loss = 0.4797, Valid Loss = 0.4848\n",
      "Iteration 43/145: Train Loss = 0.4766, Valid Loss = 0.4819\n",
      "Iteration 44/145: Train Loss = 0.4736, Valid Loss = 0.4790\n",
      "Iteration 45/145: Train Loss = 0.4707, Valid Loss = 0.4761\n",
      "Iteration 46/145: Train Loss = 0.4678, Valid Loss = 0.4733\n",
      "Iteration 47/145: Train Loss = 0.4648, Valid Loss = 0.4704\n",
      "Iteration 48/145: Train Loss = 0.4621, Valid Loss = 0.4677\n",
      "Iteration 49/145: Train Loss = 0.4594, Valid Loss = 0.4651\n",
      "Iteration 50/145: Train Loss = 0.4567, Valid Loss = 0.4625\n",
      "Iteration 51/145: Train Loss = 0.4541, Valid Loss = 0.4600\n",
      "Iteration 52/145: Train Loss = 0.4515, Valid Loss = 0.4575\n",
      "Iteration 53/145: Train Loss = 0.4490, Valid Loss = 0.4551\n",
      "Iteration 54/145: Train Loss = 0.4465, Valid Loss = 0.4526\n",
      "Iteration 55/145: Train Loss = 0.4440, Valid Loss = 0.4502\n",
      "Iteration 56/145: Train Loss = 0.4416, Valid Loss = 0.4478\n",
      "Iteration 57/145: Train Loss = 0.4392, Valid Loss = 0.4456\n",
      "Iteration 58/145: Train Loss = 0.4369, Valid Loss = 0.4434\n",
      "Iteration 59/145: Train Loss = 0.4347, Valid Loss = 0.4412\n",
      "Iteration 60/145: Train Loss = 0.4325, Valid Loss = 0.4390\n",
      "Iteration 61/145: Train Loss = 0.4303, Valid Loss = 0.4370\n",
      "Iteration 62/145: Train Loss = 0.4282, Valid Loss = 0.4349\n",
      "Iteration 63/145: Train Loss = 0.4261, Valid Loss = 0.4329\n",
      "Iteration 64/145: Train Loss = 0.4240, Valid Loss = 0.4309\n",
      "Iteration 65/145: Train Loss = 0.4220, Valid Loss = 0.4289\n",
      "Iteration 66/145: Train Loss = 0.4200, Valid Loss = 0.4269\n",
      "Iteration 67/145: Train Loss = 0.4179, Valid Loss = 0.4250\n",
      "Iteration 68/145: Train Loss = 0.4160, Valid Loss = 0.4232\n",
      "Iteration 69/145: Train Loss = 0.4141, Valid Loss = 0.4213\n",
      "Iteration 70/145: Train Loss = 0.4122, Valid Loss = 0.4195\n",
      "Iteration 71/145: Train Loss = 0.4104, Valid Loss = 0.4178\n",
      "Iteration 72/145: Train Loss = 0.4086, Valid Loss = 0.4161\n",
      "Iteration 73/145: Train Loss = 0.4068, Valid Loss = 0.4143\n",
      "Iteration 74/145: Train Loss = 0.4051, Valid Loss = 0.4126\n",
      "Iteration 75/145: Train Loss = 0.4034, Valid Loss = 0.4110\n",
      "Iteration 76/145: Train Loss = 0.4016, Valid Loss = 0.4093\n",
      "Iteration 77/145: Train Loss = 0.4000, Valid Loss = 0.4077\n",
      "Iteration 78/145: Train Loss = 0.3984, Valid Loss = 0.4062\n",
      "Iteration 79/145: Train Loss = 0.3968, Valid Loss = 0.4047\n",
      "Iteration 80/145: Train Loss = 0.3952, Valid Loss = 0.4031\n",
      "Iteration 81/145: Train Loss = 0.3937, Valid Loss = 0.4016\n",
      "Iteration 82/145: Train Loss = 0.3921, Valid Loss = 0.4001\n",
      "Iteration 83/145: Train Loss = 0.3906, Valid Loss = 0.3986\n",
      "Iteration 84/145: Train Loss = 0.3891, Valid Loss = 0.3971\n",
      "Iteration 85/145: Train Loss = 0.3876, Valid Loss = 0.3956\n",
      "Iteration 86/145: Train Loss = 0.3861, Valid Loss = 0.3941\n",
      "Iteration 87/145: Train Loss = 0.3847, Valid Loss = 0.3928\n",
      "Iteration 88/145: Train Loss = 0.3832, Valid Loss = 0.3914\n",
      "Iteration 89/145: Train Loss = 0.3819, Valid Loss = 0.3901\n",
      "Iteration 90/145: Train Loss = 0.3805, Valid Loss = 0.3888\n",
      "Iteration 91/145: Train Loss = 0.3792, Valid Loss = 0.3875\n",
      "Iteration 92/145: Train Loss = 0.3779, Valid Loss = 0.3863\n",
      "Iteration 93/145: Train Loss = 0.3766, Valid Loss = 0.3851\n",
      "Iteration 94/145: Train Loss = 0.3753, Valid Loss = 0.3839\n",
      "Iteration 95/145: Train Loss = 0.3740, Valid Loss = 0.3826\n",
      "Iteration 96/145: Train Loss = 0.3728, Valid Loss = 0.3815\n",
      "Iteration 97/145: Train Loss = 0.3715, Valid Loss = 0.3803\n",
      "Iteration 98/145: Train Loss = 0.3703, Valid Loss = 0.3791\n",
      "Iteration 99/145: Train Loss = 0.3691, Valid Loss = 0.3779\n",
      "Iteration 100/145: Train Loss = 0.3679, Valid Loss = 0.3768\n",
      "Iteration 101/145: Train Loss = 0.3667, Valid Loss = 0.3756\n",
      "Iteration 102/145: Train Loss = 0.3656, Valid Loss = 0.3745\n",
      "Iteration 103/145: Train Loss = 0.3644, Valid Loss = 0.3734\n",
      "Iteration 104/145: Train Loss = 0.3633, Valid Loss = 0.3723\n",
      "Iteration 105/145: Train Loss = 0.3622, Valid Loss = 0.3713\n",
      "Iteration 106/145: Train Loss = 0.3611, Valid Loss = 0.3702\n",
      "Iteration 107/145: Train Loss = 0.3600, Valid Loss = 0.3692\n",
      "Iteration 108/145: Train Loss = 0.3589, Valid Loss = 0.3682\n",
      "Iteration 109/145: Train Loss = 0.3579, Valid Loss = 0.3671\n",
      "Iteration 110/145: Train Loss = 0.3568, Valid Loss = 0.3661\n",
      "Iteration 111/145: Train Loss = 0.3557, Valid Loss = 0.3651\n",
      "Iteration 112/145: Train Loss = 0.3547, Valid Loss = 0.3642\n",
      "Iteration 113/145: Train Loss = 0.3538, Valid Loss = 0.3632\n",
      "Iteration 114/145: Train Loss = 0.3528, Valid Loss = 0.3623\n",
      "Iteration 115/145: Train Loss = 0.3519, Valid Loss = 0.3615\n",
      "Iteration 116/145: Train Loss = 0.3509, Valid Loss = 0.3605\n",
      "Iteration 117/145: Train Loss = 0.3500, Valid Loss = 0.3597\n",
      "Iteration 118/145: Train Loss = 0.3491, Valid Loss = 0.3587\n",
      "Iteration 119/145: Train Loss = 0.3481, Valid Loss = 0.3578\n",
      "Iteration 120/145: Train Loss = 0.3472, Valid Loss = 0.3569\n",
      "Iteration 121/145: Train Loss = 0.3463, Valid Loss = 0.3561\n",
      "Iteration 122/145: Train Loss = 0.3454, Valid Loss = 0.3553\n",
      "Iteration 123/145: Train Loss = 0.3445, Valid Loss = 0.3544\n",
      "Iteration 124/145: Train Loss = 0.3437, Valid Loss = 0.3536\n",
      "Iteration 125/145: Train Loss = 0.3428, Valid Loss = 0.3527\n",
      "Iteration 126/145: Train Loss = 0.3420, Valid Loss = 0.3520\n",
      "Iteration 127/145: Train Loss = 0.3411, Valid Loss = 0.3511\n",
      "Iteration 128/145: Train Loss = 0.3402, Valid Loss = 0.3503\n",
      "Iteration 129/145: Train Loss = 0.3394, Valid Loss = 0.3495\n",
      "Iteration 130/145: Train Loss = 0.3385, Valid Loss = 0.3487\n",
      "Iteration 131/145: Train Loss = 0.3377, Valid Loss = 0.3480\n",
      "Iteration 132/145: Train Loss = 0.3369, Valid Loss = 0.3472\n",
      "Iteration 133/145: Train Loss = 0.3361, Valid Loss = 0.3464\n",
      "Iteration 134/145: Train Loss = 0.3354, Valid Loss = 0.3458\n",
      "Iteration 135/145: Train Loss = 0.3346, Valid Loss = 0.3450\n",
      "Iteration 136/145: Train Loss = 0.3339, Valid Loss = 0.3443\n",
      "Iteration 137/145: Train Loss = 0.3331, Valid Loss = 0.3435\n",
      "Iteration 138/145: Train Loss = 0.3324, Valid Loss = 0.3429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:52:10,688] Trial 40 finished with value: 0.9560760135585983 and parameters: {'max_depth': 5, 'min_samples_leaf': 14, 'n_estimators': 145, 'learning_rate': 0.04900461631178423, 'subsample': 0.47153096958979973}. Best is trial 27 with value: 0.9657550072635349.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 139/145: Train Loss = 0.3316, Valid Loss = 0.3422\n",
      "Iteration 140/145: Train Loss = 0.3309, Valid Loss = 0.3415\n",
      "Iteration 141/145: Train Loss = 0.3302, Valid Loss = 0.3409\n",
      "Iteration 142/145: Train Loss = 0.3295, Valid Loss = 0.3403\n",
      "Iteration 143/145: Train Loss = 0.3289, Valid Loss = 0.3396\n",
      "Iteration 144/145: Train Loss = 0.3282, Valid Loss = 0.3390\n",
      "Iteration 145/145: Train Loss = 0.3275, Valid Loss = 0.3383\n",
      "Iteration 1/130: Train Loss = 0.6394, Valid Loss = 0.6403\n",
      "Iteration 2/130: Train Loss = 0.5943, Valid Loss = 0.5963\n",
      "Iteration 3/130: Train Loss = 0.5565, Valid Loss = 0.5589\n",
      "Iteration 4/130: Train Loss = 0.5243, Valid Loss = 0.5279\n",
      "Iteration 5/130: Train Loss = 0.4966, Valid Loss = 0.5011\n",
      "Iteration 6/130: Train Loss = 0.4724, Valid Loss = 0.4777\n",
      "Iteration 7/130: Train Loss = 0.4521, Valid Loss = 0.4584\n",
      "Iteration 8/130: Train Loss = 0.4347, Valid Loss = 0.4422\n",
      "Iteration 9/130: Train Loss = 0.4190, Valid Loss = 0.4269\n",
      "Iteration 10/130: Train Loss = 0.4052, Valid Loss = 0.4136\n",
      "Iteration 11/130: Train Loss = 0.3933, Valid Loss = 0.4021\n",
      "Iteration 12/130: Train Loss = 0.3824, Valid Loss = 0.3919\n",
      "Iteration 13/130: Train Loss = 0.3728, Valid Loss = 0.3825\n",
      "Iteration 14/130: Train Loss = 0.3646, Valid Loss = 0.3744\n",
      "Iteration 15/130: Train Loss = 0.3566, Valid Loss = 0.3668\n",
      "Iteration 16/130: Train Loss = 0.3496, Valid Loss = 0.3598\n",
      "Iteration 17/130: Train Loss = 0.3429, Valid Loss = 0.3531\n",
      "Iteration 18/130: Train Loss = 0.3367, Valid Loss = 0.3470\n",
      "Iteration 19/130: Train Loss = 0.3310, Valid Loss = 0.3417\n",
      "Iteration 20/130: Train Loss = 0.3259, Valid Loss = 0.3366\n",
      "Iteration 21/130: Train Loss = 0.3213, Valid Loss = 0.3320\n",
      "Iteration 22/130: Train Loss = 0.3172, Valid Loss = 0.3278\n",
      "Iteration 23/130: Train Loss = 0.3132, Valid Loss = 0.3239\n",
      "Iteration 24/130: Train Loss = 0.3092, Valid Loss = 0.3205\n",
      "Iteration 25/130: Train Loss = 0.3055, Valid Loss = 0.3172\n",
      "Iteration 26/130: Train Loss = 0.3021, Valid Loss = 0.3137\n",
      "Iteration 27/130: Train Loss = 0.2992, Valid Loss = 0.3111\n",
      "Iteration 28/130: Train Loss = 0.2965, Valid Loss = 0.3088\n",
      "Iteration 29/130: Train Loss = 0.2939, Valid Loss = 0.3069\n",
      "Iteration 30/130: Train Loss = 0.2914, Valid Loss = 0.3046\n",
      "Iteration 31/130: Train Loss = 0.2891, Valid Loss = 0.3022\n",
      "Iteration 32/130: Train Loss = 0.2869, Valid Loss = 0.2999\n",
      "Iteration 33/130: Train Loss = 0.2848, Valid Loss = 0.2977\n",
      "Iteration 34/130: Train Loss = 0.2827, Valid Loss = 0.2957\n",
      "Iteration 35/130: Train Loss = 0.2804, Valid Loss = 0.2938\n",
      "Iteration 36/130: Train Loss = 0.2785, Valid Loss = 0.2919\n",
      "Iteration 37/130: Train Loss = 0.2766, Valid Loss = 0.2900\n",
      "Iteration 38/130: Train Loss = 0.2746, Valid Loss = 0.2883\n",
      "Iteration 39/130: Train Loss = 0.2728, Valid Loss = 0.2865\n",
      "Iteration 40/130: Train Loss = 0.2711, Valid Loss = 0.2852\n",
      "Iteration 41/130: Train Loss = 0.2697, Valid Loss = 0.2838\n",
      "Iteration 42/130: Train Loss = 0.2684, Valid Loss = 0.2826\n",
      "Iteration 43/130: Train Loss = 0.2667, Valid Loss = 0.2809\n",
      "Iteration 44/130: Train Loss = 0.2650, Valid Loss = 0.2795\n",
      "Iteration 45/130: Train Loss = 0.2637, Valid Loss = 0.2784\n",
      "Iteration 46/130: Train Loss = 0.2623, Valid Loss = 0.2774\n",
      "Iteration 47/130: Train Loss = 0.2612, Valid Loss = 0.2765\n",
      "Iteration 48/130: Train Loss = 0.2600, Valid Loss = 0.2754\n",
      "Iteration 49/130: Train Loss = 0.2590, Valid Loss = 0.2743\n",
      "Iteration 50/130: Train Loss = 0.2577, Valid Loss = 0.2735\n",
      "Iteration 51/130: Train Loss = 0.2568, Valid Loss = 0.2727\n",
      "Iteration 52/130: Train Loss = 0.2557, Valid Loss = 0.2717\n",
      "Iteration 53/130: Train Loss = 0.2548, Valid Loss = 0.2707\n",
      "Iteration 54/130: Train Loss = 0.2538, Valid Loss = 0.2701\n",
      "Iteration 55/130: Train Loss = 0.2527, Valid Loss = 0.2692\n",
      "Iteration 56/130: Train Loss = 0.2519, Valid Loss = 0.2684\n",
      "Iteration 57/130: Train Loss = 0.2511, Valid Loss = 0.2677\n",
      "Iteration 58/130: Train Loss = 0.2504, Valid Loss = 0.2673\n",
      "Iteration 59/130: Train Loss = 0.2495, Valid Loss = 0.2664\n",
      "Iteration 60/130: Train Loss = 0.2487, Valid Loss = 0.2657\n",
      "Iteration 61/130: Train Loss = 0.2478, Valid Loss = 0.2649\n",
      "Iteration 62/130: Train Loss = 0.2469, Valid Loss = 0.2642\n",
      "Iteration 63/130: Train Loss = 0.2460, Valid Loss = 0.2633\n",
      "Iteration 64/130: Train Loss = 0.2452, Valid Loss = 0.2627\n",
      "Iteration 65/130: Train Loss = 0.2445, Valid Loss = 0.2622\n",
      "Iteration 66/130: Train Loss = 0.2437, Valid Loss = 0.2618\n",
      "Iteration 67/130: Train Loss = 0.2430, Valid Loss = 0.2614\n",
      "Iteration 68/130: Train Loss = 0.2422, Valid Loss = 0.2607\n",
      "Iteration 69/130: Train Loss = 0.2416, Valid Loss = 0.2602\n",
      "Iteration 70/130: Train Loss = 0.2407, Valid Loss = 0.2597\n",
      "Iteration 71/130: Train Loss = 0.2402, Valid Loss = 0.2592\n",
      "Iteration 72/130: Train Loss = 0.2396, Valid Loss = 0.2588\n",
      "Iteration 73/130: Train Loss = 0.2392, Valid Loss = 0.2584\n",
      "Iteration 74/130: Train Loss = 0.2386, Valid Loss = 0.2581\n",
      "Iteration 75/130: Train Loss = 0.2380, Valid Loss = 0.2576\n",
      "Iteration 76/130: Train Loss = 0.2375, Valid Loss = 0.2572\n",
      "Iteration 77/130: Train Loss = 0.2370, Valid Loss = 0.2568\n",
      "Iteration 78/130: Train Loss = 0.2364, Valid Loss = 0.2564\n",
      "Iteration 79/130: Train Loss = 0.2357, Valid Loss = 0.2560\n",
      "Iteration 80/130: Train Loss = 0.2351, Valid Loss = 0.2555\n",
      "Iteration 81/130: Train Loss = 0.2347, Valid Loss = 0.2554\n",
      "Iteration 82/130: Train Loss = 0.2342, Valid Loss = 0.2549\n",
      "Iteration 83/130: Train Loss = 0.2338, Valid Loss = 0.2545\n",
      "Iteration 84/130: Train Loss = 0.2332, Valid Loss = 0.2541\n",
      "Iteration 85/130: Train Loss = 0.2327, Valid Loss = 0.2538\n",
      "Iteration 86/130: Train Loss = 0.2322, Valid Loss = 0.2534\n",
      "Iteration 87/130: Train Loss = 0.2317, Valid Loss = 0.2530\n",
      "Iteration 88/130: Train Loss = 0.2313, Valid Loss = 0.2528\n",
      "Iteration 89/130: Train Loss = 0.2308, Valid Loss = 0.2524\n",
      "Iteration 90/130: Train Loss = 0.2303, Valid Loss = 0.2522\n",
      "Iteration 91/130: Train Loss = 0.2299, Valid Loss = 0.2519\n",
      "Iteration 92/130: Train Loss = 0.2294, Valid Loss = 0.2516\n",
      "Iteration 93/130: Train Loss = 0.2290, Valid Loss = 0.2514\n",
      "Iteration 94/130: Train Loss = 0.2286, Valid Loss = 0.2510\n",
      "Iteration 95/130: Train Loss = 0.2282, Valid Loss = 0.2507\n",
      "Iteration 96/130: Train Loss = 0.2279, Valid Loss = 0.2504\n",
      "Iteration 97/130: Train Loss = 0.2274, Valid Loss = 0.2502\n",
      "Iteration 98/130: Train Loss = 0.2271, Valid Loss = 0.2499\n",
      "Iteration 99/130: Train Loss = 0.2267, Valid Loss = 0.2496\n",
      "Iteration 100/130: Train Loss = 0.2264, Valid Loss = 0.2495\n",
      "Iteration 101/130: Train Loss = 0.2260, Valid Loss = 0.2491\n",
      "Iteration 102/130: Train Loss = 0.2256, Valid Loss = 0.2488\n",
      "Iteration 103/130: Train Loss = 0.2252, Valid Loss = 0.2487\n",
      "Iteration 104/130: Train Loss = 0.2249, Valid Loss = 0.2486\n",
      "Iteration 105/130: Train Loss = 0.2246, Valid Loss = 0.2484\n",
      "Iteration 106/130: Train Loss = 0.2243, Valid Loss = 0.2482\n",
      "Iteration 107/130: Train Loss = 0.2239, Valid Loss = 0.2480\n",
      "Iteration 108/130: Train Loss = 0.2236, Valid Loss = 0.2476\n",
      "Iteration 109/130: Train Loss = 0.2232, Valid Loss = 0.2473\n",
      "Iteration 110/130: Train Loss = 0.2229, Valid Loss = 0.2470\n",
      "Iteration 111/130: Train Loss = 0.2226, Valid Loss = 0.2468\n",
      "Iteration 112/130: Train Loss = 0.2223, Valid Loss = 0.2465\n",
      "Iteration 113/130: Train Loss = 0.2219, Valid Loss = 0.2464\n",
      "Iteration 114/130: Train Loss = 0.2216, Valid Loss = 0.2461\n",
      "Iteration 115/130: Train Loss = 0.2214, Valid Loss = 0.2461\n",
      "Iteration 116/130: Train Loss = 0.2211, Valid Loss = 0.2459\n",
      "Iteration 117/130: Train Loss = 0.2208, Valid Loss = 0.2458\n",
      "Iteration 118/130: Train Loss = 0.2205, Valid Loss = 0.2456\n",
      "Iteration 119/130: Train Loss = 0.2202, Valid Loss = 0.2455\n",
      "Iteration 120/130: Train Loss = 0.2199, Valid Loss = 0.2453\n",
      "Iteration 121/130: Train Loss = 0.2197, Valid Loss = 0.2452\n",
      "Iteration 122/130: Train Loss = 0.2194, Valid Loss = 0.2450\n",
      "Iteration 123/130: Train Loss = 0.2192, Valid Loss = 0.2448\n",
      "Iteration 124/130: Train Loss = 0.2189, Valid Loss = 0.2447\n",
      "Iteration 125/130: Train Loss = 0.2186, Valid Loss = 0.2445\n",
      "Iteration 126/130: Train Loss = 0.2183, Valid Loss = 0.2444\n",
      "Iteration 127/130: Train Loss = 0.2179, Valid Loss = 0.2441\n",
      "Iteration 128/130: Train Loss = 0.2177, Valid Loss = 0.2439\n",
      "Iteration 129/130: Train Loss = 0.2174, Valid Loss = 0.2440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:52:15,456] Trial 41 finished with value: 0.9650568690924204 and parameters: {'max_depth': 5, 'min_samples_leaf': 16, 'n_estimators': 130, 'learning_rate': 0.352576964413513, 'subsample': 0.5393649127857448}. Best is trial 27 with value: 0.9657550072635349.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 130/130: Train Loss = 0.2172, Valid Loss = 0.2440\n",
      "Iteration 1/110: Train Loss = 0.6674, Valid Loss = 0.6678\n",
      "Iteration 2/110: Train Loss = 0.6434, Valid Loss = 0.6443\n",
      "Iteration 3/110: Train Loss = 0.6213, Valid Loss = 0.6226\n",
      "Iteration 4/110: Train Loss = 0.6007, Valid Loss = 0.6025\n",
      "Iteration 5/110: Train Loss = 0.5820, Valid Loss = 0.5842\n",
      "Iteration 6/110: Train Loss = 0.5644, Valid Loss = 0.5669\n",
      "Iteration 7/110: Train Loss = 0.5482, Valid Loss = 0.5510\n",
      "Iteration 8/110: Train Loss = 0.5329, Valid Loss = 0.5362\n",
      "Iteration 9/110: Train Loss = 0.5187, Valid Loss = 0.5225\n",
      "Iteration 10/110: Train Loss = 0.5055, Valid Loss = 0.5098\n",
      "Iteration 11/110: Train Loss = 0.4934, Valid Loss = 0.4979\n",
      "Iteration 12/110: Train Loss = 0.4819, Valid Loss = 0.4871\n",
      "Iteration 13/110: Train Loss = 0.4712, Valid Loss = 0.4768\n",
      "Iteration 14/110: Train Loss = 0.4613, Valid Loss = 0.4673\n",
      "Iteration 15/110: Train Loss = 0.4517, Valid Loss = 0.4583\n",
      "Iteration 16/110: Train Loss = 0.4429, Valid Loss = 0.4498\n",
      "Iteration 17/110: Train Loss = 0.4348, Valid Loss = 0.4419\n",
      "Iteration 18/110: Train Loss = 0.4272, Valid Loss = 0.4346\n",
      "Iteration 19/110: Train Loss = 0.4197, Valid Loss = 0.4275\n",
      "Iteration 20/110: Train Loss = 0.4128, Valid Loss = 0.4208\n",
      "Iteration 21/110: Train Loss = 0.4062, Valid Loss = 0.4143\n",
      "Iteration 22/110: Train Loss = 0.4001, Valid Loss = 0.4083\n",
      "Iteration 23/110: Train Loss = 0.3941, Valid Loss = 0.4027\n",
      "Iteration 24/110: Train Loss = 0.3888, Valid Loss = 0.3976\n",
      "Iteration 25/110: Train Loss = 0.3836, Valid Loss = 0.3926\n",
      "Iteration 26/110: Train Loss = 0.3786, Valid Loss = 0.3876\n",
      "Iteration 27/110: Train Loss = 0.3739, Valid Loss = 0.3830\n",
      "Iteration 28/110: Train Loss = 0.3694, Valid Loss = 0.3787\n",
      "Iteration 29/110: Train Loss = 0.3650, Valid Loss = 0.3743\n",
      "Iteration 30/110: Train Loss = 0.3609, Valid Loss = 0.3705\n",
      "Iteration 31/110: Train Loss = 0.3570, Valid Loss = 0.3666\n",
      "Iteration 32/110: Train Loss = 0.3534, Valid Loss = 0.3631\n",
      "Iteration 33/110: Train Loss = 0.3498, Valid Loss = 0.3598\n",
      "Iteration 34/110: Train Loss = 0.3463, Valid Loss = 0.3566\n",
      "Iteration 35/110: Train Loss = 0.3430, Valid Loss = 0.3534\n",
      "Iteration 36/110: Train Loss = 0.3399, Valid Loss = 0.3505\n",
      "Iteration 37/110: Train Loss = 0.3370, Valid Loss = 0.3479\n",
      "Iteration 38/110: Train Loss = 0.3339, Valid Loss = 0.3449\n",
      "Iteration 39/110: Train Loss = 0.3311, Valid Loss = 0.3423\n",
      "Iteration 40/110: Train Loss = 0.3285, Valid Loss = 0.3398\n",
      "Iteration 41/110: Train Loss = 0.3259, Valid Loss = 0.3373\n",
      "Iteration 42/110: Train Loss = 0.3234, Valid Loss = 0.3348\n",
      "Iteration 43/110: Train Loss = 0.3210, Valid Loss = 0.3326\n",
      "Iteration 44/110: Train Loss = 0.3188, Valid Loss = 0.3304\n",
      "Iteration 45/110: Train Loss = 0.3166, Valid Loss = 0.3283\n",
      "Iteration 46/110: Train Loss = 0.3144, Valid Loss = 0.3263\n",
      "Iteration 47/110: Train Loss = 0.3122, Valid Loss = 0.3243\n",
      "Iteration 48/110: Train Loss = 0.3102, Valid Loss = 0.3226\n",
      "Iteration 49/110: Train Loss = 0.3082, Valid Loss = 0.3208\n",
      "Iteration 50/110: Train Loss = 0.3064, Valid Loss = 0.3191\n",
      "Iteration 51/110: Train Loss = 0.3045, Valid Loss = 0.3175\n",
      "Iteration 52/110: Train Loss = 0.3027, Valid Loss = 0.3159\n",
      "Iteration 53/110: Train Loss = 0.3011, Valid Loss = 0.3144\n",
      "Iteration 54/110: Train Loss = 0.2995, Valid Loss = 0.3130\n",
      "Iteration 55/110: Train Loss = 0.2979, Valid Loss = 0.3118\n",
      "Iteration 56/110: Train Loss = 0.2964, Valid Loss = 0.3103\n",
      "Iteration 57/110: Train Loss = 0.2948, Valid Loss = 0.3089\n",
      "Iteration 58/110: Train Loss = 0.2933, Valid Loss = 0.3075\n",
      "Iteration 59/110: Train Loss = 0.2919, Valid Loss = 0.3063\n",
      "Iteration 60/110: Train Loss = 0.2905, Valid Loss = 0.3050\n",
      "Iteration 61/110: Train Loss = 0.2891, Valid Loss = 0.3039\n",
      "Iteration 62/110: Train Loss = 0.2878, Valid Loss = 0.3028\n",
      "Iteration 63/110: Train Loss = 0.2865, Valid Loss = 0.3015\n",
      "Iteration 64/110: Train Loss = 0.2852, Valid Loss = 0.3003\n",
      "Iteration 65/110: Train Loss = 0.2839, Valid Loss = 0.2991\n",
      "Iteration 66/110: Train Loss = 0.2827, Valid Loss = 0.2980\n",
      "Iteration 67/110: Train Loss = 0.2815, Valid Loss = 0.2968\n",
      "Iteration 68/110: Train Loss = 0.2804, Valid Loss = 0.2959\n",
      "Iteration 69/110: Train Loss = 0.2793, Valid Loss = 0.2948\n",
      "Iteration 70/110: Train Loss = 0.2782, Valid Loss = 0.2939\n",
      "Iteration 71/110: Train Loss = 0.2770, Valid Loss = 0.2927\n",
      "Iteration 72/110: Train Loss = 0.2761, Valid Loss = 0.2919\n",
      "Iteration 73/110: Train Loss = 0.2751, Valid Loss = 0.2910\n",
      "Iteration 74/110: Train Loss = 0.2741, Valid Loss = 0.2902\n",
      "Iteration 75/110: Train Loss = 0.2732, Valid Loss = 0.2893\n",
      "Iteration 76/110: Train Loss = 0.2721, Valid Loss = 0.2884\n",
      "Iteration 77/110: Train Loss = 0.2712, Valid Loss = 0.2876\n",
      "Iteration 78/110: Train Loss = 0.2702, Valid Loss = 0.2867\n",
      "Iteration 79/110: Train Loss = 0.2694, Valid Loss = 0.2860\n",
      "Iteration 80/110: Train Loss = 0.2685, Valid Loss = 0.2851\n",
      "Iteration 81/110: Train Loss = 0.2677, Valid Loss = 0.2845\n",
      "Iteration 82/110: Train Loss = 0.2669, Valid Loss = 0.2839\n",
      "Iteration 83/110: Train Loss = 0.2659, Valid Loss = 0.2832\n",
      "Iteration 84/110: Train Loss = 0.2652, Valid Loss = 0.2826\n",
      "Iteration 85/110: Train Loss = 0.2644, Valid Loss = 0.2818\n",
      "Iteration 86/110: Train Loss = 0.2636, Valid Loss = 0.2812\n",
      "Iteration 87/110: Train Loss = 0.2629, Valid Loss = 0.2806\n",
      "Iteration 88/110: Train Loss = 0.2622, Valid Loss = 0.2800\n",
      "Iteration 89/110: Train Loss = 0.2615, Valid Loss = 0.2794\n",
      "Iteration 90/110: Train Loss = 0.2609, Valid Loss = 0.2790\n",
      "Iteration 91/110: Train Loss = 0.2602, Valid Loss = 0.2784\n",
      "Iteration 92/110: Train Loss = 0.2594, Valid Loss = 0.2779\n",
      "Iteration 93/110: Train Loss = 0.2588, Valid Loss = 0.2773\n",
      "Iteration 94/110: Train Loss = 0.2582, Valid Loss = 0.2768\n",
      "Iteration 95/110: Train Loss = 0.2575, Valid Loss = 0.2764\n",
      "Iteration 96/110: Train Loss = 0.2569, Valid Loss = 0.2758\n",
      "Iteration 97/110: Train Loss = 0.2564, Valid Loss = 0.2752\n",
      "Iteration 98/110: Train Loss = 0.2557, Valid Loss = 0.2747\n",
      "Iteration 99/110: Train Loss = 0.2551, Valid Loss = 0.2742\n",
      "Iteration 100/110: Train Loss = 0.2545, Valid Loss = 0.2736\n",
      "Iteration 101/110: Train Loss = 0.2539, Valid Loss = 0.2732\n",
      "Iteration 102/110: Train Loss = 0.2533, Valid Loss = 0.2728\n",
      "Iteration 103/110: Train Loss = 0.2528, Valid Loss = 0.2724\n",
      "Iteration 104/110: Train Loss = 0.2523, Valid Loss = 0.2720\n",
      "Iteration 105/110: Train Loss = 0.2517, Valid Loss = 0.2715\n",
      "Iteration 106/110: Train Loss = 0.2512, Valid Loss = 0.2711\n",
      "Iteration 107/110: Train Loss = 0.2505, Valid Loss = 0.2706\n",
      "Iteration 108/110: Train Loss = 0.2501, Valid Loss = 0.2702\n",
      "Iteration 109/110: Train Loss = 0.2496, Valid Loss = 0.2698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:52:19,668] Trial 42 finished with value: 0.9622544029221907 and parameters: {'max_depth': 6, 'min_samples_leaf': 19, 'n_estimators': 110, 'learning_rate': 0.16075249503637187, 'subsample': 0.5434237632208722}. Best is trial 27 with value: 0.9657550072635349.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 110/110: Train Loss = 0.2492, Valid Loss = 0.2693\n",
      "Iteration 1/125: Train Loss = 0.6520, Valid Loss = 0.6532\n",
      "Iteration 2/125: Train Loss = 0.6159, Valid Loss = 0.6182\n",
      "Iteration 3/125: Train Loss = 0.5835, Valid Loss = 0.5870\n",
      "Iteration 4/125: Train Loss = 0.5545, Valid Loss = 0.5587\n",
      "Iteration 5/125: Train Loss = 0.5291, Valid Loss = 0.5339\n",
      "Iteration 6/125: Train Loss = 0.5065, Valid Loss = 0.5122\n",
      "Iteration 7/125: Train Loss = 0.4857, Valid Loss = 0.4924\n",
      "Iteration 8/125: Train Loss = 0.4676, Valid Loss = 0.4750\n",
      "Iteration 9/125: Train Loss = 0.4511, Valid Loss = 0.4592\n",
      "Iteration 10/125: Train Loss = 0.4363, Valid Loss = 0.4448\n",
      "Iteration 11/125: Train Loss = 0.4228, Valid Loss = 0.4319\n",
      "Iteration 12/125: Train Loss = 0.4105, Valid Loss = 0.4204\n",
      "Iteration 13/125: Train Loss = 0.3995, Valid Loss = 0.4098\n",
      "Iteration 14/125: Train Loss = 0.3892, Valid Loss = 0.4000\n",
      "Iteration 15/125: Train Loss = 0.3796, Valid Loss = 0.3912\n",
      "Iteration 16/125: Train Loss = 0.3711, Valid Loss = 0.3830\n",
      "Iteration 17/125: Train Loss = 0.3632, Valid Loss = 0.3755\n",
      "Iteration 18/125: Train Loss = 0.3558, Valid Loss = 0.3688\n",
      "Iteration 19/125: Train Loss = 0.3490, Valid Loss = 0.3628\n",
      "Iteration 20/125: Train Loss = 0.3426, Valid Loss = 0.3571\n",
      "Iteration 21/125: Train Loss = 0.3367, Valid Loss = 0.3516\n",
      "Iteration 22/125: Train Loss = 0.3312, Valid Loss = 0.3465\n",
      "Iteration 23/125: Train Loss = 0.3262, Valid Loss = 0.3417\n",
      "Iteration 24/125: Train Loss = 0.3213, Valid Loss = 0.3373\n",
      "Iteration 25/125: Train Loss = 0.3164, Valid Loss = 0.3330\n",
      "Iteration 26/125: Train Loss = 0.3122, Valid Loss = 0.3293\n",
      "Iteration 27/125: Train Loss = 0.3083, Valid Loss = 0.3256\n",
      "Iteration 28/125: Train Loss = 0.3046, Valid Loss = 0.3225\n",
      "Iteration 29/125: Train Loss = 0.3011, Valid Loss = 0.3194\n",
      "Iteration 30/125: Train Loss = 0.2976, Valid Loss = 0.3163\n",
      "Iteration 31/125: Train Loss = 0.2944, Valid Loss = 0.3136\n",
      "Iteration 32/125: Train Loss = 0.2913, Valid Loss = 0.3108\n",
      "Iteration 33/125: Train Loss = 0.2884, Valid Loss = 0.3083\n",
      "Iteration 34/125: Train Loss = 0.2855, Valid Loss = 0.3055\n",
      "Iteration 35/125: Train Loss = 0.2827, Valid Loss = 0.3032\n",
      "Iteration 36/125: Train Loss = 0.2803, Valid Loss = 0.3010\n",
      "Iteration 37/125: Train Loss = 0.2777, Valid Loss = 0.2985\n",
      "Iteration 38/125: Train Loss = 0.2752, Valid Loss = 0.2965\n",
      "Iteration 39/125: Train Loss = 0.2728, Valid Loss = 0.2948\n",
      "Iteration 40/125: Train Loss = 0.2706, Valid Loss = 0.2930\n",
      "Iteration 41/125: Train Loss = 0.2684, Valid Loss = 0.2913\n",
      "Iteration 42/125: Train Loss = 0.2664, Valid Loss = 0.2898\n",
      "Iteration 43/125: Train Loss = 0.2646, Valid Loss = 0.2882\n",
      "Iteration 44/125: Train Loss = 0.2627, Valid Loss = 0.2867\n",
      "Iteration 45/125: Train Loss = 0.2610, Valid Loss = 0.2851\n",
      "Iteration 46/125: Train Loss = 0.2593, Valid Loss = 0.2837\n",
      "Iteration 47/125: Train Loss = 0.2576, Valid Loss = 0.2824\n",
      "Iteration 48/125: Train Loss = 0.2560, Valid Loss = 0.2812\n",
      "Iteration 49/125: Train Loss = 0.2545, Valid Loss = 0.2800\n",
      "Iteration 50/125: Train Loss = 0.2530, Valid Loss = 0.2788\n",
      "Iteration 51/125: Train Loss = 0.2516, Valid Loss = 0.2780\n",
      "Iteration 52/125: Train Loss = 0.2502, Valid Loss = 0.2767\n",
      "Iteration 53/125: Train Loss = 0.2489, Valid Loss = 0.2756\n",
      "Iteration 54/125: Train Loss = 0.2477, Valid Loss = 0.2745\n",
      "Iteration 55/125: Train Loss = 0.2465, Valid Loss = 0.2736\n",
      "Iteration 56/125: Train Loss = 0.2452, Valid Loss = 0.2727\n",
      "Iteration 57/125: Train Loss = 0.2441, Valid Loss = 0.2715\n",
      "Iteration 58/125: Train Loss = 0.2431, Valid Loss = 0.2708\n",
      "Iteration 59/125: Train Loss = 0.2420, Valid Loss = 0.2700\n",
      "Iteration 60/125: Train Loss = 0.2409, Valid Loss = 0.2690\n",
      "Iteration 61/125: Train Loss = 0.2398, Valid Loss = 0.2678\n",
      "Iteration 62/125: Train Loss = 0.2387, Valid Loss = 0.2671\n",
      "Iteration 63/125: Train Loss = 0.2379, Valid Loss = 0.2665\n",
      "Iteration 64/125: Train Loss = 0.2368, Valid Loss = 0.2658\n",
      "Iteration 65/125: Train Loss = 0.2359, Valid Loss = 0.2651\n",
      "Iteration 66/125: Train Loss = 0.2349, Valid Loss = 0.2646\n",
      "Iteration 67/125: Train Loss = 0.2339, Valid Loss = 0.2641\n",
      "Iteration 68/125: Train Loss = 0.2330, Valid Loss = 0.2629\n",
      "Iteration 69/125: Train Loss = 0.2321, Valid Loss = 0.2621\n",
      "Iteration 70/125: Train Loss = 0.2313, Valid Loss = 0.2614\n",
      "Iteration 71/125: Train Loss = 0.2306, Valid Loss = 0.2609\n",
      "Iteration 72/125: Train Loss = 0.2298, Valid Loss = 0.2603\n",
      "Iteration 73/125: Train Loss = 0.2290, Valid Loss = 0.2599\n",
      "Iteration 74/125: Train Loss = 0.2282, Valid Loss = 0.2594\n",
      "Iteration 75/125: Train Loss = 0.2275, Valid Loss = 0.2588\n",
      "Iteration 76/125: Train Loss = 0.2266, Valid Loss = 0.2582\n",
      "Iteration 77/125: Train Loss = 0.2259, Valid Loss = 0.2579\n",
      "Iteration 78/125: Train Loss = 0.2251, Valid Loss = 0.2576\n",
      "Iteration 79/125: Train Loss = 0.2245, Valid Loss = 0.2571\n",
      "Iteration 80/125: Train Loss = 0.2239, Valid Loss = 0.2568\n",
      "Iteration 81/125: Train Loss = 0.2233, Valid Loss = 0.2563\n",
      "Iteration 82/125: Train Loss = 0.2226, Valid Loss = 0.2557\n",
      "Iteration 83/125: Train Loss = 0.2220, Valid Loss = 0.2552\n",
      "Iteration 84/125: Train Loss = 0.2215, Valid Loss = 0.2547\n",
      "Iteration 85/125: Train Loss = 0.2209, Valid Loss = 0.2543\n",
      "Iteration 86/125: Train Loss = 0.2203, Valid Loss = 0.2539\n",
      "Iteration 87/125: Train Loss = 0.2196, Valid Loss = 0.2537\n",
      "Iteration 88/125: Train Loss = 0.2191, Valid Loss = 0.2532\n",
      "Iteration 89/125: Train Loss = 0.2185, Valid Loss = 0.2530\n",
      "Iteration 90/125: Train Loss = 0.2180, Valid Loss = 0.2526\n",
      "Iteration 91/125: Train Loss = 0.2175, Valid Loss = 0.2524\n",
      "Iteration 92/125: Train Loss = 0.2169, Valid Loss = 0.2520\n",
      "Iteration 93/125: Train Loss = 0.2164, Valid Loss = 0.2516\n",
      "Iteration 94/125: Train Loss = 0.2159, Valid Loss = 0.2513\n",
      "Iteration 95/125: Train Loss = 0.2154, Valid Loss = 0.2510\n",
      "Iteration 96/125: Train Loss = 0.2150, Valid Loss = 0.2509\n",
      "Iteration 97/125: Train Loss = 0.2145, Valid Loss = 0.2505\n",
      "Iteration 98/125: Train Loss = 0.2140, Valid Loss = 0.2502\n",
      "Iteration 99/125: Train Loss = 0.2134, Valid Loss = 0.2498\n",
      "Iteration 100/125: Train Loss = 0.2129, Valid Loss = 0.2495\n",
      "Iteration 101/125: Train Loss = 0.2124, Valid Loss = 0.2491\n",
      "Iteration 102/125: Train Loss = 0.2120, Valid Loss = 0.2489\n",
      "Iteration 103/125: Train Loss = 0.2115, Valid Loss = 0.2486\n",
      "Iteration 104/125: Train Loss = 0.2111, Valid Loss = 0.2483\n",
      "Iteration 105/125: Train Loss = 0.2106, Valid Loss = 0.2480\n",
      "Iteration 106/125: Train Loss = 0.2102, Valid Loss = 0.2478\n",
      "Iteration 107/125: Train Loss = 0.2098, Valid Loss = 0.2476\n",
      "Iteration 108/125: Train Loss = 0.2093, Valid Loss = 0.2474\n",
      "Iteration 109/125: Train Loss = 0.2089, Valid Loss = 0.2472\n",
      "Iteration 110/125: Train Loss = 0.2084, Valid Loss = 0.2469\n",
      "Iteration 111/125: Train Loss = 0.2081, Valid Loss = 0.2467\n",
      "Iteration 112/125: Train Loss = 0.2077, Valid Loss = 0.2466\n",
      "Iteration 113/125: Train Loss = 0.2073, Valid Loss = 0.2463\n",
      "Iteration 114/125: Train Loss = 0.2070, Valid Loss = 0.2460\n",
      "Iteration 115/125: Train Loss = 0.2066, Valid Loss = 0.2457\n",
      "Iteration 116/125: Train Loss = 0.2062, Valid Loss = 0.2456\n",
      "Iteration 117/125: Train Loss = 0.2059, Valid Loss = 0.2455\n",
      "Iteration 118/125: Train Loss = 0.2055, Valid Loss = 0.2454\n",
      "Iteration 119/125: Train Loss = 0.2051, Valid Loss = 0.2452\n",
      "Iteration 120/125: Train Loss = 0.2047, Valid Loss = 0.2450\n",
      "Iteration 121/125: Train Loss = 0.2044, Valid Loss = 0.2451\n",
      "Iteration 122/125: Train Loss = 0.2041, Valid Loss = 0.2450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:52:26,604] Trial 43 finished with value: 0.9655536872447754 and parameters: {'max_depth': 8, 'min_samples_leaf': 16, 'n_estimators': 125, 'learning_rate': 0.24765493815280082, 'subsample': 0.6682424955954727}. Best is trial 27 with value: 0.9657550072635349.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 123/125: Train Loss = 0.2039, Valid Loss = 0.2449\n",
      "Iteration 124/125: Train Loss = 0.2035, Valid Loss = 0.2447\n",
      "Iteration 125/125: Train Loss = 0.2032, Valid Loss = 0.2444\n",
      "Iteration 1/150: Train Loss = 0.6580, Valid Loss = 0.6588\n",
      "Iteration 2/150: Train Loss = 0.6268, Valid Loss = 0.6286\n",
      "Iteration 3/150: Train Loss = 0.5984, Valid Loss = 0.6006\n",
      "Iteration 4/150: Train Loss = 0.5731, Valid Loss = 0.5761\n",
      "Iteration 5/150: Train Loss = 0.5499, Valid Loss = 0.5537\n",
      "Iteration 6/150: Train Loss = 0.5293, Valid Loss = 0.5340\n",
      "Iteration 7/150: Train Loss = 0.5099, Valid Loss = 0.5155\n",
      "Iteration 8/150: Train Loss = 0.4927, Valid Loss = 0.4987\n",
      "Iteration 9/150: Train Loss = 0.4773, Valid Loss = 0.4836\n",
      "Iteration 10/150: Train Loss = 0.4633, Valid Loss = 0.4698\n",
      "Iteration 11/150: Train Loss = 0.4503, Valid Loss = 0.4573\n",
      "Iteration 12/150: Train Loss = 0.4382, Valid Loss = 0.4462\n",
      "Iteration 13/150: Train Loss = 0.4272, Valid Loss = 0.4357\n",
      "Iteration 14/150: Train Loss = 0.4173, Valid Loss = 0.4262\n",
      "Iteration 15/150: Train Loss = 0.4079, Valid Loss = 0.4172\n",
      "Iteration 16/150: Train Loss = 0.3993, Valid Loss = 0.4089\n",
      "Iteration 17/150: Train Loss = 0.3915, Valid Loss = 0.4018\n",
      "Iteration 18/150: Train Loss = 0.3841, Valid Loss = 0.3947\n",
      "Iteration 19/150: Train Loss = 0.3773, Valid Loss = 0.3884\n",
      "Iteration 20/150: Train Loss = 0.3709, Valid Loss = 0.3825\n",
      "Iteration 21/150: Train Loss = 0.3648, Valid Loss = 0.3767\n",
      "Iteration 22/150: Train Loss = 0.3588, Valid Loss = 0.3711\n",
      "Iteration 23/150: Train Loss = 0.3535, Valid Loss = 0.3661\n",
      "Iteration 24/150: Train Loss = 0.3482, Valid Loss = 0.3612\n",
      "Iteration 25/150: Train Loss = 0.3435, Valid Loss = 0.3568\n",
      "Iteration 26/150: Train Loss = 0.3390, Valid Loss = 0.3528\n",
      "Iteration 27/150: Train Loss = 0.3348, Valid Loss = 0.3491\n",
      "Iteration 28/150: Train Loss = 0.3305, Valid Loss = 0.3452\n",
      "Iteration 29/150: Train Loss = 0.3269, Valid Loss = 0.3417\n",
      "Iteration 30/150: Train Loss = 0.3233, Valid Loss = 0.3382\n",
      "Iteration 31/150: Train Loss = 0.3200, Valid Loss = 0.3352\n",
      "Iteration 32/150: Train Loss = 0.3168, Valid Loss = 0.3321\n",
      "Iteration 33/150: Train Loss = 0.3137, Valid Loss = 0.3291\n",
      "Iteration 34/150: Train Loss = 0.3108, Valid Loss = 0.3262\n",
      "Iteration 35/150: Train Loss = 0.3081, Valid Loss = 0.3236\n",
      "Iteration 36/150: Train Loss = 0.3052, Valid Loss = 0.3210\n",
      "Iteration 37/150: Train Loss = 0.3026, Valid Loss = 0.3186\n",
      "Iteration 38/150: Train Loss = 0.3001, Valid Loss = 0.3163\n",
      "Iteration 39/150: Train Loss = 0.2977, Valid Loss = 0.3144\n",
      "Iteration 40/150: Train Loss = 0.2953, Valid Loss = 0.3125\n",
      "Iteration 41/150: Train Loss = 0.2932, Valid Loss = 0.3106\n",
      "Iteration 42/150: Train Loss = 0.2912, Valid Loss = 0.3088\n",
      "Iteration 43/150: Train Loss = 0.2893, Valid Loss = 0.3071\n",
      "Iteration 44/150: Train Loss = 0.2874, Valid Loss = 0.3052\n",
      "Iteration 45/150: Train Loss = 0.2856, Valid Loss = 0.3035\n",
      "Iteration 46/150: Train Loss = 0.2838, Valid Loss = 0.3018\n",
      "Iteration 47/150: Train Loss = 0.2821, Valid Loss = 0.3002\n",
      "Iteration 48/150: Train Loss = 0.2804, Valid Loss = 0.2986\n",
      "Iteration 49/150: Train Loss = 0.2788, Valid Loss = 0.2973\n",
      "Iteration 50/150: Train Loss = 0.2773, Valid Loss = 0.2960\n",
      "Iteration 51/150: Train Loss = 0.2758, Valid Loss = 0.2946\n",
      "Iteration 52/150: Train Loss = 0.2743, Valid Loss = 0.2933\n",
      "Iteration 53/150: Train Loss = 0.2729, Valid Loss = 0.2920\n",
      "Iteration 54/150: Train Loss = 0.2715, Valid Loss = 0.2908\n",
      "Iteration 55/150: Train Loss = 0.2703, Valid Loss = 0.2897\n",
      "Iteration 56/150: Train Loss = 0.2690, Valid Loss = 0.2886\n",
      "Iteration 57/150: Train Loss = 0.2676, Valid Loss = 0.2875\n",
      "Iteration 58/150: Train Loss = 0.2664, Valid Loss = 0.2865\n",
      "Iteration 59/150: Train Loss = 0.2653, Valid Loss = 0.2855\n",
      "Iteration 60/150: Train Loss = 0.2642, Valid Loss = 0.2848\n",
      "Iteration 61/150: Train Loss = 0.2631, Valid Loss = 0.2840\n",
      "Iteration 62/150: Train Loss = 0.2620, Valid Loss = 0.2831\n",
      "Iteration 63/150: Train Loss = 0.2610, Valid Loss = 0.2824\n",
      "Iteration 64/150: Train Loss = 0.2600, Valid Loss = 0.2813\n",
      "Iteration 65/150: Train Loss = 0.2589, Valid Loss = 0.2804\n",
      "Iteration 66/150: Train Loss = 0.2578, Valid Loss = 0.2794\n",
      "Iteration 67/150: Train Loss = 0.2568, Valid Loss = 0.2785\n",
      "Iteration 68/150: Train Loss = 0.2558, Valid Loss = 0.2776\n",
      "Iteration 69/150: Train Loss = 0.2547, Valid Loss = 0.2766\n",
      "Iteration 70/150: Train Loss = 0.2539, Valid Loss = 0.2759\n",
      "Iteration 71/150: Train Loss = 0.2529, Valid Loss = 0.2751\n",
      "Iteration 72/150: Train Loss = 0.2518, Valid Loss = 0.2744\n",
      "Iteration 73/150: Train Loss = 0.2510, Valid Loss = 0.2736\n",
      "Iteration 74/150: Train Loss = 0.2502, Valid Loss = 0.2732\n",
      "Iteration 75/150: Train Loss = 0.2494, Valid Loss = 0.2725\n",
      "Iteration 76/150: Train Loss = 0.2485, Valid Loss = 0.2719\n",
      "Iteration 77/150: Train Loss = 0.2477, Valid Loss = 0.2711\n",
      "Iteration 78/150: Train Loss = 0.2471, Valid Loss = 0.2706\n",
      "Iteration 79/150: Train Loss = 0.2464, Valid Loss = 0.2700\n",
      "Iteration 80/150: Train Loss = 0.2455, Valid Loss = 0.2694\n",
      "Iteration 81/150: Train Loss = 0.2448, Valid Loss = 0.2689\n",
      "Iteration 82/150: Train Loss = 0.2440, Valid Loss = 0.2683\n",
      "Iteration 83/150: Train Loss = 0.2433, Valid Loss = 0.2677\n",
      "Iteration 84/150: Train Loss = 0.2426, Valid Loss = 0.2672\n",
      "Iteration 85/150: Train Loss = 0.2420, Valid Loss = 0.2669\n",
      "Iteration 86/150: Train Loss = 0.2414, Valid Loss = 0.2664\n",
      "Iteration 87/150: Train Loss = 0.2408, Valid Loss = 0.2660\n",
      "Iteration 88/150: Train Loss = 0.2403, Valid Loss = 0.2655\n",
      "Iteration 89/150: Train Loss = 0.2395, Valid Loss = 0.2650\n",
      "Iteration 90/150: Train Loss = 0.2390, Valid Loss = 0.2648\n",
      "Iteration 91/150: Train Loss = 0.2384, Valid Loss = 0.2643\n",
      "Iteration 92/150: Train Loss = 0.2379, Valid Loss = 0.2640\n",
      "Iteration 93/150: Train Loss = 0.2375, Valid Loss = 0.2636\n",
      "Iteration 94/150: Train Loss = 0.2370, Valid Loss = 0.2631\n",
      "Iteration 95/150: Train Loss = 0.2364, Valid Loss = 0.2627\n",
      "Iteration 96/150: Train Loss = 0.2358, Valid Loss = 0.2623\n",
      "Iteration 97/150: Train Loss = 0.2352, Valid Loss = 0.2619\n",
      "Iteration 98/150: Train Loss = 0.2348, Valid Loss = 0.2616\n",
      "Iteration 99/150: Train Loss = 0.2343, Valid Loss = 0.2613\n",
      "Iteration 100/150: Train Loss = 0.2338, Valid Loss = 0.2609\n",
      "Iteration 101/150: Train Loss = 0.2332, Valid Loss = 0.2604\n",
      "Iteration 102/150: Train Loss = 0.2328, Valid Loss = 0.2599\n",
      "Iteration 103/150: Train Loss = 0.2323, Valid Loss = 0.2596\n",
      "Iteration 104/150: Train Loss = 0.2318, Valid Loss = 0.2592\n",
      "Iteration 105/150: Train Loss = 0.2314, Valid Loss = 0.2589\n",
      "Iteration 106/150: Train Loss = 0.2309, Valid Loss = 0.2586\n",
      "Iteration 107/150: Train Loss = 0.2305, Valid Loss = 0.2583\n",
      "Iteration 108/150: Train Loss = 0.2301, Valid Loss = 0.2579\n",
      "Iteration 109/150: Train Loss = 0.2297, Valid Loss = 0.2576\n",
      "Iteration 110/150: Train Loss = 0.2293, Valid Loss = 0.2574\n",
      "Iteration 111/150: Train Loss = 0.2290, Valid Loss = 0.2570\n",
      "Iteration 112/150: Train Loss = 0.2286, Valid Loss = 0.2567\n",
      "Iteration 113/150: Train Loss = 0.2283, Valid Loss = 0.2563\n",
      "Iteration 114/150: Train Loss = 0.2278, Valid Loss = 0.2562\n",
      "Iteration 115/150: Train Loss = 0.2275, Valid Loss = 0.2560\n",
      "Iteration 116/150: Train Loss = 0.2270, Valid Loss = 0.2557\n",
      "Iteration 117/150: Train Loss = 0.2267, Valid Loss = 0.2554\n",
      "Iteration 118/150: Train Loss = 0.2263, Valid Loss = 0.2553\n",
      "Iteration 119/150: Train Loss = 0.2259, Valid Loss = 0.2552\n",
      "Iteration 120/150: Train Loss = 0.2256, Valid Loss = 0.2550\n",
      "Iteration 121/150: Train Loss = 0.2252, Valid Loss = 0.2548\n",
      "Iteration 122/150: Train Loss = 0.2248, Valid Loss = 0.2544\n",
      "Iteration 123/150: Train Loss = 0.2245, Valid Loss = 0.2542\n",
      "Iteration 124/150: Train Loss = 0.2241, Valid Loss = 0.2539\n",
      "Iteration 125/150: Train Loss = 0.2238, Valid Loss = 0.2537\n",
      "Iteration 126/150: Train Loss = 0.2234, Valid Loss = 0.2533\n",
      "Iteration 127/150: Train Loss = 0.2232, Valid Loss = 0.2531\n",
      "Iteration 128/150: Train Loss = 0.2228, Valid Loss = 0.2528\n",
      "Iteration 129/150: Train Loss = 0.2224, Valid Loss = 0.2524\n",
      "Iteration 130/150: Train Loss = 0.2221, Valid Loss = 0.2523\n",
      "Iteration 131/150: Train Loss = 0.2218, Valid Loss = 0.2522\n",
      "Iteration 132/150: Train Loss = 0.2215, Valid Loss = 0.2519\n",
      "Iteration 133/150: Train Loss = 0.2211, Valid Loss = 0.2517\n",
      "Iteration 134/150: Train Loss = 0.2208, Valid Loss = 0.2515\n",
      "Iteration 135/150: Train Loss = 0.2205, Valid Loss = 0.2514\n",
      "Iteration 136/150: Train Loss = 0.2202, Valid Loss = 0.2512\n",
      "Iteration 137/150: Train Loss = 0.2199, Valid Loss = 0.2510\n",
      "Iteration 138/150: Train Loss = 0.2195, Valid Loss = 0.2507\n",
      "Iteration 139/150: Train Loss = 0.2192, Valid Loss = 0.2505\n",
      "Iteration 140/150: Train Loss = 0.2189, Valid Loss = 0.2504\n",
      "Iteration 141/150: Train Loss = 0.2186, Valid Loss = 0.2504\n",
      "Iteration 142/150: Train Loss = 0.2183, Valid Loss = 0.2502\n",
      "Iteration 143/150: Train Loss = 0.2180, Valid Loss = 0.2501\n",
      "Iteration 144/150: Train Loss = 0.2177, Valid Loss = 0.2500\n",
      "Iteration 145/150: Train Loss = 0.2175, Valid Loss = 0.2498\n",
      "Iteration 146/150: Train Loss = 0.2172, Valid Loss = 0.2495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:52:33,161] Trial 44 finished with value: 0.9643633063762779 and parameters: {'max_depth': 6, 'min_samples_leaf': 14, 'n_estimators': 150, 'learning_rate': 0.2177873760562631, 'subsample': 0.6543962753356145}. Best is trial 27 with value: 0.9657550072635349.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 147/150: Train Loss = 0.2170, Valid Loss = 0.2494\n",
      "Iteration 148/150: Train Loss = 0.2167, Valid Loss = 0.2492\n",
      "Iteration 149/150: Train Loss = 0.2164, Valid Loss = 0.2490\n",
      "Iteration 150/150: Train Loss = 0.2162, Valid Loss = 0.2488\n",
      "Iteration 1/135: Train Loss = 0.6766, Valid Loss = 0.6770\n",
      "Iteration 2/135: Train Loss = 0.6608, Valid Loss = 0.6615\n",
      "Iteration 3/135: Train Loss = 0.6457, Valid Loss = 0.6468\n",
      "Iteration 4/135: Train Loss = 0.6316, Valid Loss = 0.6330\n",
      "Iteration 5/135: Train Loss = 0.6179, Valid Loss = 0.6196\n",
      "Iteration 6/135: Train Loss = 0.6053, Valid Loss = 0.6073\n",
      "Iteration 7/135: Train Loss = 0.5931, Valid Loss = 0.5954\n",
      "Iteration 8/135: Train Loss = 0.5813, Valid Loss = 0.5839\n",
      "Iteration 9/135: Train Loss = 0.5704, Valid Loss = 0.5733\n",
      "Iteration 10/135: Train Loss = 0.5599, Valid Loss = 0.5630\n",
      "Iteration 11/135: Train Loss = 0.5500, Valid Loss = 0.5533\n",
      "Iteration 12/135: Train Loss = 0.5404, Valid Loss = 0.5439\n",
      "Iteration 13/135: Train Loss = 0.5315, Valid Loss = 0.5352\n",
      "Iteration 14/135: Train Loss = 0.5230, Valid Loss = 0.5270\n",
      "Iteration 15/135: Train Loss = 0.5147, Valid Loss = 0.5188\n",
      "Iteration 16/135: Train Loss = 0.5068, Valid Loss = 0.5111\n",
      "Iteration 17/135: Train Loss = 0.4993, Valid Loss = 0.5039\n",
      "Iteration 18/135: Train Loss = 0.4922, Valid Loss = 0.4971\n",
      "Iteration 19/135: Train Loss = 0.4855, Valid Loss = 0.4905\n",
      "Iteration 20/135: Train Loss = 0.4790, Valid Loss = 0.4842\n",
      "Iteration 21/135: Train Loss = 0.4727, Valid Loss = 0.4780\n",
      "Iteration 22/135: Train Loss = 0.4669, Valid Loss = 0.4723\n",
      "Iteration 23/135: Train Loss = 0.4611, Valid Loss = 0.4668\n",
      "Iteration 24/135: Train Loss = 0.4554, Valid Loss = 0.4612\n",
      "Iteration 25/135: Train Loss = 0.4502, Valid Loss = 0.4560\n",
      "Iteration 26/135: Train Loss = 0.4452, Valid Loss = 0.4511\n",
      "Iteration 27/135: Train Loss = 0.4403, Valid Loss = 0.4463\n",
      "Iteration 28/135: Train Loss = 0.4357, Valid Loss = 0.4419\n",
      "Iteration 29/135: Train Loss = 0.4312, Valid Loss = 0.4375\n",
      "Iteration 30/135: Train Loss = 0.4269, Valid Loss = 0.4332\n",
      "Iteration 31/135: Train Loss = 0.4228, Valid Loss = 0.4292\n",
      "Iteration 32/135: Train Loss = 0.4187, Valid Loss = 0.4253\n",
      "Iteration 33/135: Train Loss = 0.4149, Valid Loss = 0.4216\n",
      "Iteration 34/135: Train Loss = 0.4111, Valid Loss = 0.4180\n",
      "Iteration 35/135: Train Loss = 0.4076, Valid Loss = 0.4145\n",
      "Iteration 36/135: Train Loss = 0.4042, Valid Loss = 0.4111\n",
      "Iteration 37/135: Train Loss = 0.4008, Valid Loss = 0.4077\n",
      "Iteration 38/135: Train Loss = 0.3975, Valid Loss = 0.4046\n",
      "Iteration 39/135: Train Loss = 0.3944, Valid Loss = 0.4015\n",
      "Iteration 40/135: Train Loss = 0.3916, Valid Loss = 0.3988\n",
      "Iteration 41/135: Train Loss = 0.3885, Valid Loss = 0.3958\n",
      "Iteration 42/135: Train Loss = 0.3856, Valid Loss = 0.3931\n",
      "Iteration 43/135: Train Loss = 0.3829, Valid Loss = 0.3903\n",
      "Iteration 44/135: Train Loss = 0.3803, Valid Loss = 0.3877\n",
      "Iteration 45/135: Train Loss = 0.3777, Valid Loss = 0.3853\n",
      "Iteration 46/135: Train Loss = 0.3753, Valid Loss = 0.3830\n",
      "Iteration 47/135: Train Loss = 0.3729, Valid Loss = 0.3806\n",
      "Iteration 48/135: Train Loss = 0.3706, Valid Loss = 0.3785\n",
      "Iteration 49/135: Train Loss = 0.3683, Valid Loss = 0.3764\n",
      "Iteration 50/135: Train Loss = 0.3662, Valid Loss = 0.3743\n",
      "Iteration 51/135: Train Loss = 0.3641, Valid Loss = 0.3722\n",
      "Iteration 52/135: Train Loss = 0.3620, Valid Loss = 0.3702\n",
      "Iteration 53/135: Train Loss = 0.3600, Valid Loss = 0.3682\n",
      "Iteration 54/135: Train Loss = 0.3580, Valid Loss = 0.3663\n",
      "Iteration 55/135: Train Loss = 0.3560, Valid Loss = 0.3642\n",
      "Iteration 56/135: Train Loss = 0.3541, Valid Loss = 0.3623\n",
      "Iteration 57/135: Train Loss = 0.3522, Valid Loss = 0.3604\n",
      "Iteration 58/135: Train Loss = 0.3504, Valid Loss = 0.3587\n",
      "Iteration 59/135: Train Loss = 0.3487, Valid Loss = 0.3570\n",
      "Iteration 60/135: Train Loss = 0.3470, Valid Loss = 0.3553\n",
      "Iteration 61/135: Train Loss = 0.3453, Valid Loss = 0.3537\n",
      "Iteration 62/135: Train Loss = 0.3438, Valid Loss = 0.3522\n",
      "Iteration 63/135: Train Loss = 0.3421, Valid Loss = 0.3506\n",
      "Iteration 64/135: Train Loss = 0.3406, Valid Loss = 0.3491\n",
      "Iteration 65/135: Train Loss = 0.3392, Valid Loss = 0.3478\n",
      "Iteration 66/135: Train Loss = 0.3378, Valid Loss = 0.3464\n",
      "Iteration 67/135: Train Loss = 0.3364, Valid Loss = 0.3450\n",
      "Iteration 68/135: Train Loss = 0.3351, Valid Loss = 0.3437\n",
      "Iteration 69/135: Train Loss = 0.3337, Valid Loss = 0.3425\n",
      "Iteration 70/135: Train Loss = 0.3324, Valid Loss = 0.3411\n",
      "Iteration 71/135: Train Loss = 0.3312, Valid Loss = 0.3400\n",
      "Iteration 72/135: Train Loss = 0.3300, Valid Loss = 0.3388\n",
      "Iteration 73/135: Train Loss = 0.3288, Valid Loss = 0.3377\n",
      "Iteration 74/135: Train Loss = 0.3276, Valid Loss = 0.3366\n",
      "Iteration 75/135: Train Loss = 0.3263, Valid Loss = 0.3354\n",
      "Iteration 76/135: Train Loss = 0.3252, Valid Loss = 0.3343\n",
      "Iteration 77/135: Train Loss = 0.3240, Valid Loss = 0.3331\n",
      "Iteration 78/135: Train Loss = 0.3228, Valid Loss = 0.3319\n",
      "Iteration 79/135: Train Loss = 0.3218, Valid Loss = 0.3307\n",
      "Iteration 80/135: Train Loss = 0.3207, Valid Loss = 0.3297\n",
      "Iteration 81/135: Train Loss = 0.3196, Valid Loss = 0.3288\n",
      "Iteration 82/135: Train Loss = 0.3186, Valid Loss = 0.3278\n",
      "Iteration 83/135: Train Loss = 0.3176, Valid Loss = 0.3267\n",
      "Iteration 84/135: Train Loss = 0.3166, Valid Loss = 0.3258\n",
      "Iteration 85/135: Train Loss = 0.3156, Valid Loss = 0.3249\n",
      "Iteration 86/135: Train Loss = 0.3147, Valid Loss = 0.3240\n",
      "Iteration 87/135: Train Loss = 0.3138, Valid Loss = 0.3233\n",
      "Iteration 88/135: Train Loss = 0.3128, Valid Loss = 0.3224\n",
      "Iteration 89/135: Train Loss = 0.3119, Valid Loss = 0.3215\n",
      "Iteration 90/135: Train Loss = 0.3110, Valid Loss = 0.3206\n",
      "Iteration 91/135: Train Loss = 0.3100, Valid Loss = 0.3198\n",
      "Iteration 92/135: Train Loss = 0.3090, Valid Loss = 0.3190\n",
      "Iteration 93/135: Train Loss = 0.3082, Valid Loss = 0.3181\n",
      "Iteration 94/135: Train Loss = 0.3074, Valid Loss = 0.3174\n",
      "Iteration 95/135: Train Loss = 0.3066, Valid Loss = 0.3167\n",
      "Iteration 96/135: Train Loss = 0.3058, Valid Loss = 0.3159\n",
      "Iteration 97/135: Train Loss = 0.3051, Valid Loss = 0.3153\n",
      "Iteration 98/135: Train Loss = 0.3043, Valid Loss = 0.3146\n",
      "Iteration 99/135: Train Loss = 0.3035, Valid Loss = 0.3139\n",
      "Iteration 100/135: Train Loss = 0.3028, Valid Loss = 0.3131\n",
      "Iteration 101/135: Train Loss = 0.3021, Valid Loss = 0.3124\n",
      "Iteration 102/135: Train Loss = 0.3013, Valid Loss = 0.3116\n",
      "Iteration 103/135: Train Loss = 0.3006, Valid Loss = 0.3109\n",
      "Iteration 104/135: Train Loss = 0.2999, Valid Loss = 0.3102\n",
      "Iteration 105/135: Train Loss = 0.2991, Valid Loss = 0.3095\n",
      "Iteration 106/135: Train Loss = 0.2985, Valid Loss = 0.3088\n",
      "Iteration 107/135: Train Loss = 0.2979, Valid Loss = 0.3082\n",
      "Iteration 108/135: Train Loss = 0.2973, Valid Loss = 0.3077\n",
      "Iteration 109/135: Train Loss = 0.2966, Valid Loss = 0.3070\n",
      "Iteration 110/135: Train Loss = 0.2960, Valid Loss = 0.3064\n",
      "Iteration 111/135: Train Loss = 0.2954, Valid Loss = 0.3058\n",
      "Iteration 112/135: Train Loss = 0.2948, Valid Loss = 0.3052\n",
      "Iteration 113/135: Train Loss = 0.2941, Valid Loss = 0.3046\n",
      "Iteration 114/135: Train Loss = 0.2935, Valid Loss = 0.3040\n",
      "Iteration 115/135: Train Loss = 0.2929, Valid Loss = 0.3035\n",
      "Iteration 116/135: Train Loss = 0.2924, Valid Loss = 0.3030\n",
      "Iteration 117/135: Train Loss = 0.2918, Valid Loss = 0.3024\n",
      "Iteration 118/135: Train Loss = 0.2912, Valid Loss = 0.3019\n",
      "Iteration 119/135: Train Loss = 0.2906, Valid Loss = 0.3013\n",
      "Iteration 120/135: Train Loss = 0.2901, Valid Loss = 0.3008\n",
      "Iteration 121/135: Train Loss = 0.2895, Valid Loss = 0.3004\n",
      "Iteration 122/135: Train Loss = 0.2890, Valid Loss = 0.2999\n",
      "Iteration 123/135: Train Loss = 0.2885, Valid Loss = 0.2994\n",
      "Iteration 124/135: Train Loss = 0.2879, Valid Loss = 0.2988\n",
      "Iteration 125/135: Train Loss = 0.2873, Valid Loss = 0.2984\n",
      "Iteration 126/135: Train Loss = 0.2868, Valid Loss = 0.2978\n",
      "Iteration 127/135: Train Loss = 0.2863, Valid Loss = 0.2973\n",
      "Iteration 128/135: Train Loss = 0.2858, Valid Loss = 0.2968\n",
      "Iteration 129/135: Train Loss = 0.2852, Valid Loss = 0.2963\n",
      "Iteration 130/135: Train Loss = 0.2847, Valid Loss = 0.2958\n",
      "Iteration 131/135: Train Loss = 0.2842, Valid Loss = 0.2953\n",
      "Iteration 132/135: Train Loss = 0.2837, Valid Loss = 0.2948\n",
      "Iteration 133/135: Train Loss = 0.2832, Valid Loss = 0.2944\n",
      "Iteration 134/135: Train Loss = 0.2827, Valid Loss = 0.2940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:52:37,556] Trial 45 finished with value: 0.9571134979734548 and parameters: {'max_depth': 4, 'min_samples_leaf': 17, 'n_estimators': 135, 'learning_rate': 0.11069822935729773, 'subsample': 0.6736377082178319}. Best is trial 27 with value: 0.9657550072635349.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 135/135: Train Loss = 0.2823, Valid Loss = 0.2936\n",
      "Iteration 1/140: Train Loss = 0.6539, Valid Loss = 0.6549\n",
      "Iteration 2/140: Train Loss = 0.6188, Valid Loss = 0.6211\n",
      "Iteration 3/140: Train Loss = 0.5878, Valid Loss = 0.5914\n",
      "Iteration 4/140: Train Loss = 0.5601, Valid Loss = 0.5646\n",
      "Iteration 5/140: Train Loss = 0.5353, Valid Loss = 0.5404\n",
      "Iteration 6/140: Train Loss = 0.5126, Valid Loss = 0.5186\n",
      "Iteration 7/140: Train Loss = 0.4928, Valid Loss = 0.4991\n",
      "Iteration 8/140: Train Loss = 0.4745, Valid Loss = 0.4813\n",
      "Iteration 9/140: Train Loss = 0.4578, Valid Loss = 0.4655\n",
      "Iteration 10/140: Train Loss = 0.4426, Valid Loss = 0.4512\n",
      "Iteration 11/140: Train Loss = 0.4288, Valid Loss = 0.4384\n",
      "Iteration 12/140: Train Loss = 0.4162, Valid Loss = 0.4263\n",
      "Iteration 13/140: Train Loss = 0.4049, Valid Loss = 0.4152\n",
      "Iteration 14/140: Train Loss = 0.3946, Valid Loss = 0.4054\n",
      "Iteration 15/140: Train Loss = 0.3850, Valid Loss = 0.3961\n",
      "Iteration 16/140: Train Loss = 0.3762, Valid Loss = 0.3881\n",
      "Iteration 17/140: Train Loss = 0.3681, Valid Loss = 0.3809\n",
      "Iteration 18/140: Train Loss = 0.3604, Valid Loss = 0.3738\n",
      "Iteration 19/140: Train Loss = 0.3533, Valid Loss = 0.3673\n",
      "Iteration 20/140: Train Loss = 0.3468, Valid Loss = 0.3616\n",
      "Iteration 21/140: Train Loss = 0.3407, Valid Loss = 0.3561\n",
      "Iteration 22/140: Train Loss = 0.3346, Valid Loss = 0.3506\n",
      "Iteration 23/140: Train Loss = 0.3290, Valid Loss = 0.3453\n",
      "Iteration 24/140: Train Loss = 0.3239, Valid Loss = 0.3409\n",
      "Iteration 25/140: Train Loss = 0.3191, Valid Loss = 0.3370\n",
      "Iteration 26/140: Train Loss = 0.3147, Valid Loss = 0.3328\n",
      "Iteration 27/140: Train Loss = 0.3105, Valid Loss = 0.3293\n",
      "Iteration 28/140: Train Loss = 0.3063, Valid Loss = 0.3255\n",
      "Iteration 29/140: Train Loss = 0.3025, Valid Loss = 0.3223\n",
      "Iteration 30/140: Train Loss = 0.2988, Valid Loss = 0.3191\n",
      "Iteration 31/140: Train Loss = 0.2954, Valid Loss = 0.3161\n",
      "Iteration 32/140: Train Loss = 0.2921, Valid Loss = 0.3133\n",
      "Iteration 33/140: Train Loss = 0.2891, Valid Loss = 0.3108\n",
      "Iteration 34/140: Train Loss = 0.2860, Valid Loss = 0.3082\n",
      "Iteration 35/140: Train Loss = 0.2833, Valid Loss = 0.3058\n",
      "Iteration 36/140: Train Loss = 0.2802, Valid Loss = 0.3035\n",
      "Iteration 37/140: Train Loss = 0.2778, Valid Loss = 0.3013\n",
      "Iteration 38/140: Train Loss = 0.2754, Valid Loss = 0.2995\n",
      "Iteration 39/140: Train Loss = 0.2729, Valid Loss = 0.2975\n",
      "Iteration 40/140: Train Loss = 0.2708, Valid Loss = 0.2956\n",
      "Iteration 41/140: Train Loss = 0.2686, Valid Loss = 0.2938\n",
      "Iteration 42/140: Train Loss = 0.2666, Valid Loss = 0.2923\n",
      "Iteration 43/140: Train Loss = 0.2646, Valid Loss = 0.2906\n",
      "Iteration 44/140: Train Loss = 0.2626, Valid Loss = 0.2892\n",
      "Iteration 45/140: Train Loss = 0.2606, Valid Loss = 0.2877\n",
      "Iteration 46/140: Train Loss = 0.2590, Valid Loss = 0.2862\n",
      "Iteration 47/140: Train Loss = 0.2571, Valid Loss = 0.2850\n",
      "Iteration 48/140: Train Loss = 0.2555, Valid Loss = 0.2837\n",
      "Iteration 49/140: Train Loss = 0.2539, Valid Loss = 0.2823\n",
      "Iteration 50/140: Train Loss = 0.2523, Valid Loss = 0.2812\n",
      "Iteration 51/140: Train Loss = 0.2508, Valid Loss = 0.2801\n",
      "Iteration 52/140: Train Loss = 0.2492, Valid Loss = 0.2786\n",
      "Iteration 53/140: Train Loss = 0.2477, Valid Loss = 0.2775\n",
      "Iteration 54/140: Train Loss = 0.2465, Valid Loss = 0.2765\n",
      "Iteration 55/140: Train Loss = 0.2451, Valid Loss = 0.2756\n",
      "Iteration 56/140: Train Loss = 0.2439, Valid Loss = 0.2747\n",
      "Iteration 57/140: Train Loss = 0.2427, Valid Loss = 0.2738\n",
      "Iteration 58/140: Train Loss = 0.2415, Valid Loss = 0.2729\n",
      "Iteration 59/140: Train Loss = 0.2404, Valid Loss = 0.2720\n",
      "Iteration 60/140: Train Loss = 0.2392, Valid Loss = 0.2710\n",
      "Iteration 61/140: Train Loss = 0.2381, Valid Loss = 0.2703\n",
      "Iteration 62/140: Train Loss = 0.2370, Valid Loss = 0.2695\n",
      "Iteration 63/140: Train Loss = 0.2360, Valid Loss = 0.2686\n",
      "Iteration 64/140: Train Loss = 0.2351, Valid Loss = 0.2677\n",
      "Iteration 65/140: Train Loss = 0.2342, Valid Loss = 0.2669\n",
      "Iteration 66/140: Train Loss = 0.2331, Valid Loss = 0.2662\n",
      "Iteration 67/140: Train Loss = 0.2323, Valid Loss = 0.2655\n",
      "Iteration 68/140: Train Loss = 0.2311, Valid Loss = 0.2649\n",
      "Iteration 69/140: Train Loss = 0.2301, Valid Loss = 0.2642\n",
      "Iteration 70/140: Train Loss = 0.2292, Valid Loss = 0.2637\n",
      "Iteration 71/140: Train Loss = 0.2283, Valid Loss = 0.2629\n",
      "Iteration 72/140: Train Loss = 0.2275, Valid Loss = 0.2623\n",
      "Iteration 73/140: Train Loss = 0.2267, Valid Loss = 0.2616\n",
      "Iteration 74/140: Train Loss = 0.2260, Valid Loss = 0.2610\n",
      "Iteration 75/140: Train Loss = 0.2252, Valid Loss = 0.2604\n",
      "Iteration 76/140: Train Loss = 0.2243, Valid Loss = 0.2598\n",
      "Iteration 77/140: Train Loss = 0.2236, Valid Loss = 0.2595\n",
      "Iteration 78/140: Train Loss = 0.2228, Valid Loss = 0.2590\n",
      "Iteration 79/140: Train Loss = 0.2222, Valid Loss = 0.2586\n",
      "Iteration 80/140: Train Loss = 0.2214, Valid Loss = 0.2582\n",
      "Iteration 81/140: Train Loss = 0.2207, Valid Loss = 0.2579\n",
      "Iteration 82/140: Train Loss = 0.2199, Valid Loss = 0.2575\n",
      "Iteration 83/140: Train Loss = 0.2193, Valid Loss = 0.2569\n",
      "Iteration 84/140: Train Loss = 0.2186, Valid Loss = 0.2566\n",
      "Iteration 85/140: Train Loss = 0.2180, Valid Loss = 0.2560\n",
      "Iteration 86/140: Train Loss = 0.2174, Valid Loss = 0.2556\n",
      "Iteration 87/140: Train Loss = 0.2168, Valid Loss = 0.2555\n",
      "Iteration 88/140: Train Loss = 0.2162, Valid Loss = 0.2552\n",
      "Iteration 89/140: Train Loss = 0.2155, Valid Loss = 0.2549\n",
      "Iteration 90/140: Train Loss = 0.2149, Valid Loss = 0.2546\n",
      "Iteration 91/140: Train Loss = 0.2143, Valid Loss = 0.2543\n",
      "Iteration 92/140: Train Loss = 0.2138, Valid Loss = 0.2539\n",
      "Iteration 93/140: Train Loss = 0.2132, Valid Loss = 0.2534\n",
      "Iteration 94/140: Train Loss = 0.2127, Valid Loss = 0.2529\n",
      "Iteration 95/140: Train Loss = 0.2121, Valid Loss = 0.2523\n",
      "Iteration 96/140: Train Loss = 0.2116, Valid Loss = 0.2520\n",
      "Iteration 97/140: Train Loss = 0.2111, Valid Loss = 0.2514\n",
      "Iteration 98/140: Train Loss = 0.2106, Valid Loss = 0.2513\n",
      "Iteration 99/140: Train Loss = 0.2102, Valid Loss = 0.2509\n",
      "Iteration 100/140: Train Loss = 0.2097, Valid Loss = 0.2507\n",
      "Iteration 101/140: Train Loss = 0.2092, Valid Loss = 0.2505\n",
      "Iteration 102/140: Train Loss = 0.2086, Valid Loss = 0.2502\n",
      "Iteration 103/140: Train Loss = 0.2081, Valid Loss = 0.2501\n",
      "Iteration 104/140: Train Loss = 0.2077, Valid Loss = 0.2499\n",
      "Iteration 105/140: Train Loss = 0.2073, Valid Loss = 0.2497\n",
      "Iteration 106/140: Train Loss = 0.2067, Valid Loss = 0.2495\n",
      "Iteration 107/140: Train Loss = 0.2063, Valid Loss = 0.2493\n",
      "Iteration 108/140: Train Loss = 0.2059, Valid Loss = 0.2491\n",
      "Iteration 109/140: Train Loss = 0.2054, Valid Loss = 0.2490\n",
      "Iteration 110/140: Train Loss = 0.2050, Valid Loss = 0.2486\n",
      "Iteration 111/140: Train Loss = 0.2045, Valid Loss = 0.2483\n",
      "Iteration 112/140: Train Loss = 0.2042, Valid Loss = 0.2482\n",
      "Iteration 113/140: Train Loss = 0.2037, Valid Loss = 0.2479\n",
      "Iteration 114/140: Train Loss = 0.2033, Valid Loss = 0.2478\n",
      "Iteration 115/140: Train Loss = 0.2029, Valid Loss = 0.2474\n",
      "Iteration 116/140: Train Loss = 0.2025, Valid Loss = 0.2472\n",
      "Iteration 117/140: Train Loss = 0.2021, Valid Loss = 0.2470\n",
      "Iteration 118/140: Train Loss = 0.2018, Valid Loss = 0.2467\n",
      "Iteration 119/140: Train Loss = 0.2014, Valid Loss = 0.2466\n",
      "Iteration 120/140: Train Loss = 0.2011, Valid Loss = 0.2464\n",
      "Iteration 121/140: Train Loss = 0.2006, Valid Loss = 0.2461\n",
      "Iteration 122/140: Train Loss = 0.2002, Valid Loss = 0.2459\n",
      "Iteration 123/140: Train Loss = 0.1999, Valid Loss = 0.2458\n",
      "Iteration 124/140: Train Loss = 0.1995, Valid Loss = 0.2458\n",
      "Iteration 125/140: Train Loss = 0.1992, Valid Loss = 0.2456\n",
      "Iteration 126/140: Train Loss = 0.1988, Valid Loss = 0.2453\n",
      "Iteration 127/140: Train Loss = 0.1985, Valid Loss = 0.2452\n",
      "Iteration 128/140: Train Loss = 0.1982, Valid Loss = 0.2449\n",
      "Iteration 129/140: Train Loss = 0.1979, Valid Loss = 0.2448\n",
      "Iteration 130/140: Train Loss = 0.1976, Valid Loss = 0.2447\n",
      "Iteration 131/140: Train Loss = 0.1972, Valid Loss = 0.2444\n",
      "Iteration 132/140: Train Loss = 0.1969, Valid Loss = 0.2442\n",
      "Iteration 133/140: Train Loss = 0.1966, Valid Loss = 0.2439\n",
      "Iteration 134/140: Train Loss = 0.1963, Valid Loss = 0.2437\n",
      "Iteration 135/140: Train Loss = 0.1959, Valid Loss = 0.2434\n",
      "Iteration 136/140: Train Loss = 0.1956, Valid Loss = 0.2432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:52:45,611] Trial 46 finished with value: 0.9657184036237603 and parameters: {'max_depth': 9, 'min_samples_leaf': 15, 'n_estimators': 140, 'learning_rate': 0.2322519273008503, 'subsample': 0.5963751114541392}. Best is trial 27 with value: 0.9657550072635349.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 137/140: Train Loss = 0.1952, Valid Loss = 0.2432\n",
      "Iteration 138/140: Train Loss = 0.1950, Valid Loss = 0.2430\n",
      "Iteration 139/140: Train Loss = 0.1946, Valid Loss = 0.2430\n",
      "Iteration 140/140: Train Loss = 0.1944, Valid Loss = 0.2427\n",
      "Iteration 1/145: Train Loss = 0.6921, Valid Loss = 0.6922\n",
      "Iteration 2/145: Train Loss = 0.6911, Valid Loss = 0.6912\n",
      "Iteration 3/145: Train Loss = 0.6901, Valid Loss = 0.6903\n",
      "Iteration 4/145: Train Loss = 0.6891, Valid Loss = 0.6893\n",
      "Iteration 5/145: Train Loss = 0.6881, Valid Loss = 0.6883\n",
      "Iteration 6/145: Train Loss = 0.6871, Valid Loss = 0.6874\n",
      "Iteration 7/145: Train Loss = 0.6861, Valid Loss = 0.6864\n",
      "Iteration 8/145: Train Loss = 0.6851, Valid Loss = 0.6855\n",
      "Iteration 9/145: Train Loss = 0.6841, Valid Loss = 0.6845\n",
      "Iteration 10/145: Train Loss = 0.6831, Valid Loss = 0.6836\n",
      "Iteration 11/145: Train Loss = 0.6821, Valid Loss = 0.6826\n",
      "Iteration 12/145: Train Loss = 0.6812, Valid Loss = 0.6817\n",
      "Iteration 13/145: Train Loss = 0.6802, Valid Loss = 0.6807\n",
      "Iteration 14/145: Train Loss = 0.6792, Valid Loss = 0.6798\n",
      "Iteration 15/145: Train Loss = 0.6782, Valid Loss = 0.6789\n",
      "Iteration 16/145: Train Loss = 0.6772, Valid Loss = 0.6779\n",
      "Iteration 17/145: Train Loss = 0.6763, Valid Loss = 0.6770\n",
      "Iteration 18/145: Train Loss = 0.6753, Valid Loss = 0.6761\n",
      "Iteration 19/145: Train Loss = 0.6743, Valid Loss = 0.6751\n",
      "Iteration 20/145: Train Loss = 0.6734, Valid Loss = 0.6742\n",
      "Iteration 21/145: Train Loss = 0.6724, Valid Loss = 0.6733\n",
      "Iteration 22/145: Train Loss = 0.6715, Valid Loss = 0.6724\n",
      "Iteration 23/145: Train Loss = 0.6705, Valid Loss = 0.6715\n",
      "Iteration 24/145: Train Loss = 0.6696, Valid Loss = 0.6706\n",
      "Iteration 25/145: Train Loss = 0.6686, Valid Loss = 0.6697\n",
      "Iteration 26/145: Train Loss = 0.6677, Valid Loss = 0.6688\n",
      "Iteration 27/145: Train Loss = 0.6667, Valid Loss = 0.6679\n",
      "Iteration 28/145: Train Loss = 0.6658, Valid Loss = 0.6670\n",
      "Iteration 29/145: Train Loss = 0.6648, Valid Loss = 0.6661\n",
      "Iteration 30/145: Train Loss = 0.6639, Valid Loss = 0.6652\n",
      "Iteration 31/145: Train Loss = 0.6630, Valid Loss = 0.6643\n",
      "Iteration 32/145: Train Loss = 0.6620, Valid Loss = 0.6634\n",
      "Iteration 33/145: Train Loss = 0.6611, Valid Loss = 0.6625\n",
      "Iteration 34/145: Train Loss = 0.6602, Valid Loss = 0.6617\n",
      "Iteration 35/145: Train Loss = 0.6593, Valid Loss = 0.6608\n",
      "Iteration 36/145: Train Loss = 0.6583, Valid Loss = 0.6599\n",
      "Iteration 37/145: Train Loss = 0.6574, Valid Loss = 0.6590\n",
      "Iteration 38/145: Train Loss = 0.6565, Valid Loss = 0.6581\n",
      "Iteration 39/145: Train Loss = 0.6556, Valid Loss = 0.6572\n",
      "Iteration 40/145: Train Loss = 0.6547, Valid Loss = 0.6564\n",
      "Iteration 41/145: Train Loss = 0.6538, Valid Loss = 0.6555\n",
      "Iteration 42/145: Train Loss = 0.6528, Valid Loss = 0.6546\n",
      "Iteration 43/145: Train Loss = 0.6520, Valid Loss = 0.6538\n",
      "Iteration 44/145: Train Loss = 0.6511, Valid Loss = 0.6529\n",
      "Iteration 45/145: Train Loss = 0.6501, Valid Loss = 0.6521\n",
      "Iteration 46/145: Train Loss = 0.6492, Valid Loss = 0.6512\n",
      "Iteration 47/145: Train Loss = 0.6484, Valid Loss = 0.6503\n",
      "Iteration 48/145: Train Loss = 0.6475, Valid Loss = 0.6495\n",
      "Iteration 49/145: Train Loss = 0.6466, Valid Loss = 0.6487\n",
      "Iteration 50/145: Train Loss = 0.6457, Valid Loss = 0.6478\n",
      "Iteration 51/145: Train Loss = 0.6448, Valid Loss = 0.6469\n",
      "Iteration 52/145: Train Loss = 0.6439, Valid Loss = 0.6461\n",
      "Iteration 53/145: Train Loss = 0.6430, Valid Loss = 0.6453\n",
      "Iteration 54/145: Train Loss = 0.6422, Valid Loss = 0.6444\n",
      "Iteration 55/145: Train Loss = 0.6413, Valid Loss = 0.6436\n",
      "Iteration 56/145: Train Loss = 0.6404, Valid Loss = 0.6427\n",
      "Iteration 57/145: Train Loss = 0.6395, Valid Loss = 0.6419\n",
      "Iteration 58/145: Train Loss = 0.6387, Valid Loss = 0.6411\n",
      "Iteration 59/145: Train Loss = 0.6378, Valid Loss = 0.6402\n",
      "Iteration 60/145: Train Loss = 0.6369, Valid Loss = 0.6394\n",
      "Iteration 61/145: Train Loss = 0.6361, Valid Loss = 0.6386\n",
      "Iteration 62/145: Train Loss = 0.6352, Valid Loss = 0.6378\n",
      "Iteration 63/145: Train Loss = 0.6344, Valid Loss = 0.6369\n",
      "Iteration 64/145: Train Loss = 0.6335, Valid Loss = 0.6361\n",
      "Iteration 65/145: Train Loss = 0.6326, Valid Loss = 0.6353\n",
      "Iteration 66/145: Train Loss = 0.6318, Valid Loss = 0.6345\n",
      "Iteration 67/145: Train Loss = 0.6310, Valid Loss = 0.6337\n",
      "Iteration 68/145: Train Loss = 0.6301, Valid Loss = 0.6329\n",
      "Iteration 69/145: Train Loss = 0.6293, Valid Loss = 0.6321\n",
      "Iteration 70/145: Train Loss = 0.6284, Valid Loss = 0.6313\n",
      "Iteration 71/145: Train Loss = 0.6276, Valid Loss = 0.6305\n",
      "Iteration 72/145: Train Loss = 0.6268, Valid Loss = 0.6297\n",
      "Iteration 73/145: Train Loss = 0.6259, Valid Loss = 0.6289\n",
      "Iteration 74/145: Train Loss = 0.6251, Valid Loss = 0.6281\n",
      "Iteration 75/145: Train Loss = 0.6242, Valid Loss = 0.6273\n",
      "Iteration 76/145: Train Loss = 0.6234, Valid Loss = 0.6265\n",
      "Iteration 77/145: Train Loss = 0.6226, Valid Loss = 0.6257\n",
      "Iteration 78/145: Train Loss = 0.6218, Valid Loss = 0.6250\n",
      "Iteration 79/145: Train Loss = 0.6210, Valid Loss = 0.6242\n",
      "Iteration 80/145: Train Loss = 0.6201, Valid Loss = 0.6234\n",
      "Iteration 81/145: Train Loss = 0.6193, Valid Loss = 0.6226\n",
      "Iteration 82/145: Train Loss = 0.6185, Valid Loss = 0.6218\n",
      "Iteration 83/145: Train Loss = 0.6177, Valid Loss = 0.6211\n",
      "Iteration 84/145: Train Loss = 0.6169, Valid Loss = 0.6203\n",
      "Iteration 85/145: Train Loss = 0.6161, Valid Loss = 0.6195\n",
      "Iteration 86/145: Train Loss = 0.6153, Valid Loss = 0.6188\n",
      "Iteration 87/145: Train Loss = 0.6145, Valid Loss = 0.6180\n",
      "Iteration 88/145: Train Loss = 0.6137, Valid Loss = 0.6172\n",
      "Iteration 89/145: Train Loss = 0.6129, Valid Loss = 0.6165\n",
      "Iteration 90/145: Train Loss = 0.6121, Valid Loss = 0.6157\n",
      "Iteration 91/145: Train Loss = 0.6113, Valid Loss = 0.6149\n",
      "Iteration 92/145: Train Loss = 0.6105, Valid Loss = 0.6142\n",
      "Iteration 93/145: Train Loss = 0.6097, Valid Loss = 0.6134\n",
      "Iteration 94/145: Train Loss = 0.6089, Valid Loss = 0.6126\n",
      "Iteration 95/145: Train Loss = 0.6081, Valid Loss = 0.6119\n",
      "Iteration 96/145: Train Loss = 0.6073, Valid Loss = 0.6111\n",
      "Iteration 97/145: Train Loss = 0.6066, Valid Loss = 0.6104\n",
      "Iteration 98/145: Train Loss = 0.6058, Valid Loss = 0.6097\n",
      "Iteration 99/145: Train Loss = 0.6050, Valid Loss = 0.6089\n",
      "Iteration 100/145: Train Loss = 0.6042, Valid Loss = 0.6082\n",
      "Iteration 101/145: Train Loss = 0.6034, Valid Loss = 0.6074\n",
      "Iteration 102/145: Train Loss = 0.6027, Valid Loss = 0.6067\n",
      "Iteration 103/145: Train Loss = 0.6019, Valid Loss = 0.6060\n",
      "Iteration 104/145: Train Loss = 0.6011, Valid Loss = 0.6053\n",
      "Iteration 105/145: Train Loss = 0.6003, Valid Loss = 0.6045\n",
      "Iteration 106/145: Train Loss = 0.5996, Valid Loss = 0.6038\n",
      "Iteration 107/145: Train Loss = 0.5988, Valid Loss = 0.6031\n",
      "Iteration 108/145: Train Loss = 0.5980, Valid Loss = 0.6023\n",
      "Iteration 109/145: Train Loss = 0.5973, Valid Loss = 0.6016\n",
      "Iteration 110/145: Train Loss = 0.5965, Valid Loss = 0.6009\n",
      "Iteration 111/145: Train Loss = 0.5958, Valid Loss = 0.6002\n",
      "Iteration 112/145: Train Loss = 0.5950, Valid Loss = 0.5994\n",
      "Iteration 113/145: Train Loss = 0.5942, Valid Loss = 0.5987\n",
      "Iteration 114/145: Train Loss = 0.5935, Valid Loss = 0.5980\n",
      "Iteration 115/145: Train Loss = 0.5927, Valid Loss = 0.5973\n",
      "Iteration 116/145: Train Loss = 0.5920, Valid Loss = 0.5966\n",
      "Iteration 117/145: Train Loss = 0.5912, Valid Loss = 0.5959\n",
      "Iteration 118/145: Train Loss = 0.5905, Valid Loss = 0.5952\n",
      "Iteration 119/145: Train Loss = 0.5898, Valid Loss = 0.5945\n",
      "Iteration 120/145: Train Loss = 0.5890, Valid Loss = 0.5938\n",
      "Iteration 121/145: Train Loss = 0.5883, Valid Loss = 0.5931\n",
      "Iteration 122/145: Train Loss = 0.5876, Valid Loss = 0.5924\n",
      "Iteration 123/145: Train Loss = 0.5868, Valid Loss = 0.5917\n",
      "Iteration 124/145: Train Loss = 0.5861, Valid Loss = 0.5910\n",
      "Iteration 125/145: Train Loss = 0.5854, Valid Loss = 0.5903\n",
      "Iteration 126/145: Train Loss = 0.5847, Valid Loss = 0.5896\n",
      "Iteration 127/145: Train Loss = 0.5839, Valid Loss = 0.5890\n",
      "Iteration 128/145: Train Loss = 0.5832, Valid Loss = 0.5883\n",
      "Iteration 129/145: Train Loss = 0.5825, Valid Loss = 0.5876\n",
      "Iteration 130/145: Train Loss = 0.5818, Valid Loss = 0.5869\n",
      "Iteration 131/145: Train Loss = 0.5811, Valid Loss = 0.5862\n",
      "Iteration 132/145: Train Loss = 0.5803, Valid Loss = 0.5855\n",
      "Iteration 133/145: Train Loss = 0.5796, Valid Loss = 0.5849\n",
      "Iteration 134/145: Train Loss = 0.5789, Valid Loss = 0.5842\n",
      "Iteration 135/145: Train Loss = 0.5782, Valid Loss = 0.5835\n",
      "Iteration 136/145: Train Loss = 0.5775, Valid Loss = 0.5829\n",
      "Iteration 137/145: Train Loss = 0.5768, Valid Loss = 0.5822\n",
      "Iteration 138/145: Train Loss = 0.5761, Valid Loss = 0.5815\n",
      "Iteration 139/145: Train Loss = 0.5754, Valid Loss = 0.5809\n",
      "Iteration 140/145: Train Loss = 0.5747, Valid Loss = 0.5802\n",
      "Iteration 141/145: Train Loss = 0.5740, Valid Loss = 0.5796\n",
      "Iteration 142/145: Train Loss = 0.5733, Valid Loss = 0.5789\n",
      "Iteration 143/145: Train Loss = 0.5726, Valid Loss = 0.5783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:52:59,673] Trial 47 finished with value: 0.9615547395994188 and parameters: {'max_depth': 11, 'min_samples_leaf': 10, 'n_estimators': 145, 'learning_rate': 0.00564316548506962, 'subsample': 0.7027446954005794}. Best is trial 27 with value: 0.9657550072635349.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 144/145: Train Loss = 0.5719, Valid Loss = 0.5776\n",
      "Iteration 145/145: Train Loss = 0.5713, Valid Loss = 0.5769\n",
      "Iteration 1/125: Train Loss = 0.6821, Valid Loss = 0.6824\n",
      "Iteration 2/125: Train Loss = 0.6713, Valid Loss = 0.6721\n",
      "Iteration 3/125: Train Loss = 0.6608, Valid Loss = 0.6621\n",
      "Iteration 4/125: Train Loss = 0.6508, Valid Loss = 0.6524\n",
      "Iteration 5/125: Train Loss = 0.6410, Valid Loss = 0.6430\n",
      "Iteration 6/125: Train Loss = 0.6315, Valid Loss = 0.6339\n",
      "Iteration 7/125: Train Loss = 0.6222, Valid Loss = 0.6252\n",
      "Iteration 8/125: Train Loss = 0.6132, Valid Loss = 0.6166\n",
      "Iteration 9/125: Train Loss = 0.6044, Valid Loss = 0.6081\n",
      "Iteration 10/125: Train Loss = 0.5961, Valid Loss = 0.6003\n",
      "Iteration 11/125: Train Loss = 0.5879, Valid Loss = 0.5924\n",
      "Iteration 12/125: Train Loss = 0.5800, Valid Loss = 0.5848\n",
      "Iteration 13/125: Train Loss = 0.5723, Valid Loss = 0.5774\n",
      "Iteration 14/125: Train Loss = 0.5648, Valid Loss = 0.5703\n",
      "Iteration 15/125: Train Loss = 0.5575, Valid Loss = 0.5633\n",
      "Iteration 16/125: Train Loss = 0.5504, Valid Loss = 0.5565\n",
      "Iteration 17/125: Train Loss = 0.5435, Valid Loss = 0.5500\n",
      "Iteration 18/125: Train Loss = 0.5368, Valid Loss = 0.5437\n",
      "Iteration 19/125: Train Loss = 0.5302, Valid Loss = 0.5375\n",
      "Iteration 20/125: Train Loss = 0.5239, Valid Loss = 0.5314\n",
      "Iteration 21/125: Train Loss = 0.5177, Valid Loss = 0.5256\n",
      "Iteration 22/125: Train Loss = 0.5118, Valid Loss = 0.5198\n",
      "Iteration 23/125: Train Loss = 0.5061, Valid Loss = 0.5143\n",
      "Iteration 24/125: Train Loss = 0.5005, Valid Loss = 0.5090\n",
      "Iteration 25/125: Train Loss = 0.4950, Valid Loss = 0.5039\n",
      "Iteration 26/125: Train Loss = 0.4897, Valid Loss = 0.4989\n",
      "Iteration 27/125: Train Loss = 0.4844, Valid Loss = 0.4938\n",
      "Iteration 28/125: Train Loss = 0.4794, Valid Loss = 0.4890\n",
      "Iteration 29/125: Train Loss = 0.4744, Valid Loss = 0.4843\n",
      "Iteration 30/125: Train Loss = 0.4696, Valid Loss = 0.4799\n",
      "Iteration 31/125: Train Loss = 0.4649, Valid Loss = 0.4754\n",
      "Iteration 32/125: Train Loss = 0.4604, Valid Loss = 0.4711\n",
      "Iteration 33/125: Train Loss = 0.4560, Valid Loss = 0.4669\n",
      "Iteration 34/125: Train Loss = 0.4517, Valid Loss = 0.4628\n",
      "Iteration 35/125: Train Loss = 0.4474, Valid Loss = 0.4588\n",
      "Iteration 36/125: Train Loss = 0.4433, Valid Loss = 0.4549\n",
      "Iteration 37/125: Train Loss = 0.4393, Valid Loss = 0.4513\n",
      "Iteration 38/125: Train Loss = 0.4354, Valid Loss = 0.4477\n",
      "Iteration 39/125: Train Loss = 0.4316, Valid Loss = 0.4442\n",
      "Iteration 40/125: Train Loss = 0.4279, Valid Loss = 0.4408\n",
      "Iteration 41/125: Train Loss = 0.4242, Valid Loss = 0.4374\n",
      "Iteration 42/125: Train Loss = 0.4207, Valid Loss = 0.4342\n",
      "Iteration 43/125: Train Loss = 0.4173, Valid Loss = 0.4309\n",
      "Iteration 44/125: Train Loss = 0.4139, Valid Loss = 0.4278\n",
      "Iteration 45/125: Train Loss = 0.4106, Valid Loss = 0.4247\n",
      "Iteration 46/125: Train Loss = 0.4073, Valid Loss = 0.4217\n",
      "Iteration 47/125: Train Loss = 0.4042, Valid Loss = 0.4189\n",
      "Iteration 48/125: Train Loss = 0.4011, Valid Loss = 0.4160\n",
      "Iteration 49/125: Train Loss = 0.3980, Valid Loss = 0.4134\n",
      "Iteration 50/125: Train Loss = 0.3951, Valid Loss = 0.4106\n",
      "Iteration 51/125: Train Loss = 0.3923, Valid Loss = 0.4080\n",
      "Iteration 52/125: Train Loss = 0.3894, Valid Loss = 0.4054\n",
      "Iteration 53/125: Train Loss = 0.3866, Valid Loss = 0.4029\n",
      "Iteration 54/125: Train Loss = 0.3838, Valid Loss = 0.4004\n",
      "Iteration 55/125: Train Loss = 0.3812, Valid Loss = 0.3981\n",
      "Iteration 56/125: Train Loss = 0.3787, Valid Loss = 0.3959\n",
      "Iteration 57/125: Train Loss = 0.3762, Valid Loss = 0.3937\n",
      "Iteration 58/125: Train Loss = 0.3737, Valid Loss = 0.3915\n",
      "Iteration 59/125: Train Loss = 0.3714, Valid Loss = 0.3892\n",
      "Iteration 60/125: Train Loss = 0.3690, Valid Loss = 0.3870\n",
      "Iteration 61/125: Train Loss = 0.3667, Valid Loss = 0.3849\n",
      "Iteration 62/125: Train Loss = 0.3644, Valid Loss = 0.3829\n",
      "Iteration 63/125: Train Loss = 0.3622, Valid Loss = 0.3808\n",
      "Iteration 64/125: Train Loss = 0.3600, Valid Loss = 0.3789\n",
      "Iteration 65/125: Train Loss = 0.3579, Valid Loss = 0.3769\n",
      "Iteration 66/125: Train Loss = 0.3558, Valid Loss = 0.3751\n",
      "Iteration 67/125: Train Loss = 0.3537, Valid Loss = 0.3732\n",
      "Iteration 68/125: Train Loss = 0.3517, Valid Loss = 0.3713\n",
      "Iteration 69/125: Train Loss = 0.3497, Valid Loss = 0.3695\n",
      "Iteration 70/125: Train Loss = 0.3477, Valid Loss = 0.3677\n",
      "Iteration 71/125: Train Loss = 0.3458, Valid Loss = 0.3660\n",
      "Iteration 72/125: Train Loss = 0.3439, Valid Loss = 0.3643\n",
      "Iteration 73/125: Train Loss = 0.3420, Valid Loss = 0.3626\n",
      "Iteration 74/125: Train Loss = 0.3402, Valid Loss = 0.3610\n",
      "Iteration 75/125: Train Loss = 0.3385, Valid Loss = 0.3595\n",
      "Iteration 76/125: Train Loss = 0.3367, Valid Loss = 0.3580\n",
      "Iteration 77/125: Train Loss = 0.3350, Valid Loss = 0.3565\n",
      "Iteration 78/125: Train Loss = 0.3333, Valid Loss = 0.3549\n",
      "Iteration 79/125: Train Loss = 0.3316, Valid Loss = 0.3535\n",
      "Iteration 80/125: Train Loss = 0.3300, Valid Loss = 0.3520\n",
      "Iteration 81/125: Train Loss = 0.3285, Valid Loss = 0.3505\n",
      "Iteration 82/125: Train Loss = 0.3269, Valid Loss = 0.3492\n",
      "Iteration 83/125: Train Loss = 0.3254, Valid Loss = 0.3478\n",
      "Iteration 84/125: Train Loss = 0.3238, Valid Loss = 0.3465\n",
      "Iteration 85/125: Train Loss = 0.3224, Valid Loss = 0.3452\n",
      "Iteration 86/125: Train Loss = 0.3210, Valid Loss = 0.3440\n",
      "Iteration 87/125: Train Loss = 0.3195, Valid Loss = 0.3427\n",
      "Iteration 88/125: Train Loss = 0.3181, Valid Loss = 0.3414\n",
      "Iteration 89/125: Train Loss = 0.3167, Valid Loss = 0.3401\n",
      "Iteration 90/125: Train Loss = 0.3154, Valid Loss = 0.3390\n",
      "Iteration 91/125: Train Loss = 0.3140, Valid Loss = 0.3378\n",
      "Iteration 92/125: Train Loss = 0.3127, Valid Loss = 0.3366\n",
      "Iteration 93/125: Train Loss = 0.3114, Valid Loss = 0.3355\n",
      "Iteration 94/125: Train Loss = 0.3101, Valid Loss = 0.3343\n",
      "Iteration 95/125: Train Loss = 0.3088, Valid Loss = 0.3331\n",
      "Iteration 96/125: Train Loss = 0.3075, Valid Loss = 0.3320\n",
      "Iteration 97/125: Train Loss = 0.3063, Valid Loss = 0.3309\n",
      "Iteration 98/125: Train Loss = 0.3051, Valid Loss = 0.3299\n",
      "Iteration 99/125: Train Loss = 0.3038, Valid Loss = 0.3288\n",
      "Iteration 100/125: Train Loss = 0.3027, Valid Loss = 0.3278\n",
      "Iteration 101/125: Train Loss = 0.3016, Valid Loss = 0.3268\n",
      "Iteration 102/125: Train Loss = 0.3004, Valid Loss = 0.3258\n",
      "Iteration 103/125: Train Loss = 0.2992, Valid Loss = 0.3248\n",
      "Iteration 104/125: Train Loss = 0.2981, Valid Loss = 0.3239\n",
      "Iteration 105/125: Train Loss = 0.2970, Valid Loss = 0.3229\n",
      "Iteration 106/125: Train Loss = 0.2960, Valid Loss = 0.3220\n",
      "Iteration 107/125: Train Loss = 0.2949, Valid Loss = 0.3211\n",
      "Iteration 108/125: Train Loss = 0.2939, Valid Loss = 0.3202\n",
      "Iteration 109/125: Train Loss = 0.2928, Valid Loss = 0.3193\n",
      "Iteration 110/125: Train Loss = 0.2918, Valid Loss = 0.3185\n",
      "Iteration 111/125: Train Loss = 0.2908, Valid Loss = 0.3177\n",
      "Iteration 112/125: Train Loss = 0.2898, Valid Loss = 0.3169\n",
      "Iteration 113/125: Train Loss = 0.2889, Valid Loss = 0.3160\n",
      "Iteration 114/125: Train Loss = 0.2879, Valid Loss = 0.3152\n",
      "Iteration 115/125: Train Loss = 0.2870, Valid Loss = 0.3144\n",
      "Iteration 116/125: Train Loss = 0.2861, Valid Loss = 0.3136\n",
      "Iteration 117/125: Train Loss = 0.2851, Valid Loss = 0.3127\n",
      "Iteration 118/125: Train Loss = 0.2841, Valid Loss = 0.3120\n",
      "Iteration 119/125: Train Loss = 0.2833, Valid Loss = 0.3113\n",
      "Iteration 120/125: Train Loss = 0.2824, Valid Loss = 0.3105\n",
      "Iteration 121/125: Train Loss = 0.2815, Valid Loss = 0.3098\n",
      "Iteration 122/125: Train Loss = 0.2806, Valid Loss = 0.3091\n",
      "Iteration 123/125: Train Loss = 0.2798, Valid Loss = 0.3084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:53:10,886] Trial 48 finished with value: 0.9634768119754908 and parameters: {'max_depth': 9, 'min_samples_leaf': 12, 'n_estimators': 125, 'learning_rate': 0.06335204897413477, 'subsample': 0.9518476761163901}. Best is trial 27 with value: 0.9657550072635349.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 124/125: Train Loss = 0.2789, Valid Loss = 0.3078\n",
      "Iteration 125/125: Train Loss = 0.2781, Valid Loss = 0.3071\n",
      "Iteration 1/140: Train Loss = 0.6521, Valid Loss = 0.6537\n",
      "Iteration 2/140: Train Loss = 0.6157, Valid Loss = 0.6189\n",
      "Iteration 3/140: Train Loss = 0.5835, Valid Loss = 0.5877\n",
      "Iteration 4/140: Train Loss = 0.5547, Valid Loss = 0.5600\n",
      "Iteration 5/140: Train Loss = 0.5289, Valid Loss = 0.5351\n",
      "Iteration 6/140: Train Loss = 0.5059, Valid Loss = 0.5130\n",
      "Iteration 7/140: Train Loss = 0.4852, Valid Loss = 0.4936\n",
      "Iteration 8/140: Train Loss = 0.4663, Valid Loss = 0.4757\n",
      "Iteration 9/140: Train Loss = 0.4493, Valid Loss = 0.4599\n",
      "Iteration 10/140: Train Loss = 0.4338, Valid Loss = 0.4455\n",
      "Iteration 11/140: Train Loss = 0.4199, Valid Loss = 0.4325\n",
      "Iteration 12/140: Train Loss = 0.4072, Valid Loss = 0.4207\n",
      "Iteration 13/140: Train Loss = 0.3956, Valid Loss = 0.4098\n",
      "Iteration 14/140: Train Loss = 0.3849, Valid Loss = 0.3998\n",
      "Iteration 15/140: Train Loss = 0.3751, Valid Loss = 0.3908\n",
      "Iteration 16/140: Train Loss = 0.3660, Valid Loss = 0.3829\n",
      "Iteration 17/140: Train Loss = 0.3578, Valid Loss = 0.3758\n",
      "Iteration 18/140: Train Loss = 0.3501, Valid Loss = 0.3687\n",
      "Iteration 19/140: Train Loss = 0.3430, Valid Loss = 0.3625\n",
      "Iteration 20/140: Train Loss = 0.3363, Valid Loss = 0.3569\n",
      "Iteration 21/140: Train Loss = 0.3299, Valid Loss = 0.3513\n",
      "Iteration 22/140: Train Loss = 0.3241, Valid Loss = 0.3462\n",
      "Iteration 23/140: Train Loss = 0.3186, Valid Loss = 0.3415\n",
      "Iteration 24/140: Train Loss = 0.3134, Valid Loss = 0.3367\n",
      "Iteration 25/140: Train Loss = 0.3085, Valid Loss = 0.3326\n",
      "Iteration 26/140: Train Loss = 0.3039, Valid Loss = 0.3286\n",
      "Iteration 27/140: Train Loss = 0.2995, Valid Loss = 0.3247\n",
      "Iteration 28/140: Train Loss = 0.2952, Valid Loss = 0.3211\n",
      "Iteration 29/140: Train Loss = 0.2914, Valid Loss = 0.3176\n",
      "Iteration 30/140: Train Loss = 0.2877, Valid Loss = 0.3145\n",
      "Iteration 31/140: Train Loss = 0.2842, Valid Loss = 0.3117\n",
      "Iteration 32/140: Train Loss = 0.2808, Valid Loss = 0.3085\n",
      "Iteration 33/140: Train Loss = 0.2777, Valid Loss = 0.3062\n",
      "Iteration 34/140: Train Loss = 0.2748, Valid Loss = 0.3040\n",
      "Iteration 35/140: Train Loss = 0.2721, Valid Loss = 0.3018\n",
      "Iteration 36/140: Train Loss = 0.2694, Valid Loss = 0.2995\n",
      "Iteration 37/140: Train Loss = 0.2667, Valid Loss = 0.2971\n",
      "Iteration 38/140: Train Loss = 0.2641, Valid Loss = 0.2953\n",
      "Iteration 39/140: Train Loss = 0.2618, Valid Loss = 0.2931\n",
      "Iteration 40/140: Train Loss = 0.2595, Valid Loss = 0.2913\n",
      "Iteration 41/140: Train Loss = 0.2574, Valid Loss = 0.2897\n",
      "Iteration 42/140: Train Loss = 0.2552, Valid Loss = 0.2880\n",
      "Iteration 43/140: Train Loss = 0.2531, Valid Loss = 0.2863\n",
      "Iteration 44/140: Train Loss = 0.2510, Valid Loss = 0.2848\n",
      "Iteration 45/140: Train Loss = 0.2492, Valid Loss = 0.2834\n",
      "Iteration 46/140: Train Loss = 0.2475, Valid Loss = 0.2820\n",
      "Iteration 47/140: Train Loss = 0.2458, Valid Loss = 0.2810\n",
      "Iteration 48/140: Train Loss = 0.2440, Valid Loss = 0.2795\n",
      "Iteration 49/140: Train Loss = 0.2425, Valid Loss = 0.2784\n",
      "Iteration 50/140: Train Loss = 0.2409, Valid Loss = 0.2772\n",
      "Iteration 51/140: Train Loss = 0.2393, Valid Loss = 0.2762\n",
      "Iteration 52/140: Train Loss = 0.2379, Valid Loss = 0.2750\n",
      "Iteration 53/140: Train Loss = 0.2366, Valid Loss = 0.2741\n",
      "Iteration 54/140: Train Loss = 0.2353, Valid Loss = 0.2732\n",
      "Iteration 55/140: Train Loss = 0.2339, Valid Loss = 0.2724\n",
      "Iteration 56/140: Train Loss = 0.2325, Valid Loss = 0.2714\n",
      "Iteration 57/140: Train Loss = 0.2311, Valid Loss = 0.2705\n",
      "Iteration 58/140: Train Loss = 0.2300, Valid Loss = 0.2697\n",
      "Iteration 59/140: Train Loss = 0.2288, Valid Loss = 0.2687\n",
      "Iteration 60/140: Train Loss = 0.2277, Valid Loss = 0.2680\n",
      "Iteration 61/140: Train Loss = 0.2264, Valid Loss = 0.2669\n",
      "Iteration 62/140: Train Loss = 0.2252, Valid Loss = 0.2662\n",
      "Iteration 63/140: Train Loss = 0.2241, Valid Loss = 0.2654\n",
      "Iteration 64/140: Train Loss = 0.2231, Valid Loss = 0.2648\n",
      "Iteration 65/140: Train Loss = 0.2220, Valid Loss = 0.2641\n",
      "Iteration 66/140: Train Loss = 0.2209, Valid Loss = 0.2632\n",
      "Iteration 67/140: Train Loss = 0.2198, Valid Loss = 0.2627\n",
      "Iteration 68/140: Train Loss = 0.2187, Valid Loss = 0.2621\n",
      "Iteration 69/140: Train Loss = 0.2177, Valid Loss = 0.2612\n",
      "Iteration 70/140: Train Loss = 0.2168, Valid Loss = 0.2607\n",
      "Iteration 71/140: Train Loss = 0.2159, Valid Loss = 0.2603\n",
      "Iteration 72/140: Train Loss = 0.2151, Valid Loss = 0.2599\n",
      "Iteration 73/140: Train Loss = 0.2142, Valid Loss = 0.2594\n",
      "Iteration 74/140: Train Loss = 0.2133, Valid Loss = 0.2588\n",
      "Iteration 75/140: Train Loss = 0.2126, Valid Loss = 0.2581\n",
      "Iteration 76/140: Train Loss = 0.2118, Valid Loss = 0.2577\n",
      "Iteration 77/140: Train Loss = 0.2112, Valid Loss = 0.2572\n",
      "Iteration 78/140: Train Loss = 0.2103, Valid Loss = 0.2567\n",
      "Iteration 79/140: Train Loss = 0.2096, Valid Loss = 0.2563\n",
      "Iteration 80/140: Train Loss = 0.2088, Valid Loss = 0.2559\n",
      "Iteration 81/140: Train Loss = 0.2082, Valid Loss = 0.2557\n",
      "Iteration 82/140: Train Loss = 0.2074, Valid Loss = 0.2553\n",
      "Iteration 83/140: Train Loss = 0.2067, Valid Loss = 0.2550\n",
      "Iteration 84/140: Train Loss = 0.2060, Valid Loss = 0.2546\n",
      "Iteration 85/140: Train Loss = 0.2052, Valid Loss = 0.2541\n",
      "Iteration 86/140: Train Loss = 0.2046, Valid Loss = 0.2537\n",
      "Iteration 87/140: Train Loss = 0.2040, Valid Loss = 0.2532\n",
      "Iteration 88/140: Train Loss = 0.2034, Valid Loss = 0.2531\n",
      "Iteration 89/140: Train Loss = 0.2027, Valid Loss = 0.2527\n",
      "Iteration 90/140: Train Loss = 0.2022, Valid Loss = 0.2524\n",
      "Iteration 91/140: Train Loss = 0.2017, Valid Loss = 0.2523\n",
      "Iteration 92/140: Train Loss = 0.2011, Valid Loss = 0.2518\n",
      "Iteration 93/140: Train Loss = 0.2005, Valid Loss = 0.2514\n",
      "Iteration 94/140: Train Loss = 0.2000, Valid Loss = 0.2513\n",
      "Iteration 95/140: Train Loss = 0.1995, Valid Loss = 0.2511\n",
      "Iteration 96/140: Train Loss = 0.1990, Valid Loss = 0.2508\n",
      "Iteration 97/140: Train Loss = 0.1985, Valid Loss = 0.2504\n",
      "Iteration 98/140: Train Loss = 0.1980, Valid Loss = 0.2500\n",
      "Iteration 99/140: Train Loss = 0.1975, Valid Loss = 0.2496\n",
      "Iteration 100/140: Train Loss = 0.1969, Valid Loss = 0.2494\n",
      "Iteration 101/140: Train Loss = 0.1964, Valid Loss = 0.2492\n",
      "Iteration 102/140: Train Loss = 0.1958, Valid Loss = 0.2491\n",
      "Iteration 103/140: Train Loss = 0.1952, Valid Loss = 0.2489\n",
      "Iteration 104/140: Train Loss = 0.1947, Valid Loss = 0.2486\n",
      "Iteration 105/140: Train Loss = 0.1942, Valid Loss = 0.2484\n",
      "Iteration 106/140: Train Loss = 0.1937, Valid Loss = 0.2484\n",
      "Iteration 107/140: Train Loss = 0.1932, Valid Loss = 0.2481\n",
      "Iteration 108/140: Train Loss = 0.1928, Valid Loss = 0.2480\n",
      "Iteration 109/140: Train Loss = 0.1923, Valid Loss = 0.2478\n",
      "Iteration 110/140: Train Loss = 0.1919, Valid Loss = 0.2476\n",
      "Iteration 111/140: Train Loss = 0.1915, Valid Loss = 0.2475\n",
      "Iteration 112/140: Train Loss = 0.1910, Valid Loss = 0.2473\n",
      "Iteration 113/140: Train Loss = 0.1906, Valid Loss = 0.2470\n",
      "Iteration 114/140: Train Loss = 0.1902, Valid Loss = 0.2469\n",
      "Iteration 115/140: Train Loss = 0.1897, Valid Loss = 0.2467\n",
      "Iteration 116/140: Train Loss = 0.1894, Valid Loss = 0.2465\n",
      "Iteration 117/140: Train Loss = 0.1889, Valid Loss = 0.2464\n",
      "Iteration 118/140: Train Loss = 0.1885, Valid Loss = 0.2462\n",
      "Iteration 119/140: Train Loss = 0.1881, Valid Loss = 0.2461\n",
      "Iteration 120/140: Train Loss = 0.1878, Valid Loss = 0.2461\n",
      "Iteration 121/140: Train Loss = 0.1873, Valid Loss = 0.2461\n",
      "Iteration 122/140: Train Loss = 0.1870, Valid Loss = 0.2461\n",
      "Iteration 123/140: Train Loss = 0.1867, Valid Loss = 0.2459\n",
      "Iteration 124/140: Train Loss = 0.1863, Valid Loss = 0.2458\n",
      "Iteration 125/140: Train Loss = 0.1860, Valid Loss = 0.2457\n",
      "Iteration 126/140: Train Loss = 0.1856, Valid Loss = 0.2456\n",
      "Iteration 127/140: Train Loss = 0.1853, Valid Loss = 0.2454\n",
      "Iteration 128/140: Train Loss = 0.1849, Valid Loss = 0.2454\n",
      "Iteration 129/140: Train Loss = 0.1846, Valid Loss = 0.2454\n",
      "Iteration 130/140: Train Loss = 0.1843, Valid Loss = 0.2454\n",
      "Iteration 131/140: Train Loss = 0.1838, Valid Loss = 0.2454\n",
      "Iteration 132/140: Train Loss = 0.1833, Valid Loss = 0.2453\n",
      "Iteration 133/140: Train Loss = 0.1830, Valid Loss = 0.2452\n",
      "Iteration 134/140: Train Loss = 0.1826, Valid Loss = 0.2450\n",
      "Iteration 135/140: Train Loss = 0.1823, Valid Loss = 0.2449\n",
      "Iteration 136/140: Train Loss = 0.1821, Valid Loss = 0.2449\n",
      "Iteration 137/140: Train Loss = 0.1818, Valid Loss = 0.2448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-05 00:53:21,911] Trial 49 finished with value: 0.9645203969969764 and parameters: {'max_depth': 11, 'min_samples_leaf': 19, 'n_estimators': 140, 'learning_rate': 0.24027093000604519, 'subsample': 0.7694667263647016}. Best is trial 27 with value: 0.9657550072635349.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 138/140: Train Loss = 0.1815, Valid Loss = 0.2447\n",
      "Iteration 139/140: Train Loss = 0.1812, Valid Loss = 0.2447\n",
      "Iteration 140/140: Train Loss = 0.1810, Valid Loss = 0.2446\n",
      "Best trial:\n",
      "  Value (AUC) = 0.9657550072635349\n",
      "  Params = \n",
      "    max_depth: 7\n",
      "    min_samples_leaf: 11\n",
      "    n_estimators: 140\n",
      "    learning_rate: 0.3219266375247475\n",
      "    subsample: 0.7623886541997188\n"
     ]
    }
   ],
   "source": [
    "### ╰( ͡☉ ͜ʖ ͡☉ )つ──☆*:・ﾟ   ฅ^•ﻌ•^ฅ   ʕ•ᴥ•ʔ\n",
    "\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 1, 15)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 20)\n",
    "\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 10, 150, step=5)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-3, 1.0, log=True)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.3, 1.0)\n",
    "\n",
    "    model = Boosting(\n",
    "        base_model_params={\n",
    "            \"max_depth\": max_depth,\n",
    "            \"min_samples_leaf\": min_samples_leaf\n",
    "        },\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        subsample=subsample,\n",
    "        early_stopping_rounds=10,\n",
    "        plot=False\n",
    "    )\n",
    "\n",
    "    model.fit(x_train, y_train, x_valid, y_valid)\n",
    "\n",
    "    valid_auc = model.score(x_valid, y_valid)\n",
    "\n",
    "    return valid_auc\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(f\"  Value (AUC) = {study.best_trial.value}\")\n",
    "print(\"  Params = \")\n",
    "for key, val in study.best_trial.params.items():\n",
    "    print(f\"    {key}: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGk-Wt_slWlV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задание 4. Интерпретация градиентного бустинга [1 балл]\n",
    "\n",
    "Постройте калибровочную кривую для вашей лучшей модели градиентного бустинга и оцените, насколько точно модель предсказывает вероятности.\n",
    "\n",
    "**Инструкция:**\n",
    "1. Постройте калибровочную кривую для лучшей модели градиентного бустинга.\n",
    "2. Постройте аналогичную кривую для логистической регрессии.\n",
    "3. Сравните полученные результаты и проанализируйте, насколько хорошо каждая модель оценивает вероятности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    \"max_depth\": 7,\n",
    "    \"min_samples_leaf\": 11,\n",
    "    \"n_estimators\": 140,\n",
    "    \"learning_rate\": 0.3219266375247475,\n",
    "    \"subsample\": 0.7623886541997188\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-05T10:28:26.095212200Z",
     "start_time": "2025-01-05T10:28:26.043883800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/140: Train Loss = 0.6405, Valid Loss = 0.6418\n",
      "Iteration 2/140: Train Loss = 0.5959, Valid Loss = 0.5982\n",
      "Iteration 3/140: Train Loss = 0.5576, Valid Loss = 0.5608\n",
      "Iteration 4/140: Train Loss = 0.5249, Valid Loss = 0.5294\n",
      "Iteration 5/140: Train Loss = 0.4965, Valid Loss = 0.5019\n",
      "Iteration 6/140: Train Loss = 0.4720, Valid Loss = 0.4783\n",
      "Iteration 7/140: Train Loss = 0.4510, Valid Loss = 0.4580\n",
      "Iteration 8/140: Train Loss = 0.4322, Valid Loss = 0.4405\n",
      "Iteration 9/140: Train Loss = 0.4159, Valid Loss = 0.4247\n",
      "Iteration 10/140: Train Loss = 0.4011, Valid Loss = 0.4110\n",
      "Iteration 11/140: Train Loss = 0.3880, Valid Loss = 0.3983\n",
      "Iteration 12/140: Train Loss = 0.3763, Valid Loss = 0.3873\n",
      "Iteration 13/140: Train Loss = 0.3657, Valid Loss = 0.3774\n",
      "Iteration 14/140: Train Loss = 0.3561, Valid Loss = 0.3685\n",
      "Iteration 15/140: Train Loss = 0.3476, Valid Loss = 0.3603\n",
      "Iteration 16/140: Train Loss = 0.3399, Valid Loss = 0.3531\n",
      "Iteration 17/140: Train Loss = 0.3327, Valid Loss = 0.3467\n",
      "Iteration 18/140: Train Loss = 0.3263, Valid Loss = 0.3408\n",
      "Iteration 19/140: Train Loss = 0.3201, Valid Loss = 0.3352\n",
      "Iteration 20/140: Train Loss = 0.3145, Valid Loss = 0.3303\n",
      "Iteration 21/140: Train Loss = 0.3093, Valid Loss = 0.3257\n",
      "Iteration 22/140: Train Loss = 0.3046, Valid Loss = 0.3215\n",
      "Iteration 23/140: Train Loss = 0.3001, Valid Loss = 0.3174\n",
      "Iteration 24/140: Train Loss = 0.2959, Valid Loss = 0.3138\n",
      "Iteration 25/140: Train Loss = 0.2921, Valid Loss = 0.3104\n",
      "Iteration 26/140: Train Loss = 0.2883, Valid Loss = 0.3074\n",
      "Iteration 27/140: Train Loss = 0.2848, Valid Loss = 0.3046\n",
      "Iteration 28/140: Train Loss = 0.2816, Valid Loss = 0.3015\n",
      "Iteration 29/140: Train Loss = 0.2788, Valid Loss = 0.2990\n",
      "Iteration 30/140: Train Loss = 0.2759, Valid Loss = 0.2968\n",
      "Iteration 31/140: Train Loss = 0.2731, Valid Loss = 0.2947\n",
      "Iteration 32/140: Train Loss = 0.2701, Valid Loss = 0.2924\n",
      "Iteration 33/140: Train Loss = 0.2678, Valid Loss = 0.2903\n",
      "Iteration 34/140: Train Loss = 0.2652, Valid Loss = 0.2878\n",
      "Iteration 35/140: Train Loss = 0.2628, Valid Loss = 0.2858\n",
      "Iteration 36/140: Train Loss = 0.2608, Valid Loss = 0.2838\n",
      "Iteration 37/140: Train Loss = 0.2589, Valid Loss = 0.2822\n",
      "Iteration 38/140: Train Loss = 0.2570, Valid Loss = 0.2807\n",
      "Iteration 39/140: Train Loss = 0.2551, Valid Loss = 0.2792\n",
      "Iteration 40/140: Train Loss = 0.2535, Valid Loss = 0.2782\n",
      "Iteration 41/140: Train Loss = 0.2518, Valid Loss = 0.2768\n",
      "Iteration 42/140: Train Loss = 0.2503, Valid Loss = 0.2756\n",
      "Iteration 43/140: Train Loss = 0.2487, Valid Loss = 0.2742\n",
      "Iteration 44/140: Train Loss = 0.2471, Valid Loss = 0.2731\n",
      "Iteration 45/140: Train Loss = 0.2457, Valid Loss = 0.2719\n",
      "Iteration 46/140: Train Loss = 0.2440, Valid Loss = 0.2712\n",
      "Iteration 47/140: Train Loss = 0.2425, Valid Loss = 0.2700\n",
      "Iteration 48/140: Train Loss = 0.2411, Valid Loss = 0.2690\n",
      "Iteration 49/140: Train Loss = 0.2399, Valid Loss = 0.2684\n",
      "Iteration 50/140: Train Loss = 0.2387, Valid Loss = 0.2679\n",
      "Iteration 51/140: Train Loss = 0.2374, Valid Loss = 0.2670\n",
      "Iteration 52/140: Train Loss = 0.2362, Valid Loss = 0.2664\n",
      "Iteration 53/140: Train Loss = 0.2351, Valid Loss = 0.2656\n",
      "Iteration 54/140: Train Loss = 0.2341, Valid Loss = 0.2649\n",
      "Iteration 55/140: Train Loss = 0.2330, Valid Loss = 0.2644\n",
      "Iteration 56/140: Train Loss = 0.2320, Valid Loss = 0.2636\n",
      "Iteration 57/140: Train Loss = 0.2311, Valid Loss = 0.2629\n",
      "Iteration 58/140: Train Loss = 0.2301, Valid Loss = 0.2625\n",
      "Iteration 59/140: Train Loss = 0.2292, Valid Loss = 0.2618\n",
      "Iteration 60/140: Train Loss = 0.2284, Valid Loss = 0.2608\n",
      "Iteration 61/140: Train Loss = 0.2275, Valid Loss = 0.2603\n",
      "Iteration 62/140: Train Loss = 0.2266, Valid Loss = 0.2596\n",
      "Iteration 63/140: Train Loss = 0.2257, Valid Loss = 0.2587\n",
      "Iteration 64/140: Train Loss = 0.2250, Valid Loss = 0.2583\n",
      "Iteration 65/140: Train Loss = 0.2242, Valid Loss = 0.2578\n",
      "Iteration 66/140: Train Loss = 0.2236, Valid Loss = 0.2572\n",
      "Iteration 67/140: Train Loss = 0.2227, Valid Loss = 0.2571\n",
      "Iteration 68/140: Train Loss = 0.2220, Valid Loss = 0.2568\n",
      "Iteration 69/140: Train Loss = 0.2211, Valid Loss = 0.2563\n",
      "Iteration 70/140: Train Loss = 0.2205, Valid Loss = 0.2559\n",
      "Iteration 71/140: Train Loss = 0.2198, Valid Loss = 0.2555\n",
      "Iteration 72/140: Train Loss = 0.2191, Valid Loss = 0.2552\n",
      "Iteration 73/140: Train Loss = 0.2184, Valid Loss = 0.2548\n",
      "Iteration 74/140: Train Loss = 0.2178, Valid Loss = 0.2544\n",
      "Iteration 75/140: Train Loss = 0.2172, Valid Loss = 0.2538\n",
      "Iteration 76/140: Train Loss = 0.2167, Valid Loss = 0.2534\n",
      "Iteration 77/140: Train Loss = 0.2162, Valid Loss = 0.2529\n",
      "Iteration 78/140: Train Loss = 0.2156, Valid Loss = 0.2528\n",
      "Iteration 79/140: Train Loss = 0.2151, Valid Loss = 0.2526\n",
      "Iteration 80/140: Train Loss = 0.2146, Valid Loss = 0.2520\n",
      "Iteration 81/140: Train Loss = 0.2139, Valid Loss = 0.2517\n",
      "Iteration 82/140: Train Loss = 0.2134, Valid Loss = 0.2513\n",
      "Iteration 83/140: Train Loss = 0.2129, Valid Loss = 0.2510\n",
      "Iteration 84/140: Train Loss = 0.2124, Valid Loss = 0.2507\n",
      "Iteration 85/140: Train Loss = 0.2118, Valid Loss = 0.2505\n",
      "Iteration 86/140: Train Loss = 0.2111, Valid Loss = 0.2502\n",
      "Iteration 87/140: Train Loss = 0.2106, Valid Loss = 0.2498\n",
      "Iteration 88/140: Train Loss = 0.2101, Valid Loss = 0.2495\n",
      "Iteration 89/140: Train Loss = 0.2096, Valid Loss = 0.2493\n",
      "Iteration 90/140: Train Loss = 0.2091, Valid Loss = 0.2490\n",
      "Iteration 91/140: Train Loss = 0.2086, Valid Loss = 0.2489\n",
      "Iteration 92/140: Train Loss = 0.2080, Valid Loss = 0.2488\n",
      "Iteration 93/140: Train Loss = 0.2076, Valid Loss = 0.2486\n",
      "Iteration 94/140: Train Loss = 0.2071, Valid Loss = 0.2483\n",
      "Iteration 95/140: Train Loss = 0.2066, Valid Loss = 0.2481\n",
      "Iteration 96/140: Train Loss = 0.2060, Valid Loss = 0.2478\n",
      "Iteration 97/140: Train Loss = 0.2057, Valid Loss = 0.2476\n",
      "Iteration 98/140: Train Loss = 0.2053, Valid Loss = 0.2474\n",
      "Iteration 99/140: Train Loss = 0.2050, Valid Loss = 0.2474\n",
      "Iteration 100/140: Train Loss = 0.2045, Valid Loss = 0.2473\n",
      "Iteration 101/140: Train Loss = 0.2041, Valid Loss = 0.2472\n",
      "Iteration 102/140: Train Loss = 0.2037, Valid Loss = 0.2470\n",
      "Iteration 103/140: Train Loss = 0.2032, Valid Loss = 0.2471\n",
      "Iteration 104/140: Train Loss = 0.2029, Valid Loss = 0.2469\n",
      "Iteration 105/140: Train Loss = 0.2025, Valid Loss = 0.2469\n",
      "Iteration 106/140: Train Loss = 0.2022, Valid Loss = 0.2467\n",
      "Iteration 107/140: Train Loss = 0.2018, Valid Loss = 0.2464\n",
      "Iteration 108/140: Train Loss = 0.2015, Valid Loss = 0.2462\n",
      "Iteration 109/140: Train Loss = 0.2011, Valid Loss = 0.2460\n",
      "Iteration 110/140: Train Loss = 0.2007, Valid Loss = 0.2458\n",
      "Iteration 111/140: Train Loss = 0.2003, Valid Loss = 0.2457\n",
      "Iteration 112/140: Train Loss = 0.1999, Valid Loss = 0.2455\n",
      "Iteration 113/140: Train Loss = 0.1996, Valid Loss = 0.2453\n",
      "Iteration 114/140: Train Loss = 0.1993, Valid Loss = 0.2453\n",
      "Iteration 115/140: Train Loss = 0.1990, Valid Loss = 0.2452\n",
      "Iteration 116/140: Train Loss = 0.1987, Valid Loss = 0.2450\n",
      "Iteration 117/140: Train Loss = 0.1983, Valid Loss = 0.2450\n",
      "Iteration 118/140: Train Loss = 0.1980, Valid Loss = 0.2451\n",
      "Iteration 119/140: Train Loss = 0.1977, Valid Loss = 0.2449\n",
      "Iteration 120/140: Train Loss = 0.1974, Valid Loss = 0.2448\n",
      "Iteration 121/140: Train Loss = 0.1970, Valid Loss = 0.2446\n",
      "Iteration 122/140: Train Loss = 0.1968, Valid Loss = 0.2445\n",
      "Iteration 123/140: Train Loss = 0.1965, Valid Loss = 0.2443\n",
      "Iteration 124/140: Train Loss = 0.1962, Valid Loss = 0.2443\n",
      "Iteration 125/140: Train Loss = 0.1959, Valid Loss = 0.2443\n",
      "Iteration 126/140: Train Loss = 0.1956, Valid Loss = 0.2441\n",
      "Iteration 127/140: Train Loss = 0.1953, Valid Loss = 0.2439\n",
      "Iteration 128/140: Train Loss = 0.1951, Valid Loss = 0.2439\n",
      "Iteration 129/140: Train Loss = 0.1949, Valid Loss = 0.2437\n",
      "Iteration 130/140: Train Loss = 0.1947, Valid Loss = 0.2436\n",
      "Iteration 131/140: Train Loss = 0.1943, Valid Loss = 0.2435\n",
      "Iteration 132/140: Train Loss = 0.1941, Valid Loss = 0.2435\n",
      "Iteration 133/140: Train Loss = 0.1938, Valid Loss = 0.2434\n",
      "Iteration 134/140: Train Loss = 0.1935, Valid Loss = 0.2434\n",
      "Iteration 135/140: Train Loss = 0.1932, Valid Loss = 0.2433\n",
      "Iteration 136/140: Train Loss = 0.1930, Valid Loss = 0.2434\n",
      "Iteration 137/140: Train Loss = 0.1928, Valid Loss = 0.2435\n",
      "Iteration 138/140: Train Loss = 0.1924, Valid Loss = 0.2435\n",
      "Iteration 139/140: Train Loss = 0.1922, Valid Loss = 0.2433\n",
      "Iteration 140/140: Train Loss = 0.1919, Valid Loss = 0.2433\n"
     ]
    }
   ],
   "source": [
    "best_boosting = Boosting(\n",
    "    base_model_params={\n",
    "        'max_depth': best_params['max_depth'],\n",
    "        'min_samples_leaf': best_params['min_samples_leaf']\n",
    "    },\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    subsample=best_params['subsample'],\n",
    "    early_stopping_rounds=10,\n",
    "    plot=False\n",
    ")\n",
    "\n",
    "best_boosting.fit(x_train, y_train, x_valid, y_valid)\n",
    "\n",
    "y_pred_proba_boost = best_boosting.predict_proba(x_test)[:, 1] # вероятности класса 1 на тестовой выборке"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-05T10:28:34.092762200Z",
     "start_time": "2025-01-05T10:28:26.790593Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(solver='lbfgs', max_iter=500)\n",
    "log_reg.fit(x_train, y_train)\n",
    "\n",
    "y_pred_proba_lr = log_reg.predict_proba(x_test)[:, 1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-05T10:28:42.905213900Z",
     "start_time": "2025-01-05T10:28:42.765597200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LdJG4bHClWlV",
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-01-05T10:28:47.182476400Z",
     "start_time": "2025-01-05T10:28:46.980281600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 800x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### ╰( ͡☉ ͜ʖ ͡☉ )つ──☆*:・ﾟ   ฅ^•ﻌ•^ฅ   ʕ•ᴥ•ʔ\n",
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "prob_true_boost, prob_pred_boost = calibration_curve(y_test, y_pred_proba_boost, n_bins=10)\n",
    "\n",
    "prob_true_lr, prob_pred_lr = calibration_curve(y_test, y_pred_proba_lr, n_bins=10)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot(prob_pred_boost, prob_true_boost, marker='o', label='Boosting')\n",
    "\n",
    "plt.plot(prob_pred_lr, prob_true_lr, marker='o', label='Logistic Regression')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], '--', color='gray')\n",
    "\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.title('Calibration Curves')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8O6bjp2MlWlW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Теперь оценим важность признаков для градиентного бустинга.\n",
    "\n",
    "**Задание:**\n",
    "1. Поскольку базовая модель — дерево из `sklearn`, вычислите важность каждого признака для каждого дерева, используя атрибут `feature_importances_` у `DecisionTreeRegressor`.\n",
    "2. Усредните значения важности по всем деревьям и нормализуйте их так, чтобы сумма была равна единице (убедитесь, что значения неотрицательны).\n",
    "3. Дополните вашу реализацию бустинга, добавив метод `feature_importances_`, который будет возвращать усредненные и нормализованные важности признаков.\n",
    "\n",
    "**Построение графиков:**\n",
    "1. Постройте столбчатую диаграмму важности признаков для градиентного бустинга.\n",
    "2. На соседнем графике изобразите важность признаков для логистической регрессии, используя модули весов.\n",
    "3. Сравните графики и проанализируйте полученные результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "coeff_lr = np.abs(log_reg.coef_[0])\n",
    "coeff_lr_norm = coeff_lr / coeff_lr.sum()\n",
    "\n",
    "feat_imp_boost = best_boosting.feature_importances_\n",
    "feat_imp_lr = coeff_lr_norm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-05T10:40:50.104729600Z",
     "start_time": "2025-01-05T10:40:50.026399700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "y0JNgBk_lWlW",
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-01-05T10:41:32.310171300Z",
     "start_time": "2025-01-05T10:41:31.729838100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1200x500 with 2 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### ╰( ͡☉ ͜ʖ ͡☉ )つ──☆*:・ﾟ   ฅ^•ﻌ•^ฅ   ʕ•ᴥ•ʔ\n",
    "\n",
    "features = np.arange(len(feat_imp_boost))\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5), sharey=True)\n",
    "\n",
    "axes[0].bar(features, feat_imp_boost)\n",
    "axes[0].set_title(\"Boosting Feature Importances\")\n",
    "axes[0].set_xlabel(\"Feature Index\")\n",
    "axes[0].set_ylabel(\"Importance\")\n",
    "\n",
    "axes[1].bar(features, feat_imp_lr)\n",
    "axes[1].set_title(\"Logistic Regression Coefficients (abs, normalized)\")\n",
    "axes[1].set_xlabel(\"Feature Index\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xheSi2IolWlW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Обычно избыточные признаки могут негативно влиять на качество бустинга. Попробуйте следующее:\n",
    "\n",
    "1. **Отфильтруйте неважные признаки:** Используйте построенную диаграмму важности признаков, чтобы отобрать наиболее незначительные признаки.\n",
    "2. **Обучите модель повторно:** Обучите модель на основе оставшихся признаков с теми же гиперпараметрами.\n",
    "3. **Оцените качество модели:** Сравните результаты новой модели с исходной. Улучшилось ли качество после отфильтровывания незначительных признаков?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Rw9W9MEYlWlW",
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-01-05T11:06:12.352362500Z",
     "start_time": "2025-01-05T11:06:12.301190800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen features (indexes): [  0   1   2   3   4   5  10  12  15  19  20  21  24  25  27  36  69 121\n",
      " 142 143 155 156 157 158 159 160 161 162 163 164 165 166 167 168]\n",
      "Count of chosen features: 34 from 169\n"
     ]
    }
   ],
   "source": [
    "### ╰( ͡☉ ͜ʖ ͡☉ )つ──☆*:・ﾟ   ฅ^•ﻌ•^ฅ   ʕ•ᴥ•ʔ\n",
    "\n",
    "threshold = np.percentile(feat_imp_boost, 80)\n",
    "\n",
    "important_features = np.where(feat_imp_boost >= threshold)[0]\n",
    "\n",
    "print(f\"Chosen features (indexes): {important_features}\")\n",
    "print(f\"Count of chosen features: {len(important_features)} from {len(feat_imp_boost)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The form of the training sample after filtering: (18825, 34)\n",
      "The form of the validation sample after filtering: (2354, 34)\n",
      "The form of the test sample after filtering: (2353, 34)\n"
     ]
    }
   ],
   "source": [
    "selected_features = important_features\n",
    "\n",
    "x_train_reduced = x_train[:, selected_features]\n",
    "x_valid_reduced = x_valid[:, selected_features]\n",
    "x_test_reduced = x_test[:, selected_features]\n",
    "\n",
    "print(f\"The form of the training sample after filtering: {x_train_reduced.shape}\")\n",
    "print(f\"The form of the validation sample after filtering: {x_valid_reduced.shape}\")\n",
    "print(f\"The form of the test sample after filtering: {x_test_reduced.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-05T11:07:04.797489800Z",
     "start_time": "2025-01-05T11:07:04.743022400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/140: Train Loss = 0.6411, Valid Loss = 0.6427\n",
      "Iteration 2/140: Train Loss = 0.5959, Valid Loss = 0.5994\n",
      "Iteration 3/140: Train Loss = 0.5575, Valid Loss = 0.5621\n",
      "Iteration 4/140: Train Loss = 0.5247, Valid Loss = 0.5297\n",
      "Iteration 5/140: Train Loss = 0.4968, Valid Loss = 0.5029\n",
      "Iteration 6/140: Train Loss = 0.4717, Valid Loss = 0.4791\n",
      "Iteration 7/140: Train Loss = 0.4502, Valid Loss = 0.4581\n",
      "Iteration 8/140: Train Loss = 0.4317, Valid Loss = 0.4402\n",
      "Iteration 9/140: Train Loss = 0.4149, Valid Loss = 0.4245\n",
      "Iteration 10/140: Train Loss = 0.4001, Valid Loss = 0.4108\n",
      "Iteration 11/140: Train Loss = 0.3870, Valid Loss = 0.3980\n",
      "Iteration 12/140: Train Loss = 0.3754, Valid Loss = 0.3873\n",
      "Iteration 13/140: Train Loss = 0.3650, Valid Loss = 0.3773\n",
      "Iteration 14/140: Train Loss = 0.3554, Valid Loss = 0.3685\n",
      "Iteration 15/140: Train Loss = 0.3467, Valid Loss = 0.3607\n",
      "Iteration 16/140: Train Loss = 0.3389, Valid Loss = 0.3530\n",
      "Iteration 17/140: Train Loss = 0.3320, Valid Loss = 0.3469\n",
      "Iteration 18/140: Train Loss = 0.3259, Valid Loss = 0.3412\n",
      "Iteration 19/140: Train Loss = 0.3198, Valid Loss = 0.3352\n",
      "Iteration 20/140: Train Loss = 0.3143, Valid Loss = 0.3302\n",
      "Iteration 21/140: Train Loss = 0.3091, Valid Loss = 0.3257\n",
      "Iteration 22/140: Train Loss = 0.3042, Valid Loss = 0.3216\n",
      "Iteration 23/140: Train Loss = 0.2997, Valid Loss = 0.3177\n",
      "Iteration 24/140: Train Loss = 0.2955, Valid Loss = 0.3140\n",
      "Iteration 25/140: Train Loss = 0.2915, Valid Loss = 0.3105\n",
      "Iteration 26/140: Train Loss = 0.2880, Valid Loss = 0.3072\n",
      "Iteration 27/140: Train Loss = 0.2847, Valid Loss = 0.3045\n",
      "Iteration 28/140: Train Loss = 0.2816, Valid Loss = 0.3019\n",
      "Iteration 29/140: Train Loss = 0.2785, Valid Loss = 0.2996\n",
      "Iteration 30/140: Train Loss = 0.2754, Valid Loss = 0.2971\n",
      "Iteration 31/140: Train Loss = 0.2727, Valid Loss = 0.2943\n",
      "Iteration 32/140: Train Loss = 0.2702, Valid Loss = 0.2922\n",
      "Iteration 33/140: Train Loss = 0.2676, Valid Loss = 0.2901\n",
      "Iteration 34/140: Train Loss = 0.2654, Valid Loss = 0.2883\n",
      "Iteration 35/140: Train Loss = 0.2632, Valid Loss = 0.2866\n",
      "Iteration 36/140: Train Loss = 0.2612, Valid Loss = 0.2850\n",
      "Iteration 37/140: Train Loss = 0.2590, Valid Loss = 0.2831\n",
      "Iteration 38/140: Train Loss = 0.2572, Valid Loss = 0.2817\n",
      "Iteration 39/140: Train Loss = 0.2552, Valid Loss = 0.2798\n",
      "Iteration 40/140: Train Loss = 0.2535, Valid Loss = 0.2783\n",
      "Iteration 41/140: Train Loss = 0.2519, Valid Loss = 0.2771\n",
      "Iteration 42/140: Train Loss = 0.2503, Valid Loss = 0.2761\n",
      "Iteration 43/140: Train Loss = 0.2484, Valid Loss = 0.2749\n",
      "Iteration 44/140: Train Loss = 0.2468, Valid Loss = 0.2735\n",
      "Iteration 45/140: Train Loss = 0.2455, Valid Loss = 0.2721\n",
      "Iteration 46/140: Train Loss = 0.2441, Valid Loss = 0.2711\n",
      "Iteration 47/140: Train Loss = 0.2426, Valid Loss = 0.2702\n",
      "Iteration 48/140: Train Loss = 0.2413, Valid Loss = 0.2691\n",
      "Iteration 49/140: Train Loss = 0.2400, Valid Loss = 0.2684\n",
      "Iteration 50/140: Train Loss = 0.2388, Valid Loss = 0.2674\n",
      "Iteration 51/140: Train Loss = 0.2377, Valid Loss = 0.2663\n",
      "Iteration 52/140: Train Loss = 0.2365, Valid Loss = 0.2652\n",
      "Iteration 53/140: Train Loss = 0.2353, Valid Loss = 0.2644\n",
      "Iteration 54/140: Train Loss = 0.2343, Valid Loss = 0.2637\n",
      "Iteration 55/140: Train Loss = 0.2332, Valid Loss = 0.2628\n",
      "Iteration 56/140: Train Loss = 0.2322, Valid Loss = 0.2622\n",
      "Iteration 57/140: Train Loss = 0.2312, Valid Loss = 0.2619\n",
      "Iteration 58/140: Train Loss = 0.2302, Valid Loss = 0.2614\n",
      "Iteration 59/140: Train Loss = 0.2291, Valid Loss = 0.2609\n",
      "Iteration 60/140: Train Loss = 0.2282, Valid Loss = 0.2602\n",
      "Iteration 61/140: Train Loss = 0.2273, Valid Loss = 0.2596\n",
      "Iteration 62/140: Train Loss = 0.2263, Valid Loss = 0.2587\n",
      "Iteration 63/140: Train Loss = 0.2254, Valid Loss = 0.2581\n",
      "Iteration 64/140: Train Loss = 0.2247, Valid Loss = 0.2576\n",
      "Iteration 65/140: Train Loss = 0.2238, Valid Loss = 0.2571\n",
      "Iteration 66/140: Train Loss = 0.2231, Valid Loss = 0.2569\n",
      "Iteration 67/140: Train Loss = 0.2222, Valid Loss = 0.2565\n",
      "Iteration 68/140: Train Loss = 0.2215, Valid Loss = 0.2558\n",
      "Iteration 69/140: Train Loss = 0.2207, Valid Loss = 0.2553\n",
      "Iteration 70/140: Train Loss = 0.2201, Valid Loss = 0.2547\n",
      "Iteration 71/140: Train Loss = 0.2194, Valid Loss = 0.2544\n",
      "Iteration 72/140: Train Loss = 0.2187, Valid Loss = 0.2542\n",
      "Iteration 73/140: Train Loss = 0.2181, Valid Loss = 0.2537\n",
      "Iteration 74/140: Train Loss = 0.2175, Valid Loss = 0.2534\n",
      "Iteration 75/140: Train Loss = 0.2168, Valid Loss = 0.2529\n",
      "Iteration 76/140: Train Loss = 0.2161, Valid Loss = 0.2528\n",
      "Iteration 77/140: Train Loss = 0.2154, Valid Loss = 0.2523\n",
      "Iteration 78/140: Train Loss = 0.2148, Valid Loss = 0.2518\n",
      "Iteration 79/140: Train Loss = 0.2141, Valid Loss = 0.2516\n",
      "Iteration 80/140: Train Loss = 0.2136, Valid Loss = 0.2513\n",
      "Iteration 81/140: Train Loss = 0.2129, Valid Loss = 0.2511\n",
      "Iteration 82/140: Train Loss = 0.2123, Valid Loss = 0.2507\n",
      "Iteration 83/140: Train Loss = 0.2117, Valid Loss = 0.2504\n",
      "Iteration 84/140: Train Loss = 0.2113, Valid Loss = 0.2500\n",
      "Iteration 85/140: Train Loss = 0.2109, Valid Loss = 0.2498\n",
      "Iteration 86/140: Train Loss = 0.2102, Valid Loss = 0.2495\n",
      "Iteration 87/140: Train Loss = 0.2098, Valid Loss = 0.2493\n",
      "Iteration 88/140: Train Loss = 0.2092, Valid Loss = 0.2490\n",
      "Iteration 89/140: Train Loss = 0.2087, Valid Loss = 0.2489\n",
      "Iteration 90/140: Train Loss = 0.2082, Valid Loss = 0.2486\n",
      "Iteration 91/140: Train Loss = 0.2077, Valid Loss = 0.2484\n",
      "Iteration 92/140: Train Loss = 0.2072, Valid Loss = 0.2482\n",
      "Iteration 93/140: Train Loss = 0.2067, Valid Loss = 0.2480\n",
      "Iteration 94/140: Train Loss = 0.2064, Valid Loss = 0.2479\n",
      "Iteration 95/140: Train Loss = 0.2059, Valid Loss = 0.2474\n",
      "Iteration 96/140: Train Loss = 0.2055, Valid Loss = 0.2471\n",
      "Iteration 97/140: Train Loss = 0.2052, Valid Loss = 0.2470\n",
      "Iteration 98/140: Train Loss = 0.2048, Valid Loss = 0.2467\n",
      "Iteration 99/140: Train Loss = 0.2044, Valid Loss = 0.2465\n",
      "Iteration 100/140: Train Loss = 0.2040, Valid Loss = 0.2465\n",
      "Iteration 101/140: Train Loss = 0.2036, Valid Loss = 0.2462\n",
      "Iteration 102/140: Train Loss = 0.2031, Valid Loss = 0.2458\n",
      "Iteration 103/140: Train Loss = 0.2027, Valid Loss = 0.2456\n",
      "Iteration 104/140: Train Loss = 0.2024, Valid Loss = 0.2454\n",
      "Iteration 105/140: Train Loss = 0.2020, Valid Loss = 0.2454\n",
      "Iteration 106/140: Train Loss = 0.2016, Valid Loss = 0.2451\n",
      "Iteration 107/140: Train Loss = 0.2013, Valid Loss = 0.2451\n",
      "Iteration 108/140: Train Loss = 0.2008, Valid Loss = 0.2449\n",
      "Iteration 109/140: Train Loss = 0.2005, Valid Loss = 0.2448\n",
      "Iteration 110/140: Train Loss = 0.2001, Valid Loss = 0.2447\n",
      "Iteration 111/140: Train Loss = 0.1998, Valid Loss = 0.2444\n",
      "Iteration 112/140: Train Loss = 0.1994, Valid Loss = 0.2442\n",
      "Iteration 113/140: Train Loss = 0.1991, Valid Loss = 0.2444\n",
      "Iteration 114/140: Train Loss = 0.1987, Valid Loss = 0.2441\n",
      "Iteration 115/140: Train Loss = 0.1984, Valid Loss = 0.2440\n",
      "Iteration 116/140: Train Loss = 0.1980, Valid Loss = 0.2440\n",
      "Iteration 117/140: Train Loss = 0.1977, Valid Loss = 0.2438\n",
      "Iteration 118/140: Train Loss = 0.1974, Valid Loss = 0.2435\n",
      "Iteration 119/140: Train Loss = 0.1971, Valid Loss = 0.2434\n",
      "Iteration 120/140: Train Loss = 0.1966, Valid Loss = 0.2433\n",
      "Iteration 121/140: Train Loss = 0.1962, Valid Loss = 0.2432\n",
      "Iteration 122/140: Train Loss = 0.1959, Valid Loss = 0.2430\n",
      "Iteration 123/140: Train Loss = 0.1956, Valid Loss = 0.2430\n",
      "Iteration 124/140: Train Loss = 0.1953, Valid Loss = 0.2429\n",
      "Iteration 125/140: Train Loss = 0.1951, Valid Loss = 0.2430\n",
      "Iteration 126/140: Train Loss = 0.1949, Valid Loss = 0.2429\n",
      "Iteration 127/140: Train Loss = 0.1945, Valid Loss = 0.2427\n",
      "Iteration 128/140: Train Loss = 0.1942, Valid Loss = 0.2426\n",
      "Iteration 129/140: Train Loss = 0.1939, Valid Loss = 0.2424\n",
      "Iteration 130/140: Train Loss = 0.1936, Valid Loss = 0.2423\n",
      "Iteration 131/140: Train Loss = 0.1934, Valid Loss = 0.2422\n",
      "Iteration 132/140: Train Loss = 0.1932, Valid Loss = 0.2420\n",
      "Iteration 133/140: Train Loss = 0.1929, Valid Loss = 0.2419\n",
      "Iteration 134/140: Train Loss = 0.1927, Valid Loss = 0.2419\n",
      "Iteration 135/140: Train Loss = 0.1924, Valid Loss = 0.2418\n",
      "Iteration 136/140: Train Loss = 0.1920, Valid Loss = 0.2414\n",
      "Iteration 137/140: Train Loss = 0.1917, Valid Loss = 0.2414\n",
      "Iteration 138/140: Train Loss = 0.1913, Valid Loss = 0.2410\n",
      "Iteration 139/140: Train Loss = 0.1911, Valid Loss = 0.2411\n",
      "Iteration 140/140: Train Loss = 0.1907, Valid Loss = 0.2411\n"
     ]
    }
   ],
   "source": [
    "boosting_reduced = Boosting(\n",
    "    base_model_params={\n",
    "        \"max_depth\": best_params[\"max_depth\"],\n",
    "        \"min_samples_leaf\": best_params[\"min_samples_leaf\"]\n",
    "    },\n",
    "    n_estimators=best_params[\"n_estimators\"],\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    subsample=best_params[\"subsample\"],\n",
    "    early_stopping_rounds=10,\n",
    "    plot=False\n",
    ")\n",
    "\n",
    "boosting_reduced.fit(x_train_reduced, y_train, x_valid_reduced, y_valid)\n",
    "\n",
    "y_pred_proba_boost_reduced = boosting_reduced.predict_proba(x_test_reduced)[:, 1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-05T11:08:05.521609100Z",
     "start_time": "2025-01-05T11:07:58.161851900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Boosting Test ROC-AUC: 0.9661\n",
      "Reduced Boosting Test ROC-AUC: 0.9649\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_original = best_boosting.score(x_test, y_test)\n",
    "\n",
    "roc_auc_reduced = roc_auc_score(y_test, y_pred_proba_boost_reduced)\n",
    "\n",
    "print(f\"Original Boosting Test ROC-AUC: {roc_auc_original:.4f}\")\n",
    "print(f\"Reduced Boosting Test ROC-AUC: {roc_auc_reduced:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-05T11:12:19.243987900Z",
     "start_time": "2025-01-05T11:12:19.151466200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Качество модели стало незначительно хуже"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pH3489RklWlW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задание 5 (бонус). Блендинговое [0.5 балла]\n",
    "\n",
    "Реализуйте блендинг над вашей лучшей моделью и логистической регрессией. Улучшилось ли качество?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWcDMMVilWlW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### ╰( ͡☉ ͜ʖ ͡☉ )つ──☆*:・ﾟ   ฅ^•ﻌ•^ฅ   ʕ•ᴥ•ʔ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "eEF0yJ4vlWlX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задание 6 (бонус). Катбустовое [0.5 балла]\n",
    "\n",
    "Запустите [CatBoost](https://catboost.ai/en/docs/concepts/python-quickstart) на наших данных, сравните с вашей реализацией. Где получилось лучше?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0lYGhc_clWlX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### ╰( ͡☉ ͜ʖ ͡☉ )つ──☆*:・ﾟ   ฅ^•ﻌ•^ฅ   ʕ•ᴥ•ʔ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqHzTXCllZO8"
   },
   "source": [
    "Оставьте пожалуйста отзыв о курсе!\n",
    "\n",
    "https://forms.gle/LajA3Xrps6u96Q5A8\n",
    "\n",
    "\n",
    "Это очень важно. Благодаря обратной связи мы будем двигаться в сторону антиградиента)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "210px",
    "width": "492px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
